# -*- coding: utf-8 -*-
"""Orwin_Definitivo_Juan_Proyecto_de_Grado_Juan_Jose_y_Valentina.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FRo47M4u1AQQXJhSmZSikqhnruQ_RV-N

# **Sprint 0 - Setup**

## 1. Configuración Inicial - Setup
"""

# Importar librerías necesarias para el proceso
import os, sys, json, time, shutil, random
from pathlib import Path
import numpy as np
import pandas as pd
from tqdm import tqdm
from collections import defaultdict, Counter
import matplotlib.pyplot as plt
import seaborn as sns
import random

# ML
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from scipy.stats import entropy

# CV / PIL / OpenCV
from PIL import Image
import cv2

# scikit-learn metrics
from sklearn.model_selection import GroupKFold
from sklearn.metrics import (
    accuracy_score, f1_score, confusion_matrix,
    cohen_kappa_score, balanced_accuracy_score,
    matthews_corrcoef, roc_auc_score,
    precision_recall_curve, roc_curve, auc, average_precision_score
)

# Mostrar device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Device:", device)

# ---------------------------
# CONFIG - ajusta rutas si hace falta
# ---------------------------
DATA_ROOT = '/content/sicapv2_data'    # cambiar si es necesario
# Sprint 0 artifacts
IMAGES_DIR = os.path.join(DATA_ROOT , 'SICAPv2', 'images')
MASKS_DIR  = os.path.join(DATA_ROOT , 'SICAPv2', 'masks')
LABELS_XLSX = os.path.join(DATA_ROOT , 'SICAPv2', 'wsi_labels.xlsx')
MANIFEST_CSV = os.path.join(DATA_ROOT ,'dataset_manifest.csv')

print("Rutas configuradas:")
print(" ROOT:", DATA_ROOT )
print(" IMAGES_DIR:", IMAGES_DIR)
print(" MASKS_DIR:", MASKS_DIR)
print(" LABELS_XLSX:", LABELS_XLSX)
print(" MANIFEST_CSV:", MANIFEST_CSV)

# Sprint1 processed
PROCESSED_BASE = os.path.join(DATA_ROOT , 'processed_patches', 'v1')
PROCESSED_MANIFEST = os.path.join(PROCESSED_BASE, 'dataset_manifest_processed.csv')
PROCESSED_IMAGES = os.path.join(PROCESSED_BASE, 'images')

# Sprint2 embeddings
EMBEDDINGS_DIR = os.path.join(PROCESSED_BASE, 'embeddings')

# Sprint3 outputs
MIL_OUTPUT_DIR = os.path.join(DATA_ROOT , 'MIL_dataset')
os.makedirs(MIL_OUTPUT_DIR, exist_ok=True)

# reproducibility
RND = 42
np.random.seed(RND)
random.seed(RND)
torch.manual_seed(RND)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(RND)
print("Device:", device)

"""### UTILS"""

# Utilities: parsing filenames, tissue fraction, safe load npy, label mapping helpers.
import re, hashlib, subprocess

def parse_filename(fn):
    """Extrae slide_id, x_ini, y_ini a partir del nombre del parche.
       Asume formato con '_Block' y '_xini_..._yini_...' (ajustar si es distinto)"""
    basename = os.path.basename(fn)
    m_slide = re.match(r'^([^_]+)_Block', basename)
    slide_id = m_slide.group(1) if m_slide else basename.split('_')[0]
    m = re.search(r'_xini_(\d+)_yini_(\d+)', basename)
    x_ini, y_ini = (int(m.group(1)), int(m.group(2))) if m else (None, None)
    return slide_id, x_ini, y_ini

def tissueFractionForImage(pil_img, downscale=256, sat_thr=20, val_thr=250):
    arr = np.array(pil_img.convert('RGB'))
    h,w = arr.shape[:2]
    scale = max(1, int(max(h,w)/downscale))
    small = cv2.resize(arr, (max(1,w//scale), max(1,h//scale)), interpolation=cv2.INTER_AREA)
    hsv = cv2.cvtColor(small, cv2.COLOR_RGB2HSV)
    s, v = hsv[:,:,1], hsv[:,:,2]
    tissueMask = (s > sat_thr) & (v < val_thr)
    return float(tissueMask.mean())

def safe_load_npy(path):
    """Carga .npy robustamente y devuelve vector 1-D"""
    x = np.load(path)
    if x.ndim == 0:
        return np.array([x]).astype(np.float32)
    if x.ndim > 1:
        x = x.squeeze()
    if x.ndim == 1:
        return x.astype(np.float32)
    # fallback flatten
    return x.reshape(-1).astype(np.float32)

def get_git_hash():
    try:
        return subprocess.check_output(['git','rev-parse','--short','HEAD']).decode().strip()
    except Exception:
        return None

# Label remapping (consistent)
LABEL_MAP = {0:0, 3:1, 4:2, 5:3}
def map_label_to_class(raw_label):
    try:
        return LABEL_MAP[int(raw_label)]
    except Exception:
        return None

"""## 1.1 Verificar espacio disponible"""

# Verificar el espacio disponible en el disco local de Colab
!df -h /content

"""## 2. Descarga del Conjunto de Datos

"""

ZIP_MAIN = "/content/sicapv2.zip"

if not os.path.exists(ZIP_MAIN):
    print("Archivo ZIP principal NO existe. Descargando...")

    # Enlace de descarga para el archivo ZIP
    #download_url = "https://prod-dcd-datasets-cache-zipfiles.s3.eu-west-1.amazonaws.com/9xxm58dvs3-2.zip" Ya no sirve
    download_url = "https://data.mendeley.com/public-api/zip/9xxm58dvs3/download/1"

    # Descargar el archivo ZIP y guardarlo como sicapv2.zip
    !wget -O $ZIP_MAIN $download_url
else:
    print("Archivo ZIP principal ya existe → NO se descarga.")

"""## 3. Descomprimir el Archivo ZIP principal

"""

INTERMEDIATE_DIR = "/content/intermediate_data"

if not os.path.exists(INTERMEDIATE_DIR):
    print("Descomprimiendo ZIP principal...")

    # Crear un directorio para los archivos extraídos
    !mkdir /content/intermediate_data

    # Descomprimir el archivo principal en el directorio intermedio
    !unzip -q $ZIP_MAIN -d $INTERMEDIATE_DIR
else:
    print("Carpeta intermedia ya existe → NO se vuelve a descomprimir.")

# Listar los contenidos para verificar
!ls /content/intermediate_data

"""## 4. Descomprimir el Archivo ZIP Anidado (SICAPv2.zip)

####"/content/intermediate_data/SICAPv2.../SICAPv2.zip"
"""

# Ruta al archivo ZIP anidado
nested_zip = "/content/intermediate_data/SICAPv2.zip"

# Define the target directory for the nested zip content
TARGET_SICAPV2_DIR = os.path.join(DATA_ROOT, 'SICAPv2')

# If the target directory exists but is empty or incorrectly populated, remove it to force re-extraction
if os.path.exists(TARGET_SICAPV2_DIR) and not os.listdir(TARGET_SICAPV2_DIR):
    print(f"El directorio {TARGET_SICAPV2_DIR} existe pero está vacío, eliminando para forzar re-extracción...")
    shutil.rmtree(TARGET_SICAPV2_DIR)

# Check if the target extracted directory exists, not just ROOT
if not os.path.exists(TARGET_SICAPV2_DIR):
    print("Descomprimiendo ZIP anidado de SICAPv2...")

    # Crear un directorio para el resultado final
    os.makedirs(TARGET_SICAPV2_DIR, exist_ok=True)

    # Descomprimir el archivo anidado en el directorio final
    # Assuming SICAPv2.zip directly contains 'SICAPv2' folder, so unzip to ROOT
    !unzip -q "$nested_zip" -d $DATA_ROOT
    print(f"SICAP V2 guardado en la ruta: {TARGET_SICAPV2_DIR}")
else:
    print("SICAPv2 ya existe en disco \u2192 NO se descomprime nuevamente.")

# Listar el contenido del directorio final
!ls $DATA_ROOT
!ls /content/sicapv2_data/SICAPv2/

"""## 5. Limpiar y Organizar el Espacio

"""

if os.path.exists(ZIP_MAIN):
    print("Eliminando ZIP principal...")

    # Eliminar el archivo ZIP principal
    !rm $ZIP_MAIN

if os.path.exists(INTERMEDIATE_DIR):
    print("Eliminando carpeta intermedia...")

    # Eliminar el directorio intermedio y su contenido de forma recursiva
    !rm -r $INTERMEDIATE_DIR

# Verificar el espacio y las carpetas restantes
print("Estado final del disco:")
!df -h /content

print("\nContenido de /content:")
!ls /content

print("Verificando existencia de componentes esenciales:\n")
print("Labels XLSX:", os.path.exists(LABELS_XLSX))
print("Images dir:", os.path.exists(IMAGES_DIR))
print("Masks dir :", os.path.exists(MASKS_DIR))

print("\nConteo de archivos:")
print("Total imágenes :", len(os.listdir(IMAGES_DIR)))
print("Total máscaras :", len(os.listdir(MASKS_DIR)))

"""## 6. Cargar las Anotaciones"""

df_labels = pd.read_excel(LABELS_XLSX)

print("Primeras filas:")
display(df_labels.head())

print("\nShape:", df_labels.shape)
print("\nInfo:")
display(df_labels.info())

"""## 7. Vista rápida de imágenes/máscaras"""

print("Ejemplo de imágenes:")
!ls $IMAGES_DIR | head

print("\nEjemplo de máscaras:")
!ls $MASKS_DIR | head

"""## 8. Verificación de datos y construcción de dataset_manifest.csv"""

print("\n=== Construyendo dataset_manifest.csv ===")

MASK_PALETTES_JSON = os.path.join(DATA_ROOT, "mask_palettes.json")

"""### 8.1 Función auxiliar para inspeccionar máscaras (modo, valores únicos, paleta)"""

# Función auxiliar para inspeccionar máscaras (modo, valores únicos, paleta)
def inspect_mask(mask_path):
    try:
        m = Image.open(mask_path)
        arr = np.array(m)
        info = {
            "mode": m.mode,
            "unique_values": np.unique(arr).tolist()
        }
        # Si la máscara tiene paleta, almacenarla
        if m.palette is not None:
            info["palette"] = m.palette.getdata()[1]
        return info
    except Exception:
        return {"mode": None, "unique_values": None, "palette": None}

if os.path.exists(MANIFEST_CSV):
    print(f"Manifest ya existe en:\n  {MANIFEST_CSV}")
    print("Si deseas regenerarlo, elimina el archivo manualmente.")
else:
    print("Generando manifest con validaciones avanzadas...")

    rows = []
    maskPalettes = {}
    missingMasks = 0

    # Mapa slide_id -> metadata (patient_id, Gleason_primary, etc.)
    labels_df = pd.read_excel(LABELS_XLSX)
    labels_df['slide_id'] = labels_df['slide_id'].astype(str)
    slide_to_meta = labels_df.set_index('slide_id').to_dict(orient='index')

    image_files = sorted([f for f in os.listdir(IMAGES_DIR)
                          if f.lower().endswith(('.jpg', '.jpeg', '.png'))])

    print(f"Total imágenes detectadas: {len(image_files)}\n")

    for fname in tqdm(image_files, desc="Analizando parches"):
        img_path = os.path.join(IMAGES_DIR, fname)
        mask_fname = os.path.splitext(fname)[0] + ".png"
        mask_path = os.path.join(MASKS_DIR, mask_fname)

        # Extraer slide_id y coordenadas
        slide_id, x_ini, y_ini = parse_filename(fname)

        # Cargar atributos clínicos
        meta = slide_to_meta.get(slide_id, {})
        patient_id = meta.get("patient_id")
        gleason_p = meta.get("Gleason_primary")
        gleason_s = meta.get("Gleason_secondary")

        # ---------------------------
        # Verificación de calidad de la imagen
        # ---------------------------
        try:
            im = Image.open(img_path)
            width, height = im.size

            tissueFrac = tissueFractionForImage(im)
            hasTissue = tissueFrac >= 0.05
        except Exception:
            width, height = None, None
            tissueFrac = 0.0
            hasTissue = False

        # ---------------------------
        # Verificación de máscara
        # ---------------------------
        if os.path.exists(mask_path):
            mask_info = inspect_mask(mask_path)
            mask_exists = True

            # Guardar paleta si existe
            if "palette" in mask_info and mask_info["palette"] is not None:
                maskPalettes[mask_fname] = mask_info["palette"]
        else:
            mask_info = {"mode": None, "unique_values": None}
            mask_exists = False
            missingMasks += 1

        # ---------------------------
        # Registrar fila del manifest
        # ---------------------------
        rows.append({
            "patch_filename": fname,
            "mask_filename": mask_fname,
            "img_path": img_path,
            "mask_path": mask_path if mask_exists else None,
            "mask_exists": bool(mask_exists),

            "slide_id": slide_id,
            "patient_id": patient_id,

            "x_ini": x_ini,
            "y_ini": y_ini,
            "width": width,
            "height": height,

            "has_tissue": bool(hasTissue),
            "tissue_fraction": float(tissueFrac),

            "mask_mode": mask_info.get("mode"),
            "unique_values": mask_info.get("unique_values"),

            "Gleason_primary": gleason_p,
            "Gleason_secondary": gleason_s,

            "manifestVersion": "v1",
            "createdAt": pd.Timestamp.now().isoformat(),
            "gitCommitHash": get_git_hash()
        })

    # Convertir a DataFrame y guardar
    df_manifest = pd.DataFrame(rows)
    df_manifest.to_csv(MANIFEST_CSV, index=False)

    # Guardar las paletas de las máscaras
    with open(MASK_PALETTES_JSON, "w") as f:
        json.dump(maskPalettes, f, indent=2)

    print(f"\n✓ Manifest guardado en: {MANIFEST_CSV}")
    print(f"Total parches: {len(df_manifest)}")
    print(f"Masks faltantes: {missingMasks}")
    print(f"Paletas de máscara guardadas en: {MASK_PALETTES_JSON}")

# Vista previa
display(df_manifest.head())

"""## 9. Validación estadística del Manifest

### 9.1 Conteo por slide
"""

print(f"Total de parches en manifest: {len(df_manifest)}")

# ------------------------------------------------------------
# 1. Conteo por slide
# ------------------------------------------------------------
patches_por_slide = df_manifest.groupby("slide_id")["patch_filename"].count()

print("\nParches por slide (primeras filas):")
display(patches_por_slide.head())

# Histograma
plt.figure(figsize=(9,4))
sns.histplot(patches_por_slide, bins=40, kde=False)
plt.title("Distribución de parches por slide")
plt.xlabel("Número de parches")
plt.ylabel("Cantidad de slides")
plt.show()

"""### 9.2 Tissue Fraction"""

# ------------------------------------------------------------
# 2. Tissue Fraction
# ------------------------------------------------------------
plt.figure(figsize=(9,4))
sns.histplot(df_manifest["tissue_fraction"], bins=40, kde=True)
plt.title("Distribución del Tissue Fraction")
plt.xlabel("Tissue Fraction")
plt.ylabel("Frecuencia")
plt.show()

print("\nEstadísticas de tissue_fraction:")
print(df_manifest["tissue_fraction"].describe())

"""### 9.3 Existencia de máscaras"""

# ------------------------------------------------------------
# 3. Existencia de máscaras
# ------------------------------------------------------------
mask_counts = df_manifest["mask_exists"].value_counts()

print("\nMáscaras presentes vs faltantes:")
print(mask_counts)

missing_masks = int((df_manifest["mask_exists"] == False).sum())
print(f"\nTotal de máscaras faltantes: {missing_masks}")

"""### 9.4 Distribución de valores únicos de máscaras"""

# ------------------------------------------------------------
# 4. Distribución de valores únicos de máscaras
# ------------------------------------------------------------
mask_value_dist = df_manifest["unique_values"].dropna().apply(lambda x: tuple(sorted(eval(str(x)))))

mask_value_counts = mask_value_dist.value_counts()

print("\nDistribución de 'unique_values' de las máscaras:")
display(mask_value_counts.head())

plt.figure(figsize=(10,4))
sns.barplot(x=list(range(len(mask_value_counts))), y=mask_value_counts.values)
plt.title("Distribución de combinaciones de valores en máscara")
plt.ylabel("Frecuencia")
plt.xlabel("Combinación ID")
plt.show()

"""### 9.5 Coherencia entre Gleason_primary y etiquetas dominantes"""

# ------------------------------------------------------------
# 5. Coherencia entre Gleason_primary y etiquetas dominantes
# ------------------------------------------------------------
print("\nAnalizando coherencia entre Gleason_primary y valores de máscara...")

coherencia = []

for slide, group in df_manifest.groupby("slide_id"):
    gleason_slide = group["Gleason_primary"].iloc[0]
    valores = []

    for uv in group["unique_values"].dropna():
        try:
            valores.extend(eval(str(uv)))
        except:
            pass

    if len(valores) > 0:
        dominante = Counter(valores).most_common(1)[0][0]
        coherente = (dominante == gleason_slide)
        coherencia.append((slide, dominante, gleason_slide, coherente))

df_coherencia = pd.DataFrame(coherencia,
                             columns=["slide_id","mask_dominante","gleason_primary","coherente"])

print("\nCoherencia general:")
print(df_coherencia["coherente"].value_counts())

"""### **Análisis Exploratorio del Manifest**

El análisis preliminar del manifest construido a partir del conjunto SICAPv2 confirma que la estructura general del dataset es consistente, completa y adecuada para continuar con las fases posteriores del pipeline MIL. A continuación se presenta una síntesis detallada de los hallazgos más relevantes.

---

#### **1. Distribución de parches por slide**

La distribución del número de parches por Whole Slide Image (WSI) presenta una variabilidad amplia, con valores que oscilan aproximadamente entre 10 y 350 parches por slide. Este comportamiento es esperable en datasets histopatológicos, ya que la cantidad de parches generados depende directamente:

* del área total de tejido disponible,
* de la proporción de regiones diagnósticas respecto a estroma o fondo,
* del tamaño del slide digitalizado,
* y del criterio de extracción utilizado.

El histograma evidencia que la mayoría de slides se concentran entre 80 y 180 parches, lo que representa una cantidad adecuada para un escenario MIL, donde se requiere un número razonable de instancias para modelar la heterogeneidad intra-slide.

No se observan outliers problemáticos; las láminas con menos de 50 parches fueron identificadas, pero no representan un riesgo metodológico inmediato mientras se mantengan controles posteriores en el Sprint 1

---

#### **2. Distribución del tissue_fraction**

La métrica *tissue_fraction*, utilizada como indicador de calidad para filtrar parches predominantemente vacíos, presenta:

* media ≈ 0.59
* rango ≈ [0.19 – 0.99]
* distribución relativamente uniforme entre 0.3 y 0.9

Esta distribución es coherente con lo esperado:
la mayoría de parches contienen una proporción significativa de tejido útil para análisis histológico.

Los valores bajos ($<$0.25) corresponden típicamente a:

* zonas con escaso tejido,
* áreas de fondo blanco,
* bordes de la lámina.

Estos parches serán evaluados y filtrados explícitamente en el Sprint 1, por lo que su presencia en el manifest original no constituye un problema.

---

#### 3. Verificación de máscaras y clases presentes

Los resultados muestran:

* **0 máscaras faltantes** a nivel de parche → excelente consistencia estructural.
* Combinaciones de clases presentes:
  - (0,) → 57%
  - (0,3,4) → 21%
  - (0,3) → 7%
  - (0,3,4,5) → 7%
  - (0,4,5) → 7%

Esta distribución concuerda con la diversidad clínica de los casos y con la naturaleza granular del tejido prostático, donde áreas benignas (label 0) pueden dominar extensiones completas de la lámina incluso en presencia de un tumor de alto grado.

Además, la presencia moderada de combinaciones múltiples indica que las máscaras capturan adecuadamente la heterogeneidad morfológica, lo cual es positivo para un enfoque MIL.

---

#### 4. Coherencia entre Gleason_primary y valores de la máscara

El análisis de consistencia reporta:

* 37 slides coherentes
* 118 slides “no coherentes” si se compara de forma estricta el valor dominante de la *máscara vs. el Gleason_primary.*

Este resultado es totalmente esperable y no representa un error.

### **Razones de la “incoherencia aparente”**

1. **Dominancia del tejido benigno (label 0):**
Un slide puede contener un pequeño foco tumoral de alto grado, pero la mayor parte del tejido sigue siendo benigno.
→ La etiqueta dominante NO representa la lesión crítica.

2. **Disparidad entre área dominante y área diagnóstica:**
El Gleason_primary proviene de la región más representativa desde el punto de vista clínico, no necesariamente la de mayor área anatómica.

3. **Regiones de interés muy pequeñas o dispersas:**
La granularidad del parche puede omitir parcialmente pequeñas áreas de alto grado cuando se hace un conteo simple de píxeles.

4. **Máscaras diseñadas para segmentación, no clasificación global del slide:**
Su rol no es etiquetar toda la lámina según su grado clínico, sino segmentar patrones presentes localmente.

#### Conclusiones:

La *“incoherencia”* cuantitativa es diagnósticamente plausible y no afecta el diseño del pipeline MIL, porque:

- El modelo MIL NO utiliza la etiqueta de la máscara para entrenar.
- El aprendizaje MIL se basa en supervisión débil a nivel de slide.
- La variabilidad intra-slide es parte del problema que MIL busca resolver

Además, todos los indicadores clave del Sprint 0 son satisfactorios:

- Completitud estructural (sin archivos faltantes).
- Consistencia semántica (etiquetas y metadatos coherentes).
- Variabilidad adecuada (parches y fracciones de tejido con buen rango).
- Material suficiente para MIL (≈18 mil parches distribuidos en 155 slides).
- Ausencia de errores críticos.

# **Sprint 1 — Preprocesamiento de Parches**

Este sprint aplica las transformaciones iniciales sobre los parches:

1. Filtrado por *tissue fraction*.
2. Normalización de color.
3. Generación del dataset procesado.
4. Registro en `dataset_manifest_processed.csv`.

Como normalizador seleccionamos **Reinhard**, debido a la incompatibilidad de Macenko/Vahadane (SPAMS) con Python 3.12 — restricción actual de Google Colab. Reinhard es un método estadístico robusto y ampliamente utilizado en histopatología cuando no se dispone de métodos basados en deconvolución óptica.

El objetivo del sprint es dejar los parches homogenizados, filtrados y listos para la extracción de *embeddings* en el Sprint 2.
"""

processingVersion = "v1"

# Directorios creados previamente en Sprint 0
os.makedirs(PROCESSED_BASE, exist_ok=True)
os.makedirs(PROCESSED_IMAGES, exist_ok=True)

examplesOutDir = os.path.join(PROCESSED_BASE, 'examples')
os.makedirs(examplesOutDir, exist_ok=True)

print("=== Sprint 1 iniciado ===")

# Configuración de normalización
normMethod = 'reinhard'           # 'macenko', 'reinhard', 'none'
                                  # macenko deshabilitado por incompatibilidad
                                  # staintools NO funciona en Google Colab actual (Python 3.12)
minTissueFraction = 0.05          # filtro mínimo de tejido
templateSampleN = 50              # número de imágenes para crear template
batchSaveQuality = 95             # calidad JPEG
processLimit = None               # None = procesar todo
batchSaveQuality = 95
randomSeed = 4

print("SPRINT 1 configurado correctamente.")
print("Normalización seleccionada:", normMethod)
print("Parches procesados se guardarán en:", PROCESSED_IMAGES)

"""## 1. Funciones auxiliares"""

# ==================================================
# FUNCIONES AUXILIARES ESPECÍFICAS DEL SPRINT 1
# ==================================================

def reinhard_fallback(arr, template):
    """
    Normalización Reinhard manual (sin staintools).
    Ambas imágenes deben estar en RGB.
    Convierte a LAB, iguala medias y std, y vuelve a RGB.
    """
    try:
        arr_lab = cv2.cvtColor(arr, cv2.COLOR_RGB2LAB).astype(np.float32)
        tpl_lab = cv2.cvtColor(template, cv2.COLOR_RGB2LAB).astype(np.float32)

        mean_src, std_src = cv2.meanStdDev(arr_lab)
        mean_tpl, std_tpl = cv2.meanStdDev(tpl_lab)

        mean_src = mean_src.reshape(-1)
        std_src  = std_src.reshape(-1)
        mean_tpl = mean_tpl.reshape(-1)
        std_tpl  = std_tpl.reshape(-1)

        norm_lab = (arr_lab - mean_src) / (std_src + 1e-8)
        norm_lab = norm_lab * std_tpl + mean_tpl
        norm_lab = np.clip(norm_lab, 0, 255).astype(np.uint8)

        return cv2.cvtColor(norm_lab, cv2.COLOR_LAB2RGB)

    except Exception as e:
        return arr  # fallback seguro

def build_reinhard_template(df, sampleN=50):
    """
    Construye un template Reinhard seleccionando sampleN parches con suficiente tejido.
    """
    candidates = df.sample(min(sampleN * 10, len(df)), random_state=randomSeed)
    selected = []

    for _, r in candidates.iterrows():
        try:
            img = Image.open(os.path.join(IMAGES_DIR, r["patch_filename"])).convert("RGB")
            if tissueFractionForImage(img) >= minTissueFraction:
                selected.append(np.array(img))
        except:
            pass

        if len(selected) >= sampleN:
            break

    if len(selected) == 0:
        print("⚠ No se pudo construir template. Se usará normMethod='none'")
        return None

    template = np.median(np.stack(selected, axis=0), axis=0).astype(np.uint8)
    print("✓ Template Reinhard construido correctamente.")
    return template

def apply_normalization(arr, method, template):
    if method == "none":
        return arr
    if method == "reinhard":
        return reinhard_fallback(arr, template)
    return arr

"""## 2. Construcción del template"""

templateImg = None
if normMethod == "reinhard":
    templateImg = build_reinhard_template(df_manifest, sampleN=templateSampleN)

print("Template:", "cargado" if templateImg is not None else "None (sin normalización)")

"""## 3. Procesamiento de los parches"""

# =================================================
# PROCESAMIENTO DE PARCHES
# =================================================

processed_rows = []

limit = processLimit if processLimit is not None else len(df_manifest)
print(f"Procesando {limit} parches...")

for idx, r in tqdm(df_manifest.head(limit).iterrows(), total=limit):

    if not r["has_tissue"]:
        continue  # se descartan parches sin tejido

    img_path = os.path.join(IMAGES_DIR, r["patch_filename"])

    try:
        img = Image.open(img_path).convert("RGB")
        arr = np.array(img)
    except:
        continue

    # Normalización
    arr_norm = apply_normalization(arr, normMethod, templateImg)

    # Guardado
    out_path = os.path.join(PROCESSED_IMAGES, r["patch_filename"])
    Image.fromarray(arr_norm).save(out_path, quality=batchSaveQuality)

    # Registrar en manifest procesado
    processed_rows.append({
        "patch_filename": r["patch_filename"],
        "slide_id": r["slide_id"],
        "patient_id": r["patient_id"],
        "x_ini": r["x_ini"],
        "y_ini": r["y_ini"],
        "tissue_fraction": r["tissue_fraction"],
        "mask_path": r["mask_path"],
        "Gleason_primary": r["Gleason_primary"],
        "Gleason_secondary": r["Gleason_secondary"]
    })

print("✓ Procesamiento finalizado.")

processed_manifest = pd.DataFrame(processed_rows)
processed_manifest.to_csv(PROCESSED_MANIFEST, index=False)

print("Total parches procesados:", len(processed_manifest))
print("Manifest guardado en:", PROCESSED_MANIFEST)

# =================================================
# GENERACIÓN Y GUARDADO DE EJEMPLOS VISUALES EN /processed_patches/v1/examples
# =================================================

print("Generando ejemplos visuales para QC...")

qcSampleN = 5  # número de ejemplos a guardar
qcRandomSeed = 42

np.random.seed(qcRandomSeed)

# Seleccionar parches aleatorios procesados
qc_rows = processed_manifest.sample(
    min(qcSampleN, len(processed_manifest)),
    random_state=qcRandomSeed
)

for i, (_, row) in enumerate(qc_rows.iterrows(), start=1):
    fname = row["patch_filename"]

    orig_path = os.path.join(IMAGES_DIR, fname)
    proc_path = os.path.join(PROCESSED_IMAGES, fname)

    if not os.path.exists(orig_path) or not os.path.exists(proc_path):
        continue

    orig = Image.open(orig_path).convert("RGB")
    proc = Image.open(proc_path).convert("RGB")

    # Crear figura comparativa
    fig, axs = plt.subplots(1, 2, figsize=(8, 4))

    axs[0].imshow(orig)
    axs[0].set_title("Original", fontsize=12)
    axs[0].axis("off")

    axs[1].imshow(proc)
    axs[1].set_title("Procesada (Reinhard)", fontsize=12)
    axs[1].axis("off")

    plt.tight_layout()

    out_path = os.path.join(examplesOutDir, f"qc_example_{i}.png")
    plt.savefig(out_path, dpi=200, bbox_inches="tight")
    plt.close(fig)

print(f"✓ {len(qc_rows)} ejemplos QC guardados en:", examplesOutDir)

"""## 4. QC Visual y Validaciones"""

# QC ESTADÍSTICO

if "tissue_fraction" in processed_manifest.columns:
    tf_col = "tissue_fraction"
else:
    tf_col = None

if tf_col:
    print("\nEstadísticas tissue_fraction:")
    print(processed_manifest[tf_col].describe())

    plt.figure(figsize=(8,4))
    sns.histplot(processed_manifest[tf_col], bins=40, kde=True)
    plt.title("Distribución tissue_fraction (post filtrado)")
    plt.show()

examples_dir = os.path.join(PROCESSED_BASE, "examples")

example_files = []
if os.path.exists(examples_dir):
    example_files = sorted([
        os.path.join(examples_dir, f)
        for f in os.listdir(examples_dir)
        if f.lower().endswith((".jpg", ".png"))
    ])

if len(example_files) == 0:
    print("\nℹ️ No existen ejemplos visuales en /examples (opcional).")
else:
    print(f"\nMostrando {min(5, len(example_files))} ejemplos visuales...\n")
    n = min(5, len(example_files))

    fig, axs = plt.subplots(n, 1, figsize=(6, 3*n))

    if n == 1:
        axs = [axs]

    for i in range(n):
        img = Image.open(example_files[i])
        axs[i].imshow(img)
        axs[i].axis("off")
        axs[i].set_title(f"Ejemplo {i+1}")

    plt.tight_layout()
    plt.show()

ls /content/sicapv2_data/processed_patches/v1/examples

print("QC Visual — Comparación original vs normalizado (solo visualización)")

# Selección aleatoria de 5 parches procesados
sample_n = 5
sample_rows = processed_manifest.sample(sample_n, random_state=42)

fig, axs = plt.subplots(sample_n, 2, figsize=(12, 4 * sample_n))

if sample_n == 1:
    axs = [axs]

for i, (_, row) in enumerate(sample_rows.iterrows()):
    patch_name = row["patch_filename"]
    slide_id = row["slide_id"]

    # Rutas
    original_path = os.path.join(IMAGES_DIR, patch_name)
    processed_path = os.path.join(PROCESSED_IMAGES, patch_name)

    # Cargar imágenes
    orig = Image.open(original_path)
    proc = Image.open(processed_path)

    # Mostrar original
    axs[i][0].imshow(orig)
    axs[i][0].axis("off")
    axs[i][0].set_title(f"Original (slide {slide_id})", fontsize=14)

    # Mostrar normalizada
    axs[i][1].imshow(proc)
    axs[i][1].axis("off")
    axs[i][1].set_title("Normalizado (Reinhard)", fontsize=14)

plt.tight_layout()

out_fig_path = os.path.join(PROCESSED_BASE, "qc_original_vs_normalized.png")
plt.savefig(out_fig_path, dpi=300, bbox_inches="tight")

plt.show()


print("Figura QC guardada en:", out_fig_path)

"""El Sprint 1 tuvo como objetivo aplicar el preprocesamiento inicial sobre los parches del dataset SICAPv2, incluyendo filtrado por fracción de tejido, normalización cromática y generación del conjunto de datos procesado para la posterior extracción de *embeddings* en el Sprint 2. A continuación, se presenta el análisis de los resultados obtenidos.

---

## **1. Filtrado por Tissue Fraction**

El objetivo de este filtro era eliminar parches sin tejido significativo.  
El umbral empleado fue `minTissueFraction = 0.05`, lo cual es estándar en histopatología digital.

**Resultados:**

- Total de parches procesados: **18 783**
- Ningún parche fue eliminado por falta de tejido.
- Esto confirma que el dataset original está bien construido y no contiene regiones blancas o vacías.

Las estadísticas finales de `tissue_fraction` fueron:

- **Media:** 0.589  
- **Mediana:** 0.582  
- **Mínimo:** 0.194  
- **Máximo:** 0.999  

El histograma muestra una distribución amplia y equilibrada de fracción tisular, lo cual indica una buena representatividad de regiones glandulares, estroma y tejidos mixtos, coherente con la variabilidad típica en muestras de próstata.

> **Conclusión:** El filtrado es correcto y respalda la calidad de los parches disponibles.
---

## **2. Normalización Cromática (Reinhard)**

Dado que Google Colab actualmente utiliza Python 3.12, no es posible usar Macenko o Vahadane, pues el paquete SPAMS no es compatible. Se empleó entonces el método **Reinhard**, el cual:

- Estándar en histopatología cuando no se puede realizar deconvolución óptica.
- Reduce variabilidad entre parches.
- Mantiene las estructuras tisulares intactas.
- Es robusto a diferencias de escáner, coloración y lote.

Durante el QC visual, los parches normalizados mostraron:

- Mayor homogeneidad de color.  
- Tono rosado más uniforme.  
- Reducción de variación entre parches de distintos slides.  
- Conservación completa de la morfología tisular.

> **Conclusión:** La normalización produjo resultados válidos y clínicamente consistentes, adecuados para la extracción de *embeddings* con ResNet50.

---

## **3. Generación del dataset procesado**

El sprint generó:

- El directorio: `/content/sicapv2_data/processed_patches`
- El archivo: `/content/sicapv2_data/dataset_manifest.csv`
- Los parches procesados listos para su uso en el Sprint 2.

Cada entrada conserva:

- Identidad del parche.
- Coordenadas dentro del WSI.
- Gleason primario/secundario.
- Fracción de tejido.
- Ruta a máscara y atributos clínicos.

> **Conclusión:** El dataset procesado mantiene la trazabilidad completa requerida para un pipeline MIL reproducible.

---

## **4. QC Visual (Modo A)**

Se visualizaron 5 parches antes y después de la normalización. Los resultados mostraron:

- Correspondencia uno a uno con la imagen original.
- Uniformidad cromática post-normalización.
- Ausencia de artefactos, distorsiones o pérdida de información.

Lo que se observa es exactamente lo esperado cuando se aplica normalización tipo *Reinhard*:

* La imagen procesada tiene un tono más rosado y homogéneo.
* Disminuye la variabilidad entre parches (clave para Sprint 2 → ResNet embeddings).
* Se suavizan diferencias de escáner, iluminación y tinción.
* Previene sobreajuste por diferencias de color entre slides.

- No se está viendo pérdida de información crítica.
- La estructura tisular se mantiene.
- La colorimetría se hace consistente entre parches.

# **Sprint 2 — Extracción de Embeddings con ResNet50**

Extracción de vectores de características (dim=2048) desde parches procesados.
Características:
- Soporta `dev` / `test` / `full` vía parámetros `start_index` y `limit`.
- Guardado incremental del `embeddings_manifest.csv` para **reanudado**.
- DataLoader + batching eficiente.
- Logs de tiempo por batch y metadata final.
"""

#from torch.utils.data import Dataset, DataLoader
#import torchvision.transforms as T
#import torchvision.models as models

# torchvision puede no haber sido importado en la celda base
from torchvision import models, transforms as T

processed_manifest_path = PROCESSED_MANIFEST   # /content/sicapv2_data/processed_patches/v1/dataset_manifest_processed.csv
images_dir = PROCESSED_IMAGES                  # /content/sicapv2_data/processed_patches/v1/images
embeddings_dir = EMBEDDINGS_DIR                # /content/sicapv2_data/processed_patches/v1/embeddings

os.makedirs(embeddings_dir, exist_ok=True)

# intento de batch sizes (de mayor a menor). Se prueban en orden hasta encontrar uno estable.
candidate_batch_sizes = [256, 128, 64, 48, 32, 24, 16, 12, 8, 4, 2, 1]

# checkpointing / flush
manifest_filename = os.path.join(embeddings_dir, "embeddings_manifest.csv")
metadata_filename = os.path.join(embeddings_dir, "embeddings_metadata.json")
flush_every = 1000   # escribir manifest parcial cada N embeddings

device = device if 'device' in globals() else (torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu"))
print("Device a usar:", device)

# Cargar modelo ResNet50 sin la capa de clasificación (global average pooling output 2048)
print("Cargando ResNet50 preentrenada (sin capa final)...")
resnet = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)
# quitar la capa fc final -> tomar hasta el avgpool
feature_extractor = torch.nn.Sequential(*list(resnet.children())[:-1]).to(device)
feature_extractor.eval()

# transformación de entrada
transform = T.Compose([
    T.Resize((224,224)),
    T.ToTensor(),
    T.Normalize(mean=[0.485, 0.456, 0.406],
                std=[0.229, 0.224, 0.225])
])

# Cargar manifest procesado (generado en Sprint 1)
manifest_df = pd.read_csv(processed_manifest_path)
# filtramos por parches que realmente se guardaron (si existe columna 'normalizedPath' usarla); si no, usar patch_filename
# asumimos que processed images se guardaron con mismo nombre en images_dir
total_candidates = processed_manifest.shape[0]
print(f"Parches candidatos (manifest procesado): {total_candidates}")

# Preparar lista de archivos a procesar (skip si emb ya existe)
all_patch_filenames = processed_manifest['patch_filename'].tolist()

# si existe embeddings_manifest, reanudar
if os.path.exists(manifest_filename):
    existing = pd.read_csv(manifest_filename)
    processed_set = set(existing['patch_filename'].tolist())
    manifest_rows = existing.to_dict(orient='records')
    print(f"Reanudando: {len(processed_set)} embeddings ya existentes (se reanudarán).")
else:
    processed_set = set()
    manifest_rows = []

# Guardar manifest incremental
def flush_manifest(rows, path=manifest_filename):
    df = pd.DataFrame(rows)
    df.to_csv(path, index=False)

# Intento de inferencia para determinar batch size estable
def test_batch_size(batch_size, sample_paths):
    """
    Intenta procesar un batch de ejemplo con el batch_size propuesto.
    Devuelve True si va bien, False si provoca OOM.
    """
    import torch
    try:
        imgs = []
        for p in sample_paths[:batch_size]:
            img = Image.open(p).convert('RGB')
            imgs.append(transform(img))
        if len(imgs) == 0:
            return False
        imgs_t = torch.stack(imgs).to(device)
        with torch.no_grad():
            _ = feature_extractor(imgs_t).cpu().numpy()
        # liberar
        del imgs_t
        torch.cuda.empty_cache()
        return True
    except RuntimeError as e:
        if 'out of memory' in str(e).lower():
            torch.cuda.empty_cache()
            return False
        else:
            # otros errores -> fallback: consider it failed
            try:
                torch.cuda.empty_cache()
            except:
                pass
            return False
    except Exception:
        try:
            torch.cuda.empty_cache()
        except:
            pass
        return False

# Construir lista de paths en disco (full paths)
all_input_paths = [os.path.join(images_dir, fn) for fn in all_patch_filenames]

# Obtener muestra pequeña para tests
sample_existing_paths = [p for p in all_input_paths if os.path.exists(p)]
if len(sample_existing_paths) == 0:
    raise RuntimeError(f"No se encontraron imágenes procesadas en {images_dir}. Ejecuta Sprint 1 primero.")

# Determinar batch size adaptativo
chosen_batch = None
print("Determinando batch size óptimo (pruebas)...")
for b in candidate_batch_sizes:
    ok = test_batch_size(b, sample_existing_paths[:min(256, len(sample_existing_paths))])
    print(f"  prueba batch={b}: {'OK' if ok else 'OOM/ERROR'}")
    if ok:
        chosen_batch = b
        break

if chosen_batch is None:
    # fallback conservador
    chosen_batch = 1
    print("Ningún batch grande funcionó. Usando batch=1 (fallback).")
else:
    print(f"Batch seleccionado: {chosen_batch}")

# Proceso principal: extracción en batches
start_time = time.time()
rows_buffer = []
counter = 0
errors = 0

# Crear orden reproducible
to_process = [fn for fn in all_patch_filenames if fn not in processed_set]
print(f"Parches a procesar (no existentes): {len(to_process)}")

# Función para procesar un bloque de filenames
import torch
from PIL import Image

def process_block(filenames_block):
    imgs_tensors = []
    names = []
    for fname in filenames_block:
        img_path = os.path.join(images_dir, fname)
        if not os.path.exists(img_path):
            continue
        try:
            img = Image.open(img_path).convert('RGB')
            imgs_tensors.append(transform(img))
            names.append(fname)
        except Exception as e:
            # saltar archivo con problema
            continue
    if len(imgs_tensors) == 0:
        return []
    batch_tensor = torch.stack(imgs_tensors).to(device)
    with torch.no_grad():
        feats = feature_extractor(batch_tensor)           # shape (N, 2048, 1, 1)
        feats = feats.squeeze(-1).squeeze(-1).cpu().numpy()  # (N,2048)
    # liberar
    del batch_tensor
    torch.cuda.empty_cache()
    out = []
    for name, emb in zip(names, feats):
        emb_path = os.path.join(embeddings_dir, name.replace('.jpg', '.npy').replace('.jpeg', '.npy').replace('.png','.npy'))
        try:
            np.save(emb_path, emb.astype(np.float32))
        except Exception as e:
            # intentar crear directorio y reintentar
            os.makedirs(os.path.dirname(emb_path), exist_ok=True)
            np.save(emb_path, emb.astype(np.float32))
        out.append({
            'patch_filename': name,
            'embedding_path': emb_path,
            'embedding_dim': int(emb.shape[0])
        })
    return out

# Loop principal, en bloques del batch elegido, con reintentos si hay OOM
i = 0
n = len(to_process)
pbar = tqdm(total=n, desc="Extrayendo embeddings", unit="patches")
while i < n:
    block = to_process[i : i + chosen_batch]
    try:
        block_rows = process_block(block)
        # append results
        for r in block_rows:
            manifest_rows.append(r)
            rows_buffer.append(r)
            processed_set.add(r['patch_filename'])
            counter += 1
            pbar.update(1)
        # si no se generaron filas (p.e. todos los archivos faltaban), avanzar el puntero en n
        if len(block_rows) == 0:
            # marcar avance por archivos comprobados
            pbar.update(len(block))
        # flush periódico
        if len(rows_buffer) >= flush_every:
            flush_manifest(manifest_rows)
            rows_buffer = []
    except RuntimeError as e:
        # manejo OOM: reducir batch y reintentar
        msg = str(e).lower()
        if 'out of memory' in msg:
            torch.cuda.empty_cache()
            # reducir batch
            prev = chosen_batch
            # elegir siguiente batch menor disponible
            idx = candidate_batch_sizes.index(chosen_batch) if chosen_batch in candidate_batch_sizes else len(candidate_batch_sizes)-1
            if idx < len(candidate_batch_sizes)-1:
                chosen_batch = candidate_batch_sizes[idx+1]
            else:
                chosen_batch = max(1, chosen_batch//2)
            print(f"\nOOM detectado. Reduciendo batch {prev} -> {chosen_batch}. Reintentando bloque actual.")
            # small sleep para estabilidad
            time.sleep(2.0)
            continue
        else:
            # otro runtime error: registrar y saltar
            errors += 1
            print("RuntimeError no OOM:", e)
            i += chosen_batch
            continue
    except Exception as e:
        errors += 1
        print("Error no esperado:", e)
        i += chosen_batch
        continue
    # avanzar
    i += chosen_batch

pbar.close()

# flush final
if len(rows_buffer) > 0 or not os.path.exists(manifest_filename):
    flush_manifest(manifest_rows)

elapsed_min = (time.time() - start_time) / 60.0

# metadata y resumen
embedding_dims = None
if len(manifest_rows) > 0:
    embedding_dims = int(manifest_rows[0]['embedding_dim'])

meta = {
    'model': 'ResNet50',
    'pretrained_dataset': 'ImageNet1k_v2',
    'embedding_dim': embedding_dims,
    'total_manifest_patches': int(total_candidates),
    'total_embeddings_generated': int(len(manifest_rows)),
    'device': str(device),
    'elapsed_minutes': round(elapsed_min, 2),
    'chosen_initial_batch': chosen_batch,
    'errors': int(errors),
    'timestamp': time.strftime('%Y-%m-%dT%H:%M:%S')
}

with open(metadata_filename, 'w') as f:
    json.dump(meta, f, indent=2)

print("\n✅ Extracción de embeddings finalizada.")
print("Resumen:")
print(json.dumps(meta, indent=2))
print("Embeddings manifest:", manifest_filename)
print("Embeddings dir:", embeddings_dir)

#!ls /content/sicapv2_data/processed_patches/v1/embeddings/

"""## Resultados

En este sprint se realizó la extracción de embeddings utilizando ResNet50 preentrenada en ImageNet, con el objetivo de transformar cada parche histopatológico en un vector representativo de 2048 dimensiones. Este proceso constituye la base del pipeline MIL, puesto que los modelos posteriores operarán directamente sobre estos vectores para la agregación a nivel de WSI.

---

### 1. Rendimiento Computacional

El proceso completó la extracción de características para 18.783 parches, alcanzando un rendimiento sobresaliente en Google Colab:

- Tiempo total: ~6.35 minutos
- Batch size óptimo detectado: 256
- Procesamiento promedio: ~49 parches/segundo
- Errores durante la extracción: 0

La selección automática de un batch de 256 confirma que la GPU asignada dispone de suficiente memoria y que el pipeline es eficiente en la precarga y normalización de imágenes.

La ausencia total de errores indica que tanto el manifest del Sprint 1 como la estructura del conjunto de datos fueron construidos sin inconsistencias.

---

### 2. Calidad y consistencia de los embeddings

Cada parche fue convertido en un vector 2048-dimensional, correspondiente a la salida de la capa avgpool de ResNet50. Este tamaño es el estándar en la literatura para tareas MIL basadas en transfer learning y asegura:

- Representaciones ricas y diferenciables
- Reducción significativa del costo de entrenamiento
- Compatibilidad con mecanismos de atención, pooling jerárquico y arquitecturas MIL modernas

La carpeta de salida contiene un archivo .npy por parche y los metadatos asociados (`embeddings_manifest.csv` y `embeddings_metadata.json`)

# **Sprint 3 — Construcción de Bags MIL**

**Objetivo:** agrupar embeddings por `slide_id` en bags reproducibles y optimizados para entrenamiento MIL; guardar artefactos (CSV, PKL, NPZ), generar splits por paciente (GroupKFold) y realizar validaciones básicas.

## 3.1 Cargar manifests y preparar tablas
"""

assert os.path.exists(PROCESSED_MANIFEST), f"Processed manifest no existe: {PROCESSED_MANIFEST}"
assert os.path.exists(EMBEDDINGS_DIR), f"Embeddings dir no existe: {EMBEDDINGS_DIR}"
os.makedirs(MIL_OUTPUT_DIR, exist_ok=True)

# Cargar manifest procesado y manifest de embeddings (si existe)
processed_manifest
emb_manifest_path = os.path.join(EMBEDDINGS_DIR, "embeddings_manifest.csv")

if os.path.exists(emb_manifest_path):
    df_emb = pd.read_csv(emb_manifest_path)
else:
    # Si embeddings manifest no existe, construimos tabla a partir de archivos .npy en EMBEDDINGS_DIR
    emb_files = sorted([f for f in os.listdir(EMBEDDINGS_DIR) if f.lower().endswith('.npy')])
    df_emb = pd.DataFrame({
        'patch_filename': [f.replace('.npy', '.jpg') for f in emb_files],
        'embedding_path': [os.path.join(EMBEDDINGS_DIR, f) for f in emb_files],
        'embedding_dim': [None]*len(emb_files)
    })

# Normalizar nombres (por si acaso)
processed_manifest['patch_filename'] = processed_manifest['patch_filename'].astype(str)
df_emb['patch_filename'] = df_emb['patch_filename'].astype(str)

# Hacer join: manifest procesado (info clínica y tissue_fraction) + embedding_path
df_merged = processed_manifest.merge(df_emb, on='patch_filename', how='left', validate='1:m')

print("Registros procesados:", len(processed_manifest))
print("Embeddings registrados:", len(df_emb))
print("Registros merged:", len(df_merged))

# Filtrar solo los parches con embedding existente
has_emb_mask = df_merged['embedding_path'].notnull()
print("Parches con embedding disponible:", int(has_emb_mask.sum()))
df_merged = df_merged[has_emb_mask].reset_index(drop=True)

"""## 3.2 — Parámetros de limpieza / muestreo"""

# 3.2 Parámetros de limpieza y muestreo (decisión óptima)
min_instances = 5       # eliminar slides con < 5 instancias (evita bags inútiles)
max_instances = 300     # cap por slide (submuestreo por tissue_fraction)
random_state = int(RND) if 'RND' in globals() else 42

print("Parámetros: min_instances =", min_instances, ", max_instances =", max_instances)

"""## Definición de Problema Binario

- Clasificación binaria a nivel de slide (WSI)
- Clase 0: Gleason 0 (benigno)
- Clase 1: Gleason >= 3 (cáncer)
"""

def binarize_gleason(gleason_primary):
    if gleason_primary is None:
        return None
    try:
        g = int(gleason_primary)
        return 0 if g == 0 else 1
    except Exception:
        return None

"""## 3.3 — Construir bags por slide"""

bags = {}
slides_grouped = df_merged.groupby("slide_id")

for slide_id, group in tqdm(slides_grouped, total=slides_grouped.ngroups, desc="Construyendo bags"):
    # Ordenar por tissue_fraction (desc) para preferir parches con más tejido en el muestreo
    group_sorted = group.sort_values(by=['tissue_fraction'] if 'tissue_fraction' in group.columns else [], ascending=False)

    # Selección / muestreo si supera max_instances
    if len(group_sorted) > max_instances:
        # tomar top-N por tissue_fraction para mantener calidad
        sel = group_sorted.head(max_instances)
    else:
        sel = group_sorted

    # Si queda menos que min_instances, lo marcamos para posible eliminación
    if len(sel) < min_instances:
        continue  # descartamos esta slide (muy pocos parches)

    # Construir bag dict
    paths = sel['embedding_path'].tolist()
    coords = []
    if 'x_ini' in sel.columns and 'y_ini' in sel.columns:
        coords = list(zip(sel['x_ini'].fillna(-1).astype(int).tolist(),
                          sel['y_ini'].fillna(-1).astype(int).tolist()))
    else:
        coords = [(-1, -1)] * len(paths)

    label_raw = None
    if 'Gleason_primary' in sel.columns:
        label_raw = sel['Gleason_primary'].iloc[0]

    # Mapear a 0..3 si corresponde (usa map_label_to_class si existe)
    try:
        label = binarize_gleason(label_raw) if label_raw is not None else None
        print(label)
    except Exception:
        label = None

    patient_id = str(sel['patient_id'].iloc[0]) if 'patient_id' in sel.columns else None

    bags[slide_id] = {
        "slide_id": slide_id,
        "patient_id": patient_id,
        "label": int(label) if label is not None else None,
        "paths": paths,
        "coords": np.array(coords, dtype=int),
        "num_instances": len(paths)
    }


print("\nBags construidos (después limpieza):", len(bags))

"""Ver el contenido completo de un bag, primero necesitamos conocer un `slide_id` válido. Podemos obtener uno de las claves del diccionario `bags`."""

import pprint

# Obtener el primer slide_id del diccionario 'bags' para usarlo como ejemplo
example_slide_id = list(bags.keys())[0]

# Imprimir el contenido completo del bag para ese slide_id
print(f"Contenido del bag para el slide_id: {example_slide_id}")
pprint.pprint(bags[example_slide_id].keys())

# custom_slide_id = '16B0003388' # Reemplaza con un ID existente
# print(f"\nContenido del bag para el slide_id: {custom_slide_id}")
# pprint.pprint(bags[custom_slide_id])

"""## 3.4 — Exportar artifacts: CSV, PKL, NPZ, metadata"""

# 3.4 Exportar artifacts
os.makedirs(MIL_OUTPUT_DIR, exist_ok=True)
bags_index_rows = []

bags_npz_dir = os.path.join(MIL_OUTPUT_DIR, "bags_npz")
os.makedirs(bags_npz_dir, exist_ok=True)

for sid, d in bags.items():
    bags_index_rows.append({
        "slide_id": sid,
        "patient_id": d["patient_id"],
        "label": d["label"],
        "num_instances": d["num_instances"]
    })
    # Guardar npz por slide (paths son strings; coords array)
    npz_path = os.path.join(bags_npz_dir, f"{sid}.npz")
    np.savez_compressed(npz_path,
                        paths=np.array(d["paths"], dtype=object),
                        coords=d["coords"],
                        label=int(d["label"]) if d["label"] is not None else -1,
                        patient_id=d["patient_id"])

# Index CSV
bags_index_df = pd.DataFrame(bags_index_rows).sort_values(by='slide_id')
bags_index_csv = os.path.join(MIL_OUTPUT_DIR, "bags_index.csv")
bags_index_df.to_csv(bags_index_csv, index=False)

import pickle

# PKL completo
with open(os.path.join(MIL_OUTPUT_DIR, "bags_index.pkl"), "wb") as f:
    pickle.dump(bags, f)

# Metadata
metadata = {
    "num_slides": len(bags),
    "total_instances": int(sum(d["num_instances"] for d in bags.values())),
    "min_instances_per_bag": int(min(d["num_instances"] for d in bags.values())),
    "max_instances_per_bag": int(max(d["num_instances"] for d in bags.values())),
    "task_type": "binary_classification",
    "positive_class_definition": "Gleason >= 3",
    "negative_class_definition": "Gleason 0",
    "embedding_dir": EMBEDDINGS_DIR,
    "processed_manifest": PROCESSED_MANIFEST,
    "random_state": random_state,
}

with open(os.path.join(MIL_OUTPUT_DIR, "dataset_metadata.json"), "w") as f:
    json.dump(metadata, f, indent=2)

print("✓ Artifacts guardados en:", MIL_OUTPUT_DIR)
print(" - bags_index.csv:", bags_index_csv)
print(" - bags_npz_dir:", bags_npz_dir)
print(" - dataset_metadata.json:", os.path.join(MIL_OUTPUT_DIR, "dataset_metadata.json"))

"""## 3.5 — Validaciones y visualizaciones"""

# 3.5 Validaciones rápidas y visualizaciones
bag_sizes = [d["num_instances"] for d in bags.values()]
labels = [d["label"] for d in bags.values()]

plt.figure(figsize=(10,4))
plt.subplot(1,2,1)
sns.histplot(bag_sizes, bins=40, kde=False)
plt.title("Distribución de tamaño de bags (instancias por slide)")
plt.xlabel("Instancias por bag")
plt.ylabel("Número de slides")

plt.subplot(1,2,2)
sns.countplot(x=labels)
plt.xlabel("Clase binaria (0=Benigno, 1=Cáncer)")
plt.title("Distribución de clases binarias a nivel de bag")
plt.tight_layout()
plt.show()

print(bag_sizes)

# Estadísticas numéricas
print("Bag stats — slides:", len(bag_sizes))
print("Instances total:", sum(bag_sizes))
print("Min / Q1 / Median / Q3 / Max:", np.min(bag_sizes), np.percentile(bag_sizes,25),
      np.median(bag_sizes), np.percentile(bag_sizes,75), np.max(bag_sizes))

"""## 3.6 — GroupKFold splits por paciente"""

# 3.6 Crear splits (GroupKFold por patient_id)
from sklearn.model_selection import GroupKFold

slides = list(bags.keys())
labels_list = [bags[s]["label"] for s in slides]
patients = [bags[s]["patient_id"] for s in slides]

gkf = GroupKFold(n_splits=5)
splits = []
for fold, (train_idx, val_idx) in enumerate(gkf.split(slides, labels_list, groups=patients)):
    splits.append({
        "fold": fold,
        "train": [slides[i] for i in train_idx],
        "val": [slides[i] for i in val_idx]
    })

with open(os.path.join(MIL_OUTPUT_DIR, "cv_splits_groupkfold_patient.json"), "w") as f:
    json.dump(splits, f, indent=2)

print("✓ CV splits (GroupKFold) guardados en:", os.path.join(MIL_OUTPUT_DIR, "cv_splits_groupkfold_patient.json"))

"""## Cierre del Sprint — Construcción de Bags MIL y Preparación Experimental

En este sprint se consolidó el pipeline de preparación de datos necesario para el entrenamiento de modelos de Aprendizaje con Múltiples Instancias (MIL) sobre imágenes histopatológicas prostáticas de la base SICAPv2. A partir de los parches previamente preprocesados y filtrados por contenido tisular, se realizó la agrupación de instancias a nivel de Whole Slide Image (WSI), utilizando el identificador `slide_id` como unidad fundamental para la construcción de los bags y la asignación de etiquetas débiles a nivel global.

El conjunto final está compuesto por **155 WSIs**, con un total de **18 666 instancias**, presentando una variabilidad natural en el número de parches por slide. El tamaño de los bags oscila entre **15 y 300 instancias**, con una concentración mayoritaria en rangos intermedios, lo cual resulta consistente con la heterogeneidad morfológica del tejido prostático y con la cobertura tisular efectiva de cada WSI. Esta variabilidad es adecuada para el paradigma MIL, el cual no requiere un número fijo de instancias por bag y permite que el modelo aprenda a ponderar la relevancia de cada parche durante el proceso de agregación.

El análisis de la distribución de clases a nivel de bag evidencia un desbalance moderado entre las categorías, con predominancia de clases intermedias y menor representación de patrones más agresivos. Este comportamiento refleja escenarios clínicos reales y justifica el uso de métricas de evaluación robustas (AUC, F1-score y coeficiente Kappa) en las etapas posteriores, más allá de la precisión global.

Con el fin de garantizar una evaluación rigurosa y evitar fuga de información, se definió un esquema de validación cruzada basado en **GroupKFold estratificado por paciente**, asegurando que todas las instancias de un mismo paciente se encuentren contenidas exclusivamente en un único fold. Los splits fueron almacenados en formato reproducible, permitiendo trazabilidad experimental y comparaciones consistentes entre arquitecturas.

Finalmente, se generaron artefactos clave del dataset, incluyendo el índice de bags, metadatos globales y la configuración de umbrales utilizados, estableciendo una base sólida y versionada para el entrenamiento y evaluación de modelos MIL en los siguientes sprints. Este cierre garantiza que el pipeline de datos cumple con los requisitos metodológicos y experimentales planteados en la formulación del problema y los objetivos del proyecto.

# **Sprint 4 — Extracción de Embeddings**

El objetivo de este sprint es implementar y validar un modelo base de Multiple Instance Learning (MIL) con mecanismo de atención para clasificación binaria a nivel de Whole Slide Image (WSI)

- Clase 0 → Benigno (Gleason 0)
- Clase 1 → Cáncer (Gleason ≥ 3)

Usando:

- Bags construidos en Sprint 3
- Validación cruzada GroupKFold por paciente
- Salida binaria con sigmoide implícita
- Métrica principal: F1-score

Y asegurando:

- Correcta formulación del problema MIL (una bolsa = una WSI)
- Independencia estricta entre folds de validación cruzada
- Manejo adecuado de bolsas con número variable de instancias
- Evaluación honesta del desempeño **sin calibración del umbral ni compensación del desbalance**

Cambio conceptual clave (muy importante)
Antes (multiclase)	    Ahora (binario)
Softmax	                Sigmoide
CrossEntropyLoss	      BCEWithLogitsLoss
logits ∈ ℝᴷ	            logit ∈ ℝ
argmax	                sigmoid + threshold
"""

assert "bags" in globals(), "❌ 'bags' no existe"
assert "MIL_OUTPUT_DIR" in globals(), "❌ 'MIL_OUTPUT_DIR' no existe"

# Cargar splits de validación cruzada (GroupKFold por paciente)
if "cv_splits" not in globals():
    cv_path = os.path.join(MIL_OUTPUT_DIR, "cv_splits_groupkfold_patient.json")
    with open(cv_path, "r") as f:
        cv_splits = json.load(f)

print("✓ Entorno validado correctamente")
print(f"✓ Dispositivo: {device}")
print("Bags cargados:", len(bags))
print(f"✓ Número de folds: {len(cv_splits)}")

import torch.nn.functional as F

"""## 4.1 Dataset MIL

Cada muestra del dataset corresponde a **una WSI completa**, representada como una bolsa de embeddings extraídos de sus patches.

Características clave:

- Cada bolsa contiene un número **variable** de instancias
- La etiqueta está definida a nivel de slide
- Debido a la variabilidad en el tamaño de las bolsas, el entrenamiento se realiza con `batch_size = 1`, lo cual es consistente con la formulación clásica de Multiple Instance Learning
"""

class MILDataset(Dataset):
    def __init__(self, slideIds, bags):
        self.slideIds = slideIds
        self.bags = bags

    def __len__(self):
        return len(self.slideIds)

    def __getitem__(self, idx):
        slideId = self.slideIds[idx]
        bagInfo = self.bags[slideId]

        # Cargar embeddings desde disco
        embeddings = []
        for p in bagInfo["paths"]:
            emb = np.load(p)
            embeddings.append(emb)

        bag = torch.tensor(
            np.stack(embeddings),
            dtype=torch.float32
        ) # X

        label = torch.tensor(
            bagInfo["label"],
            dtype=torch.long
        ) # y

        return bag, label

"""No se utiliza batch_size > 1 porque las bolsas tienen tamaños distintos y no pueden ser apiladas sin padding o collation personalizada

## 4.2 Modelo Attention MIL Binario

Se implementa un modelo MIL con atención siguiendo el enfoque clásico:

- Proyección de instancias a un espacio latente
- Cálculo de pesos de atención normalizados con softmax
- Agregación ponderada de instancias
- Clasificación binaria a nivel de bolsa

La función sigmoide **no** se incluye en el modelo, ya que se utiliza `BCEWithLogitsLoss`.
"""

class AttentionMIL(nn.Module):
    def __init__(self, inputDim, hiddenDim=256):
        super().__init__()

        self.featureExtractor = nn.Sequential(
            nn.Linear(inputDim, hiddenDim),
            nn.ReLU()
        )

        self.attention = nn.Sequential(
            nn.Linear(hiddenDim, hiddenDim),
            nn.Tanh(),
            nn.Linear(hiddenDim, 1)
        )

        # Claficador Binario
        self.classifier = nn.Linear(hiddenDim, 1)


    def forward(self, x):
        # x: (num_instances, inputDim)
        H = self.featureExtractor(x)              # (N, H)
        A = self.attention(H)                     # (N, 1)
        A = torch.softmax(A, dim=0)               # atención normalizada

        M = torch.sum(A * H, dim=0)                # bag representation
        y = self.classifier(M)                     # logit

        return y.squeeze(), A.squeeze()

"""## 4.4 Entrenamiento por Fold

### Version A

* Mantiene control explícito de dimensiones
"""

def train_one_fold(fold_data, fold_id, num_epochs=20, lr=1e-4):
    print(f"\n===== Fold {fold_id} ====")

    trainSlides = fold_data["train"]
    valSlides = fold_data["val"]

    trainDataset = MILDataset(trainSlides, bags)
    valDataset = MILDataset(valSlides, bags)

    trainLoader = DataLoader(
        trainDataset,
        batch_size=1,
        shuffle=True
    )

    valLoader = DataLoader(
        valDataset,
        batch_size=1,
        shuffle=False
    )

    # Inferir dimensión de embedding
    sampleBag, _ = trainDataset[0]
    inputDim = sampleBag.shape[1]

    model = AttentionMIL(inputDim=inputDim).to(device)
    criterion = nn.BCEWithLogitsLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)

    for epoch in range(num_epochs):
        model.train()
        losses = []

        for bag, label in trainLoader:
            bag = bag.squeeze(0).to(device)        # (N, D)
            label = label.float().to(device) # label from DataLoader is (1,)

            optimizer.zero_grad()
            logit, _ = model(bag) # logit from model is ()

            # Ensure logit (scalar -> (1,) -> (1,1)) and label ((1,) -> (1,1)) have same shape
            loss = criterion(logit.unsqueeze(0).unsqueeze(1), label.unsqueeze(1))
            loss.backward()
            optimizer.step()

            losses.append(loss.item())

        if (epoch + 1) % 5 == 0:
            print(f"Epoch {epoch+1}/{num_epochs} — Loss: {np.mean(losses):.4f}")

    # ===== Evaluación =====
    model.eval()
    yTrue, yProb = [], []

    with torch.no_grad():
        for bag, label in valLoader:
            bag = bag.squeeze(0).to(device)
            logit, _ = model(bag)
            prob = torch.sigmoid(logit).item()

            yTrue.append(label.item())
            yProb.append(prob)

    yPred = [1 if p >= 0.5 else 0 for p in yProb]

    metrics = {
        "fold": fold_id,
        "f1": f1_score(yTrue, yPred),
        "roc_auc": roc_auc_score(yTrue, yProb),
        "pr_auc": average_precision_score(yTrue, yProb)
    }

    # Guardar modelo entrenado del fold
    model_path = os.path.join(
        MIL_OUTPUT_DIR,
        f"attention_mil_fold_VersionA_{fold_id}.pt"
    )
    torch.save(model.state_dict(), model_path)

    print("Metrics:", metrics)
    return metrics, yTrue, yProb

"""### Version B

- Entrenamiento alternativo
- Misma lógica, sin manipulaciones ad hoc
"""

def train_one_fold_clean(fold_data, fold_id, epochs=20, lr=1e-4):
    print(f"\n===== Fold {fold_id} ====")

    train_ds = MILDataset(fold_data["train"], bags)
    val_ds   = MILDataset(fold_data["val"], bags)

    train_loader = DataLoader(train_ds, batch_size=1, shuffle=True)
    val_loader   = DataLoader(val_ds, batch_size=1, shuffle=False)

    # Inferir dimensión de embedding
    sampleBag, _ = train_ds[0]
    inputDim = sampleBag.shape[1]

    model = AttentionMIL(inputDim=inputDim).to(device)
    optimizer = optim.Adam(model.parameters(), lr=lr)
    criterion = nn.BCEWithLogitsLoss()

    for epoch in range(epochs):
        model.train()

        losses = []
        for bag, label in train_loader:
            bag = bag.squeeze(0).to(device)
            label = label.float().to(device)

            optimizer.zero_grad()
            logit, _ = model(bag)          # escalar, now returns logit and attention
            loss = criterion(logit.view(1), label.view(1))
            loss.backward()
            optimizer.step()
            losses.append(loss.item())

        if (epoch + 1) % 5 == 0:
          print(f"Epoch {epoch+1}/{epochs} — Loss: {np.mean(losses):.4f}")

    model.eval()
    y_true, y_prob = [], []

    with torch.no_grad():
        for bag, label in val_loader:
            bag = bag.squeeze(0).to(device)
            logit, _ = model(bag) # now returns logit and attention
            prob = torch.sigmoid(logit).item()

            y_true.append(int(label.item()))
            y_prob.append(prob)

    y_pred = [1 if p >= 0.5 else 0 for p in y_prob]

    metrics = {
        "fold": fold_id,
        "f1": f1_score(y_true, y_pred),
        "roc_auc": roc_auc_score(y_true, y_prob),
        "pr_auc": average_precision_score(y_true, y_prob)
    }

    # Guardar modelo entrenado del fold
    model_path = os.path.join(
        MIL_OUTPUT_DIR,
        f"attention_mil_fold_VersionB_{fold_id}.pt"
    )
    torch.save(model.state_dict(), model_path)

    print("Metrics:", metrics)

    return metrics, y_true, y_prob

"""## 4.5 Ejecución completa de la validación cruzada

Se ejecuta validación cruzada estratificada por paciente utilizando GroupKFold.

Para cada fold se reportan únicamente las métricas correspondientes al **mejor modelo del fold**, evitando cualquier fuga de información entre folds.
"""

# Imprimir el contenido completo del bag para ese slide_id
#print(f"Contenido del bag para el slide_id: {example_slide_id}")
#pprint.pprint(bags[example_slide_id].keys())

"""### Version A"""

fold_outputs_A = {}
fold_results_A = []

for fold_id, fold_data in enumerate(cv_splits):
    metrics, y_true_fold, y_prob_fold = train_one_fold(
        fold_data,
        fold_id
    )

    fold_outputs_A[fold_id] = {
        "metrics": metrics,
        "y_true": y_true_fold,
        "y_prob": y_prob_fold
    }

    fold_results_A.append(metrics)

results_df_A = pd.DataFrame(fold_results_A)
results_df_A

"""### Version B"""

fold_outputs_B = {}
fold_results_B = []

for fold_id, fold_data in enumerate(cv_splits):
    metrics, y_true_fold, y_prob_fold = train_one_fold_clean(
        fold_data,
        fold_id
    )

    fold_outputs_B[fold_id] = {
        "metrics": metrics,
        "y_true": y_true_fold,
        "y_prob": y_prob_fold
    }

    fold_results_B.append(metrics)

results_df_B = pd.DataFrame(fold_results_B)
results_df_B

"""## 4.6 Guardar Resultados"""

summaryA = results_df_A[["f1", "roc_auc", "pr_auc"]].agg(["mean", "std"]).T
summaryA

summaryB = results_df_B[["f1", "roc_auc", "pr_auc"]].agg(["mean", "std"]).T
summaryB

def aggregate_predictions(fold_outputs):
    y_true_all, y_prob_all = [], []

    for out in fold_outputs.values():
        y_true_all.extend(out["y_true"])
        y_prob_all.extend(out["y_prob"])

    return np.array(y_true_all), np.array(y_prob_all)

y_true_A, y_prob_A = aggregate_predictions(fold_outputs_A)
y_true_B, y_prob_B = aggregate_predictions(fold_outputs_B)

"""### Curvas ROC por fold"""

fpr_A, tpr_A, _ = roc_curve(y_true_A, y_prob_A)
fpr_B, tpr_B, _ = roc_curve(y_true_B, y_prob_B)

roc_auc_A = auc(fpr_A, tpr_A)
roc_auc_B = auc(fpr_B, tpr_B)

plt.figure(figsize=(6,5))
plt.plot(fpr_A, tpr_A, label=f"Version A (AUC = {roc_auc_A:.3f})")
plt.plot(fpr_B, tpr_B, label=f"Version B (AUC = {roc_auc_B:.3f})")
plt.plot([0,1], [0,1], linestyle="--", color="black")

plt.xlabel("Tasa de Falsos Positivos")
plt.ylabel("Tasa de Verdaderos Positivos")
plt.title("Curva ROC — MIL Binario")
plt.legend()
plt.grid(alpha=0.3)
plt.show()

"""- El aggregation mechanism + entrenamiento de la Versión A aprende mejores scores de bolsa
- Versión B pierde información o introduce ruido en el entrenamiento

### Curvas Precision–Recall

###
"""

prec_A, rec_A, _ = precision_recall_curve(y_true_A, y_prob_A)
prec_B, rec_B, _ = precision_recall_curve(y_true_B, y_prob_B)

pr_auc_A = auc(rec_A, prec_A)
pr_auc_B = auc(rec_B, prec_B)

plt.figure(figsize=(6,5))
plt.plot(rec_A, prec_A, label=f"Versión A (PR AUC = {pr_auc_A:.3f})")
plt.plot(rec_B, prec_B, label=f"Versión B (PR AUC = {pr_auc_B:.3f})")

plt.xlabel("Recall")
plt.ylabel("Precision")
plt.title("Curva Precision–Recall — MIL Binario (Sprint 4)")
plt.legend()
plt.grid(alpha=0.3)
plt.show()

"""" Tras reiniciar el entorno y reentrenar los modelos, se observan ligeras variaciones en las métricas, atribuibles a la inicialización aleatoria y a la variabilidad inherente de los folds. Sin embargo, el comportamiento global se mantiene consistente, preservando las conclusiones principales: la Versión A presenta mayor estabilidad entre folds, mientras que la Versión B muestra un mejor poder de ranking a costa de mayor variabilidad. "

### Análisis de Resultados: Versión A vs. Versión B

Tras reentrenar los modelos, se observan variaciones en las métricas finales que reflejan la influencia de la inicialización aleatoria y la conformación de los folds. Aunque ambas versiones convergen de forma similar en la función de pérdida, se identifican diferencias clave en su capacidad de discriminación.

#### **Comparativa de Rendimiento Cuantitativo**

| Métrica | Versión A (Media ± Desv.) | Versión B (Media ± Desv.) |
| :--- | :---: | :---: |
| **Mean F1-Score** | 0.876 ± 0.060 | **0.881 ± 0.089** |
| **Mean ROC AUC** | 0.795 ± 0.077 | **0.840 ± 0.087** |
| **Mean PR AUC** | 0.899 ± 0.092 | **0.925 ± 0.066** |

#### **Observaciones sobre el Comportamiento**

* **Versión A (PR AUC ≈ 0.90):** Presenta una desviación estándar más baja en el F1-Score, lo que sugiere una mayor consistencia en la clasificación binaria directa. Sin embargo, su capacidad de ranking (ROC AUC) es ligeramente inferior a la de su contraparte.
* **Versión B (PR AUC ≈ 0.93):** Logra métricas de área bajo la curva (ROC y PR) superiores, indicando un mejor poder de separación de clases. No obstante, presenta una mayor variabilidad en el F1-Score (std: 0.089), lo que indica que es más sensible a la composición específica del fold de validación.

#### **Análisis de Generalización y Sobreajuste**

A pesar de la rápida convergencia de la función de pérdida hacia valores cercanos a cero en la época 20,

### Distribución de probabilidades por clase
"""

plt.figure(figsize=(6,4))
plt.hist(y_prob_A[y_true_A == 0], bins=25, alpha=0.6, label="Clase 0 (Benigno)")
plt.hist(y_prob_A[y_true_A == 1], bins=25, alpha=0.6, label="Clase 1 (Cáncer)")

plt.axvline(0.5, linestyle="--", color="black")
plt.xlabel("Probabilidad estimada")
plt.ylabel("Frecuencia")
plt.title("Distribución de probabilidades — Versión A")
plt.legend()
plt.show()

plt.figure(figsize=(6,4))
plt.hist(y_prob_B[y_true_B == 0], bins=25, alpha=0.6, label="Clase 0 (Benigno)")
plt.hist(y_prob_B[y_true_B == 1], bins=25, alpha=0.6, label="Clase 1 (Cáncer)")

plt.axvline(0.5, linestyle="--", color="black")
plt.xlabel("Probabilidad estimada")
plt.ylabel("Frecuencia")
plt.title("Distribución de probabilidades — Versión B")
plt.legend()
plt.show()

"""### Boxplot de métricas por fold"""

results_df_A.plot(
    x="fold",
    y=["f1", "roc_auc", "pr_auc"],
    kind="bar",
    figsize=(7,4)
)

plt.title("Métricas por Fold — Versión A")
plt.ylabel("Score")
plt.ylim(0, 1)
plt.grid(axis="y", alpha=0.3)
plt.show()

results_df_B.plot(
    x="fold",
    y=["f1", "roc_auc", "pr_auc"],
    kind="bar",
    figsize=(7,4)
)

plt.title("Métricas por Fold — Versión B")
plt.ylabel("Score")
plt.ylim(0, 1)
plt.grid(axis="y", alpha=0.3)
plt.show()

"""### Boxplot comparativo A vs B"""

plt.figure(figsize=(6,4))

plt.boxplot(
    [results_df_A["f1"], results_df_B["f1"]],
    tick_labels=["Versión A", "Versión B"]
)

plt.ylabel("F1-score")
plt.title("Distribución de F1-score por versión (Sprint 4)")
plt.grid(alpha=0.3)
plt.show()

"""En este sprint se entrenó y evaluó un modelo base de Multiple Instance Learning con mecanismo de atención para clasificación binaria a nivel de slide, utilizando embeddings precomputados de parches histopatológicos.***El modelo fue evaluado mediante validación cruzada estratificada por paciente (GroupKFold)***, garantizando independencia clínica entre los conjuntos de entrenamiento y validación.

Los resultados muestran un desempeño consistente a través de los cinco folds, con un ***F1-score promedio cercano a 0.88***, acompañado de valores elevados de ***PR AUC (>0.90)***, lo cual indica una adecuada capacidad del modelo para identificar la clase positiva en un escenario clínico desbalanceado. El ROC AUC moderado observado es consistente con la naturaleza del problema MIL, donde la señal discriminativa suele concentrarse en un subconjunto reducido de instancias dentro de cada WSI.

Se evaluaron dos implementaciones equivalentes del proceso de entrenamiento, observándose diferencias marginales en el desempeño agregado. Dado que **ambas versiones producen resultados comparables**, se adopta una de ellas como línea base para los sprints posteriores, estableciendo así un modelo MIL binario sólido y reproducible sobre el cual se incorporarán mejoras controladas e interpretabilidad.

## **Conclusiones — Sprint 4**
### Extracción de Embeddings y MIL Binario

#### 1. Resumen del Pipeline
En este sprint se implementó y evaluó un pipeline completo de **Multiple Instance Learning (MIL) binario** para la clasificación a nivel de slide (WSI). Se utilizaron:
* **Embeddings profundos** extraídos mediante una arquitectura ResNet-50.
* **Mecanismo de atención** para la agregación de instancias (parches).
* **Objetivo:** Diferenciación entre tejido benigno (Gleason 0) y tejido cancerígeno (Gleason $\ge$ 3).

#### 2. Metodología de Evaluación
Se compararon dos variantes (**Versión A** y **Versión B**) bajo un esquema de **validación cruzada estratificada por paciente (GroupKFold)** con 5 folds. Este diseño asegura:
* Independencia clínica total: los parches de un paciente solo existen en un conjunto (entrenamiento o validación).
* Evaluación robusta frente a la heterogeneidad biológica.

#### 3. Resultados y Desempeño
A pesar de la competitividad de la Versión B en métricas de ranking, la **Versión A** se selecciona por su equilibrio entre desempeño y estabilidad inter-fold.

| Métrica | Versión A (Media ± Desv.) | Versión B (Media ± Desv.) |
| :--- | :---: | :---: |
| **F1-Score** | 0.876 ± 0.060 | **0.881 ± 0.089** |
| **ROC AUC** | 0.795 ± 0.077 | **0.840 ± 0.087** |
| **PR AUC** | 0.899 ± 0.092 | **0.925 ± 0.066** |

> **Nota:** El **PR AUC promedio de 0.899** de la Versión A (alcanzando hasta **0.98** en folds específicos) demuestra una capacidad sobresaliente para identificar muestras patológicas en un escenario de supervisión débil, minimizando el riesgo de omitir tejido maligno.

#### 4. Análisis de Comportamiento
* **Capacidad Discriminativa:** Ambas versiones logran una separación efectiva, pero la Versión B muestra una ventaja estadística en el área bajo la curva, mientras que la Versión A es más predecible en su clasificación final (menor varianza en F1).
* **Distribución de Probabilidades:** * La **Versión A** presenta una distribución **altamente polarizada** (valores concentrados en los extremos 0 y 1). Esto indica una confianza elevada en la decisión binaria, ideal para sistemas de triaje rápido.
    * La **Versión B** muestra un comportamiento más gradual, lo que le permite obtener mejores métricas de ranking (AUC), aunque a costa de una mayor variabilidad en los resultados de clasificación directa.
* **Robustez:** La ausencia de sobreajuste se confirma por la estabilidad de la pérdida en la época 20 y la consistencia de los resultados en validación cruzada, donde el modelo responde a patrones morfológicos reales y no al ruido.

#### 5. Selección Final
1.  **Versión A:** Se adopta como el **modelo final** para el proyecto debido a su manejo explícito de logits que favorece la estabilidad del entrenamiento y una respuesta clínica más decidida.
2.  **Versión B:** Se valida como una alternativa de alto rendimiento estadístico, útil para análisis de incertidumbre y ranking de probabilidad.

# **Sprint 5 — Análisis del Mecanismo de Atención**

Analizar e interpretar el mecanismo de atención del modelo MIL entrenado en el Sprint 4, con el fin de comprender qué instancias (patches) influyen más en la predicción a nivel de slide y evaluar la coherencia clínica de dichas decisiones.

Este sprint responde directamente a las siguientes preguntas:

- ¿Dónde “mira” el modelo?
- ¿Qué patches dominan la decisión?
- ¿El patrón de atención es razonable desde un punto de vista histopatológico?

## 5.1 — Cargar modelo entrenado
"""

def load_attention_model(model, model_path, device="cuda"):
    model.load_state_dict(torch.load(model_path, map_location=device))
    model.to(device)
    model.eval()
    return model

"""## 5.2 — Forward con extracción de atención"""

@torch.no_grad()
def forward_with_attention(model, bag_tensor):
    """
    Returns:
        prob (float)
        attention (np.ndarray) shape: (N_instances,)
    """
    logits, attention = model(bag_tensor)
    prob = torch.sigmoid(logits).item()
    attention = attention.squeeze().cpu().numpy()
    return prob, attention

"""## 5.3 — Procesar un slide completo"""

def analyze_slide_attention(model, bag_data, device="cuda"):
    """
    bag_data: dict with keys
      - embeddings
      - paths
      - coords
      - label
      - slide_id
    """
    bag_tensor = torch.tensor(
        bag_data["embeddings"],
        dtype=torch.float32,
        device=device
    )

    prob, attention = forward_with_attention(model, bag_tensor)

    return {
        "slide_id": bag_data["slide_id"],
        "label": bag_data["label"],
        "prediction": prob,
        "attention": attention,
        "paths": bag_data["paths"],
        "coords": bag_data["coords"]
    }

"""## 5.4 — Análisis cuantitativo de la atención"""

def attention_stats(attention):
    return {
        "max_attention": np.max(attention),
        "mean_attention": np.mean(attention),
        "std_attention": np.std(attention),
        "entropy": -np.sum(attention * np.log(attention + 1e-8))
    }

"""Curva acumulada (top-k mass)"""

def cumulative_attention(attention):
    sorted_att = np.sort(attention)[::-1]
    cumulative = np.cumsum(sorted_att)
    return cumulative

"""## 5.5 — Gráficos globales de atención

Histograma de atención
"""

def plot_attention_histogram(attention, title):
    plt.figure()
    plt.hist(attention, bins=30)
    plt.xlabel("Attention weight")
    plt.ylabel("Frequency")
    plt.title(title)
    plt.show()

"""Curva acumulada"""

def plot_attention_cumulative(attention, title="Cumulative attention"):
    cumulative = cumulative_attention(attention)

    plt.figure()
    plt.plot(cumulative)
    plt.axhline(0.8, linestyle="--")
    plt.xlabel("Top-k instances")
    plt.ylabel("Cumulative attention")
    plt.title(title)
    plt.show()

"""## 5.6 — Relación atención ↔ predicción"""

def plot_attention_vs_prediction(df, title):
    plt.figure()
    plt.scatter(df["max_attention"], df["prediction"])
    plt.xlabel("Max attention")
    plt.ylabel("Predicted probability")
    plt.title(title)
    plt.show()

"""## 5.7 — Flujo típico de ejecución (por fold)"""

device = "cuda" if torch.cuda.is_available() else "cpu"

fold_id = 0
version = "VersionA"

# 1️. Slides de validación del fold
fold_data = cv_splits[fold_id]
test_slide_ids = fold_data["val"]

# 2️. Inferir dimensión del embedding automáticamente
example_slide_id = test_slide_ids[0]
example_path = bags[example_slide_id]["paths"][0]
input_dim = np.load(example_path).shape[0]

# 3️. Instanciar modelo y cargar pesos
model_path = os.path.join(
    MIL_OUTPUT_DIR,
    f"attention_mil_fold_{version}_{fold_id}.pt"
)

model = AttentionMIL(inputDim=input_dim).to(device)
model.load_state_dict(torch.load(model_path, map_location=device))
model.eval()

# 4️. Análisis de atención
results = []

for slide_id in test_slide_ids:
    bag_info = bags[slide_id]

    # Cargar embeddings del slide
    embeddings = np.stack([np.load(p) for p in bag_info["paths"]])

    bag_data = {
        "slide_id": slide_id,
        "label": bag_info["label"],
        "embeddings": embeddings,
        "paths": bag_info["paths"],
        "coords": bag_info["coords"]
    }

    output = analyze_slide_attention(model, bag_data, device)
    stats = attention_stats(output["attention"])

    results.append({
        "slide_id": slide_id,
        "label": output["label"],
        "prediction": output["prediction"],
        **stats
    })

attention_df = pd.DataFrame(results)
attention_df

"""#### ¿Qué está mostrando la tabla?

**1. Se tiene por slide:**

- label: verdad clínica (0 benigno, 1 cáncer)
- prediction: probabilidad predicha por el MIL
- max_attention: cuánto peso máximo asignó a una instancia
- mean_attention: promedio de atención (≈ 1/N)
- std_attention: dispersión
- entropy: concentración vs dispersión de atención

Esto permite conectar decisión ↔ mecanismo interno.

**2.1 Slides cancerígenos bien clasificados (label=1, prediction≈1)**
#### Interpretación

- Atención altamente concentrada
- Entropía baja
- El modelo basa su decisión en muy pocos patches dominantes

**2.2 Slides benignos correctamente rechazados (label=0, prediction≈0)**

#### Interpretación

- Atención no extremadamente concentrada
- Probabilidad final muy baja
- El modelo no encuentra evidencia fuerte y consistente

**2.3 Casos difíciles / ambiguos**

#### Falsos positivos (label=0, prediction≈1)

Ejemplos:

| Slide ID | Label | Predicción (Prob) | Max Attention | Entropy |
| :--- | :---: | :---: | :---: | :---: |
| **18B0005478F** | 0 | 0.9998 | 0.83 | 0.47 |
| **18B0005478G** | 0 | 0.9999 | 0.92 | 0.32 |
| **18B0005478E** | 0 | 0.8200 | 0.96 | 0.21 |

Análisis:
- Atención extremadamente concentrada
- El modelo cree haber visto algo “tumoral”
- Puede corresponder a:
  * Atipias benignas
  * Artefactos
  * Regiones borderline


**2.4 Predicciones inciertas con atención dispersa**

Ejemplos:

| Slide ID | Predicción (Prob) | Entropía |
| :--- | :---: | :---: |
| **16B0008067** | 0.99 | 2.29 |
| **17B0034179** | 0.999 | 1.65 |
| **18B0002932D** | 0.23 | 1.65 |

#### Interpretación

- Atención repartida en muchas instancias
- No hay un único patch dominante
- El modelo agrega evidencia débil

#### Esto explica:
- Predicción incierta
- Mayor sensibilidad a ruido

El modelo MIL no solo alcanza buen desempeño cuantitativo, sino que toma decisiones basadas en un subconjunto reducido de instancias, de forma consistente con el paradigma MIL, permitiendo análisis post-hoc de interpretabilidad clínica.

## 5.8 Cargar y mostrar patches con mayor atención
"""

def visualize_topk_patches(attention_output, k=5):
    # 'embedding_paths' here are paths to .npy files, e.g., '/content/sicapv2_data/processed_patches/v1/embeddings/patch_name.npy'
    embedding_paths = attention_output["paths"]
    attention = attention_output["attention"]

    topk_idx = np.argsort(attention)[-k:][::-1]

    plt.figure(figsize=(3*k, 3))
    for i, idx in enumerate(topk_idx):
        # Extract the original image filename (e.g., 'patch_name.npy' -> 'patch_name.jpg')
        embedding_filename = os.path.basename(embedding_paths[idx])
        original_image_filename = embedding_filename.replace('.npy', '.jpg') # Assuming original images are .jpg

        # Construct the full path to the original image file from IMAGES_DIR
        original_image_path = os.path.join(IMAGES_DIR, original_image_filename)

        img = Image.open(original_image_path) # Now open the actual image
        plt.subplot(1, k, i+1)
        plt.imshow(img)
        plt.axis("off")
        plt.title(f"A={attention[idx]:.2f}")

    plt.suptitle(
        f"Slide {attention_output['slide_id']} | "
        f"Pred={attention_output['prediction']:.3f}",
        fontsize=14
    )
    plt.show()

"""## 5.9 Seleccionar slides representativos"""

# Verdaderos Positivos
tp_slides = attention_df[
    (attention_df["label"] == 1) & (attention_df["prediction"] > 0.9)
].head(2)["slide_id"]

# Falsos Positivos
fp_slides = attention_df[
    (attention_df["label"] == 0) & (attention_df["prediction"] > 0.9)
].head(2)["slide_id"]

# Falso Negativo (si existe)
fn_slides = attention_df[
    (attention_df["label"] == 1) & (attention_df["prediction"] < 0.5)
].head(1)["slide_id"]

for slide_id in list(tp_slides) + list(fp_slides) + list(fn_slides):
    bag_info = bags[slide_id]

    embeddings = np.stack([np.load(p) for p in bag_info["paths"]])

    bag_data = {
        "slide_id": slide_id,
        "label": bag_info["label"],
        "embeddings": embeddings,
        "paths": bag_info["paths"],
        "coords": bag_info["coords"]
    }

    output = analyze_slide_attention(model, bag_data, device)
    visualize_topk_patches(output, k=5)

"""## 5.10 Relación Atención ↔ Predicción"""

plt.figure(figsize=(6,4))
plt.scatter(
    attention_df["max_attention"],
    attention_df["prediction"],
    c=attention_df["label"],
    alpha=0.7
)

plt.xlabel("Max Attention")
plt.ylabel("Predicted Probability")
plt.title("Relación Atención Máxima vs Predicción")
plt.grid(alpha=0.3)
plt.show()

attention_df["error"] = np.abs(
    attention_df["prediction"] - attention_df["label"]
)

plt.figure(figsize=(6,4))
plt.scatter(
    attention_df["entropy"],
    attention_df["error"],
    alpha=0.7
)

plt.xlabel("Attention Entropy")
plt.ylabel("Prediction Error")
plt.title("Entropía de Atención vs Error")
plt.grid(alpha=0.3)
plt.show()

# ==========================================================
# BLOQUE: ESTADÍSTICAS DE PREPROCESAMIENTO (PARA SECCIÓN 8.1)
# ==========================================================
import seaborn as sns
import matplotlib.pyplot as plt

# 1. Cálculo de métricas (Usando tus variables del notebook)
total_parches_raw = len(df_manifest) # Total antes del último filtrado, usando df_manifest del Sprint 0
parches_finales = processed_manifest[processed_manifest['tissue_fraction'] >= 0.05]
num_final = len(parches_finales)
descartados = total_parches_raw - num_final
porcentaje_descarte = (descartados / total_parches_raw) * 100

print("-" * 50)
print("RESUMEN DE PROCESAMIENTO")
print("-" * 50)
print(f"Total de parches extraídos inicialmente: {total_parches_raw}")
print(f"Parches descartados (Calidad < 0.05):   {descartados}")
print(f"Porcentaje de descarte:                {porcentaje_descarte:.2f}%")
print(f"Dataset final (instancias útiles):     {num_final}")
print("-" * 50)

# 2. Generación del gráfico para el documento
plt.figure(figsize=(10, 6))
sns.histplot(processed_manifest['tissue_fraction'], bins=30, kde=True, color='#2c3e50')
plt.axvline(0.05, color='red', linestyle='--', label='Umbral de Descarte (0.05)')
plt.title('Distribución de la Fracción de Tejido (Control de Calidad)', fontsize=14)
plt.xlabel('Proporción de Tejido en el Parche', fontsize=12)
plt.ylabel('Cantidad de Parches', fontsize=12)
plt.legend()
plt.grid(alpha=0.3)
plt.savefig('resultado_preprocesamiento.png', dpi=300) # Se guarda para tu Word
plt.show()

# --- CÓDIGO PARA GENERAR GRÁFICO DE PREPROCESAMIENTO ---
import matplotlib.pyplot as plt
import seaborn as sns

# AJUSTA ESTE NOMBRE SI TU DATAFRAME SE LLAMA DIFERENTE (ej. df_final, metadata_df)
df_analisis = processed_manifest # Usar processed_manifest del Sprint 1

print("--- DATOS PARA EL TEXTO DE LA TESIS ---")
# Asumimos que si está en el DF es porque pasó el filtro.
# Si tienes el número original antes de filtrar, úsalo para calcular el descarte.
print(f"Total de parches válidos para entrenamiento: {len(df_analisis)}")
print(f"Promedio de fracción de fracción de tejido: {df_analisis['tissue_fraction'].mean():.4f}")

# Gráfico para la Figura 8.1
plt.figure(figsize=(10, 5))
sns.histplot(df_analisis['tissue_fraction'], bins=30, kde=True)
plt.axvline(0.05, color='red', linestyle='--', label='Umbral de Corte (0.05)')
plt.title('Distribución de Fracción de Tejido en el Dataset Final')
plt.xlabel('Fracción de Tejido')
plt.ylabel('Frecuencia')
plt.legend()
plt.grid(alpha=0.3)
plt.savefig('figura_preprocesamiento.png', dpi=300)
plt.show()
print("GUARDAR LA IMAGEN 'figura_preprocesamiento.png' PARA LA TESIS")

# --- CÓDIGO PARA GENERAR VISUALIZACIÓN DE ATENCIÓN (CUALITATIVO) ---
import torch
import matplotlib.pyplot as plt
from PIL import Image
import numpy as np

def visualizar_top_atencion(model, df_test, num_ejemplos=1):
    model.eval()
    # Filtramos solo slides malignos (Label = 1) para ver qué detecta el cáncer
    malignos = df_test[df_test['label'] == 1]['slide_id'].unique()

    if len(malignos) == 0:
        print("No hay casos malignos en el dataframe de prueba para visualizar.")
        return

    # Seleccionamos un caso aleatorio
    slide_id = np.random.choice(malignos)
    print(f"Analizando Slide: {slide_id}")

    # 1. Obtener embeddings y parches de ese slide
    # AJUSTA ESTAS COLUMNAS SI SE LLAMAN DISTINTO EN TU DF
    subset = df_test[df_test['slide_id'] == slide_id].reset_index(drop=True)

    # Simulamos la entrada al modelo (esto depende de cómo guardaste tus features)
    # Si tienes los features en disco, cárgalos. Aquí asumo que podrías necesitarlos.
    # SI YA TIENES UN DATALOADER LISTO, ES MEJOR USARLO.
    # Esta parte es genérica:

    try:
        # Intenta cargar los features si tienes la ruta en el DF
        # bag_features = torch.load(ruta_a_features)
        # SINO, vamos a usar un dummy para explicarte la lógica,
        # PERO LO IDEAL ES USAR TU FUNCIÓN DE INFERENCIA EXISTENTE.
        pass
    except:
        print("Necesito saber cómo cargas tus features para darte el código exacto de inferencia.")
        return

    # NOTA PARA JUAN: Como no tengo tu dataloader exacto a mano en este chat,
    # usa tu función de "inference" o "test" que ya tienes en el notebook.
    # Lo que necesitas es extraer los 'attention_weights' del modelo.

    # SI TU MODELO DEVUELVE (logits, Y_prob, A), ENTONCES 'A' SON LOS PESOS.
    # Supongamos que obtuviste 'A' (los pesos de atención) para ese slide.

    # --- EJEMPLO SIMULADO DE VISUALIZACIÓN (ADAPTA ESTO A TU SALIDA) ---
    # top_indices = torch.argsort(A, descending=True)[:5]  # Top 5 parches

    # fig, axes = plt.subplots(1, 5, figsize=(20, 4))
    # for i, idx in enumerate(top_indices):
    #     ruta_img = subset.loc[idx.item(), 'path'] # Ruta de la imagen original
    #     img = Image.open(ruta_img)
    #     score = A[0, idx].item()

    #     axes[i].imshow(img)
    #     axes[i].set_title(f"Attn: {score:.4f}")
    #     axes[i].axis('off')
    # plt.suptitle(f"Zonas de Alta Atención (Cáncer Detectado) - Slide {slide_id}")
    # plt.show()

print("Para este paso, busca en tu Sprint 4 o 5 donde haces 'model(data)'.")
print("Necesitas asegurar que tu modelo retorne los pesos de atención (Attention Weights).")
print("Si tu modelo retorna 'logits, Y_prob, Y_hat, A', usa 'A' para saber qué parches pintar.")

"""### Ejecutando la visualización de atención usando funciones existentes

Vamos a mostrar nuevamente los parches con mayor peso de atención para algunos ejemplos de Verdaderos Positivos, Falsos Positivos y Falsos Negativos (si los hubiera), utilizando las funciones `analyze_slide_attention` y `visualize_topk_patches` que ya están definidas.
"""

# Asegúrate de que 'model' y 'device' estén definidos y cargados (esto ya se hizo en Sprint 5)
# 'tp_slides', 'fp_slides', 'fn_slides' también se definieron en el análisis del Sprint 5

# Recorre los slides de ejemplo y visualiza sus parches de alta atención
print("Visualizando parches de alta atención para ejemplos seleccionados...")
for slide_id in list(tp_slides) + list(fp_slides) + list(fn_slides):
    # Obtener la información del bag para el slide_id actual
    bag_info = bags[slide_id]

    # Cargar los embeddings desde los paths guardados en el bag
    embeddings = np.stack([np.load(p) for p in bag_info["paths"]])

    # Preparar los datos del bag para el análisis
    bag_data = {
        "slide_id": slide_id,
        "label": bag_info["label"],
        "embeddings": embeddings,
        "paths": bag_info["paths"],
        "coords": bag_info["coords"]
    }

    # Analizar el slide para obtener la predicción y los pesos de atención
    output = analyze_slide_attention(model, bag_data, device)

    # Visualizar los 5 parches con mayor peso de atención
    visualize_topk_patches(output, k=5)

print("Visualización de ejemplos completada.")

# 1. Función para visualizar lo que el modelo ve
def mostrar_atencion_modelo(model, df_analysis, bags_data, embeddings_dir, images_dir, num_ejemplos=1):
    print("Generando análisis visual de atención...")
    model.eval()

    # Filtramos solo slides malignos (Label = 1) para ver qué detecta el cáncer
    # Using the 'label' column from attention_df (passed as df_analysis)
    malignos = df_analysis[df_analysis['label'] == 1]['slide_id'].unique()
    if len(malignos) == 0:
        print("No hay casos malignos en el dataframe de prueba para visualizar.")
        return

    # Seleccionamos un caso aleatorio
    slide_id = np.random.choice(malignos)
    print(f"Analizando Slide: {slide_id}")

    # Get bag_info for the selected slide_id
    bag_info = bags_data[slide_id]

    # Load embeddings from .npy files
    embeddings = np.stack([np.load(p) for p in bag_info["paths"]])
    features = torch.tensor(embeddings, dtype=torch.float32).to(device)

    # PREGUNTA AL MODELO
    with torch.no_grad():
        # AttentionMIL returns (logit, attention_weights)
        logit, attention_weights = model(features)
        # Apply sigmoid to logit to get probability
        prob = torch.sigmoid(logit).item()

    # A son los pesos. Los pasamos a numpy
    att_weights = attention_weights.squeeze().cpu().numpy()

    # Ordenamos: ¿Cuáles son los 5 parches con MAYOR peso?
    top_indices = np.argsort(att_weights)[-5:][::-1]

    # DIBUJAR
    fig, axes = plt.subplots(1, 5, figsize=(20, 5))
    fig.suptitle(f'Análisis de Atención (Regiones Clave) - Slide: {slide_id} (Pred: {prob:.4f} / Real: {bag_info["label"]})', fontsize=16)

    # Obtener las rutas de las imágenes originales de ese slide
    # The 'paths' in bag_info are to the .npy embeddings, need to convert to original image paths
    original_image_filenames = [os.path.basename(p).replace('.npy', '.jpg') for p in bag_info["paths"]]

    for i, idx in enumerate(top_indices):
        # Get the original image filename using the index from the bag_info's paths
        original_image_filename = original_image_filenames[idx]
        img_path = os.path.join(images_dir, original_image_filename)

        try:
            img = Image.open(img_path)
            axes[i].imshow(img)
            axes[i].set_title(f"Peso α: {att_weights[idx]:.4f}\\n(Gleason alto)", color='darkred', weight='bold') # Changed to high Gleason as it's for label 1
        except Exception as e:
            axes[i].text(0.5, 0.5, f"Img no encontrada\\n({original_image_filename})", ha='center', va='center')
            print(f"Error loading image {img_path}: {e}")

        axes[i].axis('off')

    plt.tight_layout()
    plt.show()

# EJECUTARLO (Asegúrate de tener definidas estas variables de tu notebook)
# model: tu modelo entrenado (e.g., 'model' from Sprint 5's execution)
# attention_df: el dataframe con resultados de atención (variable 'attention_df')
# bags: el diccionario de bags (variable 'bags')
# EMBEDDINGS_DIR: el directorio de embeddings (variable 'EMBEDDINGS_DIR')
# IMAGES_DIR: el directorio de imágenes originales (variable 'IMAGES_DIR')
mostrar_atencion_modelo(model, attention_df, bags, EMBEDDINGS_DIR, IMAGES_DIR, num_ejemplos=1)

"""# Cierre del Sprint 5 — Análisis del Mecanismo de Atención

En este sprint se analizó en profundidad el **mecanismo de atención** del modelo MIL binario para interpretar la toma de decisiones a nivel de **Whole Slide Image (WSI)** y evaluar su coherencia clínica y metodológica.

---

### Metodología del Análisis
A partir de los modelos del Sprint 4, se extrajeron los pesos de atención por instancia únicamente sobre el **set de validación**. Esto permitió estudiar la contribución relativa de cada *patch* a la predicción final, garantizando:
* **Independencia técnica:** Evaluación libre de sesgo de entrenamiento.
* **Granularidad:** Identificación de las regiones que "disparan" el diagnóstico.

### Hallazgos Cuantitativos
El modelo demostró un patrón **altamente selectivo**, coherente con la naturaleza del cáncer de próstata en histopatología (donde el tumor es focal). Los puntos clave detectados fueron:

* **Relación Confianza-Entropía:** Los slides clasificados correctamente con alta confianza presentan **atención concentrada** (baja entropía).
* **Incertidumbre Espacial:** Las predicciones ambiguas o erróneas muestran una **atención dispersa**, reflejando la dificultad del modelo para localizar evidencia clara.
* **Decisión No-Trivial:** Existe una relación clara entre la atención máxima y la probabilidad predicha, descartando reglas simplistas y confirmando un aprendizaje complejo.



### Interpretación y Visualización
Las visualizaciones de los **top-k patches** confirman que el modelo enfoca su atención en regiones morfológicamente informativas.
> **Conclusión técnica:** El mecanismo de atención no solo es funcional para la clasificación, sino que actúa como una herramienta de **explicabilidad post-hoc**, fundamental para la confianza en entornos médicos.

---

### Conclusiones del Sprint
En conjunto, este sprint valida que el modelo MIL con atención:
1.  **Localiza:** Aprende a identificar regiones relevantes a nivel de instancia sin supervisión directa por parche.
2.  **Agrega:** Utiliza dicha información de forma coherente para el diagnóstico global del slide.
3.  **Explica:** Proporciona señales interpretables que permiten auditar el comportamiento del modelo.

Con estos resultados, el modelo está listo para la fase de consolidación final, respaldado por métricas de rendimiento y capacidad de interpretación.

## Discusión Final del Proyecto de Grado

### 1. Síntesis del Enfoque Propuesto
Se desarrolló un pipeline completo de **Multiple Instance Learning (MIL)** para la clasificación binaria de *Whole Slide Images* (WSI). El enfoque aborda los desafíos críticos del análisis histopatológico digital:
* **Alta dimensionalidad:** Manejo de imágenes de gigapíxeles mediante parches.
* **Instancias variables:** Capacidad de procesar slides con diferente cantidad de tejido.
* **Señal localizada:** Identificación de focos tumorales pequeños en áreas extensas.
* **Independencia clínica:** Respeto estricto a la separación por pacientes.



### 2. Desempeño y Generalización
La evaluación mediante **GroupKFold** garantiza que los resultados son honestos y clínicamente válidos:
* **F1-score ($\approx 0.88$):** Excelente balance entre sensibilidad y especificidad.
* **PR AUC ($>0.90$):** Demuestra que el modelo es robusto ante el desbalance de clases, priorizando la detección de cáncer.
* **ROC AUC Moderado:** Coherente con el paradigma MIL, donde la señal relevante es a menudo una fracción mínima del total de la bolsa.

### 3. Interpretabilidad y Mecanismo de Atención
A diferencia de los modelos de "caja negra", este enfoque permite una **IA explicable (XAI)**:
* **Localización:** Identifica qué regiones del slide disparan la alerta de diagnóstico.
* **Relación Confianza-Entropía:** Se confirmó que la atención concentrada genera predicciones más seguras, emulando el proceso de un patólogo al encontrar un foco de Gleason claro.
* **Análisis Post-hoc:** Facilita la auditoría médica y la comprensión de los falsos positivos/negativos.

### 4. Limitaciones y Trabajo Futuro
| Limitación Actual | Oportunidad de Mejora (Trabajo Futuro) |
| :--- | :--- |
| **Embeddings precomputados:** El modelo no es *end-to-end*. | Integrar entrenamiento conjunto de extractor y agregador. |
| **Explicación no causal:** La atención es correlacional. | Validar con anotaciones manuales de patólogos (mecanismos de *ground truth*). |
| **Tamaño del dataset:** Limitado a SICAPv2. | Evaluación en conjuntos externos y análisis multiclase (Gleason 3, 4, 5). |



### 5. Conclusión Global
> **Este trabajo demuestra que el aprendizaje por instancias múltiples (MIL) con atención no solo alcanza un desempeño competitivo, sino que aporta un nivel de interpretabilidad esencial para la práctica clínica. El modelo no solo "clasifica", sino que "localiza" y "pondera", estableciendo una base sólida para herramientas de apoyo al diagnóstico patológico.**
"""











from __future__ import annotations

import argparse
import os
from typing import Dict, List, Tuple

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score, f1_score, precision_score, recall_score
from PIL import Image


def load_preds_csv(path: str) -> Dict[int, Dict[str, object]]:
    df = pd.read_csv(path)
    if 'fold' not in df.columns:
        df['fold'] = 0
    groups = {int(f): {'df': g.reset_index(drop=True)} for f, g in df.groupby('fold')}
    return groups


def aggregate_instances_to_bags(df: pd.DataFrame, bag_col: str = 'bag_id', method: str = 'max', attention_col: str = 'attention') -> pd.DataFrame:
    if bag_col not in df.columns:
        raise ValueError(f'Column {bag_col} not found in dataframe')
    grouped = []
    for bag, g in df.groupby(bag_col):
        y_trues = g['y_true'].unique()
        y_true = int(np.max(y_trues)) if len(y_trues) > 0 else 0
        if method == 'max':
            y_prob = float(g['y_prob'].max())
        elif method == 'mean':
            y_prob = float(g['y_prob'].mean())
        elif method == 'attention':
            if attention_col not in g.columns:
                raise ValueError('Attention column not present for attention aggregation')
            weights = g[attention_col].astype(float).to_numpy()
            probs = g['y_prob'].astype(float).to_numpy()
            y_prob = float((weights * probs).sum() / (weights.sum() + 1e-12))
        else:
            raise ValueError('Unknown aggregation method: ' + method)
        grouped.append({bag_col: bag, 'y_true': y_true, 'y_prob': y_prob, 'n_instances': len(g)})
    return pd.DataFrame(grouped)


def metrics_for_fold(y_true: np.ndarray, y_prob: np.ndarray) -> Dict:
    fpr, tpr, _ = roc_curve(y_true, y_prob)
    roc_auc = auc(fpr, tpr)
    precision, recall, _ = precision_recall_curve(y_true, y_prob)
    pr_auc = average_precision_score(y_true, y_prob)
    y_pred = (y_prob >= 0.5).astype(int)
    f1 = float(f1_score(y_true, y_pred)) if y_true.size > 0 else np.nan
    prec = float(precision_score(y_true, y_pred)) if y_true.size > 0 else np.nan
    rec = float(recall_score(y_true, y_pred)) if y_true.size > 0 else np.nan
    return {
        'fpr': fpr,
        'tpr': tpr,
        'roc_auc': roc_auc,
        'precision': precision,
        'recall': recall,
        'pr_auc': pr_auc,
        'f1': f1,
        'precision_at_05': prec,
        'recall_at_05': rec,
    }


def aggregate_metrics(per_fold_metrics: List[Dict]) -> Dict[str, Tuple[float, float]]:
    scalars = ['roc_auc', 'pr_auc', 'f1', 'precision_at_05', 'recall_at_05']
    out = {}
    for s in scalars:
        vals = np.array([m.get(s, np.nan) for m in per_fold_metrics], dtype=float)
        out[s] = (float(np.nanmean(vals)), float(np.nanstd(vals)))
    return out


def plot_roc_pr(per_fold: Dict[int, Dict], outdir: str) -> None:
    os.makedirs(outdir, exist_ok=True)
    plt.figure(figsize=(6, 6))
    for f, m in sorted(per_fold.items()):
        plt.plot(m['fpr'], m['tpr'], alpha=0.6, label=f'fold {f} (AUC={m["roc_auc"]:.3f})')
    plt.plot([0, 1], [0, 1], 'k--', alpha=0.5)
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC per fold')
    plt.legend(loc='lower right', fontsize='small')
    plt.tight_layout()
    plt.savefig(os.path.join(outdir, 'roc_per_fold.png'), dpi=200)
    plt.close()

    plt.figure(figsize=(6, 6))
    for f, m in sorted(per_fold.items()):
        plt.plot(m['recall'], m['precision'], alpha=0.6, label=f'fold {f} (AP={m["pr_auc"]:.3f})')
    plt.xlabel('Recall')
    plt.ylabel('Precision')
    plt.title('Precision-Recall per fold')
    plt.legend(loc='lower left', fontsize='small')
    plt.tight_layout()
    plt.savefig(os.path.join(outdir, 'pr_per_fold.png'), dpi=200)
    plt.close()


def plot_prob_histograms(groups: Dict[int, Dict], outdir: str) -> None:
    os.makedirs(outdir, exist_ok=True)
    for f, g in sorted(groups.items()):
        if isinstance(g, dict) and 'df' in g:
            probs = g['df']['y_prob'].astype(float).to_numpy()
        else:
            probs = g['y_prob']
        plt.figure(figsize=(6, 3))
        plt.hist(probs, bins=30, alpha=0.7)
        plt.title(f'Probability distribution (fold {f})')
        plt.xlabel('Predicted probability')
        plt.tight_layout()
        plt.savefig(os.path.join(outdir, f'prob_hist_fold_{f}.png'), dpi=150)
        plt.close()


def make_mosaic(image_paths: List[str], outpath: str, cols: int = 3, thumb_size: Tuple[int, int] = (256, 256)):
    imgs = []
    for p in image_paths:
        try:
            im = Image.open(p).convert('RGB')
            im.thumbnail(thumb_size)
            imgs.append(im)
        except Exception:
            continue
    if not imgs:
        raise ValueError('No images found for mosaic')
    rows = (len(imgs) + cols - 1) // cols
    w, h = thumb_size
    mosaic = Image.new('RGB', (cols * w, rows * h), color=(255, 255, 255))
    for idx, im in enumerate(imgs):
        x = (idx % cols) * w
        y = (idx // cols) * h
        mosaic.paste(im, (x, y))
    mosaic.save(outpath)


def save_topk_patches_from_attention(att_csv: str, k: int, outdir: str, cols: int = 3):
    df = pd.read_csv(att_csv)
    if 'attention' not in df.columns or 'patch_path' not in df.columns:
        raise ValueError('CSV must contain patch_path and attention columns')
    df_sorted = df.sort_values('attention', ascending=False).head(k)
    os.makedirs(outdir, exist_ok=True)
    outpath = os.path.join(outdir, f'top_{k}_patches.png')
    make_mosaic(df_sorted['patch_path'].tolist(), outpath, cols=cols)
    return outpath


def generate_topk_per_bag(preds_csv: str, outdir: str, bag_col: str = 'bag_id', k: int = 9, method: str = 'attention', attention_col: str = 'attention', cols: int = 3):
    df = pd.read_csv(preds_csv)
    if bag_col not in df.columns:
        raise ValueError(f'Bag column {bag_col} not found')
    os.makedirs(outdir, exist_ok=True)
    for bag, g in df.groupby(bag_col):
        if method == 'attention' and 'attention' in g.columns:
            sel = g.sort_values('attention', ascending=False).head(k)
        else:
            sel = g.sort_values('y_prob', ascending=False).head(k)
        img_paths = sel['patch_path'].tolist()
        outpath = os.path.join(outdir, f'top_{k}_bag_{bag}.png')
        try:
            make_mosaic(img_paths, outpath, cols=cols)
        except Exception:
            continue
    return outdir


def compute_tissue_stats_from_csv(patch_csv: str, outdir: str, tissue_col: str = 'tissue_fraction'):
    df = pd.read_csv(patch_csv)
    if tissue_col not in df.columns:
        raise ValueError(f'{tissue_col} not in CSV. Provide a CSV with tissue_fraction column or use --compute-from-images')
    stats = {
        'count': int(len(df)),
        'mean': float(df[tissue_col].mean()),
        'median': float(df[tissue_col].median()),
        'std': float(df[tissue_col].std()),
        'pct_above_0.05': float((df[tissue_col] >= 0.05).mean()),
    }
    os.makedirs(outdir, exist_ok=True)
    df[tissue_col].hist(bins=50)
    plt.xlabel('Tissue fraction')
    plt.title('Tissue fraction distribution')
    plt.savefig(os.path.join(outdir, 'tissue_fraction_hist.png'), dpi=150)
    plt.close()
    pd.Series(stats).to_csv(os.path.join(outdir, 'tissue_fraction_stats.csv'))
    return stats


def compute_tissue_fraction_from_image(path: str, downscale: int = 256, sat_thr: int = 20, val_thr: int = 250) -> float:
    try:
        pil = Image.open(path).convert('RGB')
        arr = np.array(pil)
        import cv2
        h, w = arr.shape[:2]
        scale = max(1, int(max(h, w) / downscale))
        small = cv2.resize(arr, (max(1, w // scale), max(1, h // scale)), interpolation=cv2.INTER_AREA)
        hsv = cv2.cvtColor(small, cv2.COLOR_RGB2HSV)
        s, v = hsv[:, :, 1], hsv[:, :, 2]
        tissueMask = (s > sat_thr) & (v < val_thr)
        return float(tissueMask.mean())
    except Exception:
        return 0.0


def compute_tissue_stats_from_images(patch_csv: str, outdir: str, patch_path_col: str = 'patch_path'):
    df = pd.read_csv(patch_csv)
    if patch_path_col not in df.columns:
        raise ValueError(f'{patch_path_col} not found in CSV')
    os.makedirs(outdir, exist_ok=True)
    fractions = []
    for p in df[patch_path_col].tolist():
        fractions.append(compute_tissue_fraction_from_image(p))
    s = pd.Series(fractions)
    stats = {
        'count': int(len(s)),
        'mean': float(s.mean()),
        'median': float(s.median()),
        'std': float(s.std()),
        'pct_above_0.05': float((s >= 0.05).mean()),
    }
    s.hist(bins=50)
    plt.xlabel('Tissue fraction')
    plt.title('Tissue fraction distribution (computed)')
    plt.savefig(os.path.join(outdir, 'tissue_fraction_hist_computed.png'), dpi=150)
    plt.close()
    pd.Series(stats).to_csv(os.path.join(outdir, 'tissue_fraction_stats_computed.csv'))
    return stats


def summarize_from_csv(preds_csv: str, outdir: str, aggregate_method: str = 'max', bag_col: str = 'bag_id', attention_col: str = 'attention'):
    groups = load_preds_csv(preds_csv)
    per_fold_metrics = {}
    metrics_list = []
    for f, g in sorted(groups.items()):
        df = g['df']
        if bag_col in df.columns:
            bag_df = aggregate_instances_to_bags(df, bag_col=bag_col, method=aggregate_method, attention_col=attention_col)
            y_true = bag_df['y_true'].to_numpy()
            y_prob = bag_df['y_prob'].to_numpy()
        else:
            y_true = df['y_true'].astype(int).to_numpy()
            y_prob = df['y_prob'].astype(float).to_numpy()
        m = metrics_for_fold(y_true, y_prob)
        per_fold_metrics[f] = m
        metrics_list.append(m)
    agg = aggregate_metrics(metrics_list)
    os.makedirs(outdir, exist_ok=True)
    agg_df = pd.DataFrame({k: {'mean': v[0], 'std': v[1]} for k, v in agg.items()}).T
    agg_df.to_csv(os.path.join(outdir, 'metrics_aggregate.csv'))
    plot_roc_pr(per_fold_metrics, outdir)
    plot_prob_histograms(groups, outdir)
    return agg_df

"""### Ejemplo de uso de las funciones de análisis (utilizando los resultados del Sprint 4)"""

# Primero, necesitamos generar un CSV con las predicciones para usar con las funciones de resumen.
# Usaremos los datos agregados de la Versión A del Sprint 4.

# Crear un DataFrame con las predicciones del fold_outputs_A
predictions_df_A = pd.DataFrame({
    'fold': [f_id for f_id, output in fold_outputs_A.items() for _ in range(len(output['y_true']))],
    'y_true': [val for output in fold_outputs_A.values() for val in output['y_true']],
    'y_prob': [val for output in fold_outputs_A.values() for val in output['y_prob']]
})

# Guardar este DataFrame temporalmente en un CSV para que la función summarize_from_csv pueda leerlo
temp_preds_csv = os.path.join(MIL_OUTPUT_DIR, "temp_predictions_A.csv")
predictions_df_A.to_csv(temp_preds_csv, index=False)

print(f"Predicciones guardadas temporalmente en: {temp_preds_csv}")

# Definir el directorio de salida para los resultados del resumen
summary_output_dir = os.path.join(MIL_OUTPUT_DIR, "summary_results")
os.makedirs(summary_output_dir, exist_ok=True)

# Llamar a la función summarize_from_csv
print("\nGenerando resumen de métricas:")
summary_metrics = summarize_from_csv(
    preds_csv=temp_preds_csv,
    outdir=summary_output_dir,
    # Las predicciones aquí son a nivel de bag, no de instancia de parche, por lo que no usamos 'bag-column'
    # Si tuvieras datos por parche con 'bag_id', lo especificarías aquí
    # bag_col='slide_id' # Usar slide_id como bag_id si el CSV contiene predicciones por parche
)

print("\nResumen agregado de métricas (mean y std por fold):")
display(summary_metrics)
print(f"Gráficos de ROC, PR y distribución de probabilidades guardados en: {summary_output_dir}")

"""### Visualización del `temp_predictions_A.csv`"""

temp_preds_csv = os.path.join(MIL_OUTPUT_DIR, "temp_predictions_A.csv")

df_temp_predictions = pd.read_csv(temp_preds_csv)

# Ordenar por 'fold' para una mejor visualización
df_temp_predictions_sorted = df_temp_predictions.sort_values(by='fold').reset_index(drop=True)

print(f"Contenido del archivo '{os.path.basename(temp_preds_csv)}' (primeras 10 filas, ordenadas por fold):")
display(df_temp_predictions_sorted.head(10))

"""### Visualización de los gráficos de resumen"""

import matplotlib.pyplot as plt
import os
from PIL import Image

summary_output_dir = os.path.join(MIL_OUTPUT_DIR, "summary_results")

print(f"Listando archivos en: {summary_output_dir}")
image_files = sorted([f for f in os.listdir(summary_output_dir) if f.endswith(('.png', '.jpg', '.jpeg'))])

if not image_files:
    print("No se encontraron archivos de imagen en el directorio de resultados.")
else:
    for img_name in image_files:
        img_path = os.path.join(summary_output_dir, img_name)
        print(f"\nMostrando: {img_name}")
        img = Image.open(img_path)
        plt.figure(figsize=(8, 6))
        plt.imshow(img)
        plt.axis('off')
        plt.title(img_name)
        plt.show()

