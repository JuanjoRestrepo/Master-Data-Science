# -*- coding: utf-8 -*-
"""Definitivo_Juan_Proyecto_de_Grado_Juan_Jose_y_Valentina.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MVecYuB9uj3JcFwHH7GrIHgBlptLfABM

# ***Weakly Supervised Multiple Instance Learning for Prostate Cancer Histopathology (SICAPv2)***

# **Sprint 0: Descarga / importaci√≥n del dataset**

Este bloque deja evidencia de origen de datos, trazabilidad y organizaci√≥n reproducible.

## 0.1 Definici√≥n de rutas base
"""

import os
import shutil

BASE_PATH = "/content"
DATA_ROOT = os.path.join(BASE_PATH, "sicapv2_data")
DATASET_PATH = os.path.join(DATA_ROOT, "SICAPv2")
META_PATH = os.path.join(DATASET_PATH, "metadata")

os.makedirs(META_PATH, exist_ok=True)

"""## 0.2 Importaci√≥n del dataset"""

ZIP_MAIN = "/content/sicapv2.zip"

if not os.path.exists(ZIP_MAIN):
    print("Archivo ZIP principal NO existe. Descargando...")

    # Enlace de descarga para el archivo ZIP
    download_url = "https://data.mendeley.com/public-api/zip/9xxm58dvs3/download/1"

    # Descargar el archivo ZIP y guardarlo como sicapv2.zip
    !wget -O $ZIP_MAIN $download_url
else:
    print("Archivo ZIP principal ya existe ‚Üí NO se descarga.")

"""### Descomprimimos el Archivo ZIP principal"""

INTERMEDIATE_DIR = "/content/intermediate_data"
!unzip -q $ZIP_MAIN -d $INTERMEDIATE_DIR

# Listar los contenidos para verificar
!ls /content/intermediate_data

"""## 0.3 Descomprimir el Archivo ZIP Anidado (SICAPv2.zip)

####"/content/intermediate_data/SICAPv2.../SICAPv2.zip"
"""

nested_zip = None
for root, _, files in os.walk(INTERMEDIATE_DIR):
    for f in files:
        if f.endswith(".zip"):
            nested_zip = os.path.join(root, f)
            break

assert nested_zip is not None, "ZIP SICAPv2 no encontrado"

!unzip -q "$nested_zip" -d $DATA_ROOT

assert os.path.exists(DATASET_PATH)
assert len(os.listdir(DATASET_PATH)) > 0

"""## 0.4. Limpiar y Organizar el Espacio"""

if os.path.exists(ZIP_MAIN):
    print("Eliminando ZIP principal...")

    # Eliminar el archivo ZIP principal
    !rm $ZIP_MAIN

if os.path.exists(INTERMEDIATE_DIR):
    print("Eliminando carpeta intermedia...")

    # Eliminar el directorio intermedio y su contenido de forma recursiva
    !rm -r $INTERMEDIATE_DIR

print("\nContenido de /content:")
!ls /content



"""# **Sprint 1: Entendimiento y organizaci√≥n del dataset SICAPv2**

Prop√≥sito del sprint

1. Verificar la estructura real del dataset descargado
2. Identificar qu√© informaci√≥n tenemos (im√°genes, m√°scaras, labels WSI)
3. Preparar una organizaci√≥n coherente para Aprendizaje con M√∫ltiples Instancias (MIL)
4. Dejar trazabilidad clara para el documento de tesis

## 1.1 Verificaci√≥n de la estructura del dataset

Primero, inspeccionamos qu√© hay realmente en disco. Esto es clave porque SICAPv2 no es homog√©neo entre Kaggle y Mendeley.
"""

def printTree(path, level=2):
    for root, dirs, files in os.walk(path):
        depth = root.replace(path, "").count(os.sep)
        if depth > level:
            continue
        indent = " " * 4 * depth
        print(f"{indent}{os.path.basename(root)}/")
        for f in files[:5]:
            print(f"{indent}    {f}")

printTree(DATASET_PATH, level=3)

"""## 1.2 Definici√≥n de Paths"""

IMAGES_DIR = os.path.join(DATASET_PATH, "images")
MASKS_DIR  = os.path.join(DATASET_PATH, "masks")
PARTITION_DIR = os.path.join(DATASET_PATH, "partition")

for p in [IMAGES_DIR, MASKS_DIR, PARTITION_DIR]:
    print(p, "‚Üí", os.path.exists(p))

"""## 1.3 Conteo y verificaci√≥n b√°sica de datos"""

def countFiles(path, ext=".jpg"):
    return len([f for f in os.listdir(path) if f.lower().endswith(ext)])

print("Conteo global de patches")
stats = {
    "total_images": countFiles(IMAGES_DIR),
    "total_masks": countFiles(MASKS_DIR)
}

stats

"""## 1.4 Consistencia imagen‚Äìm√°scara"""

imagesSet = set(os.listdir(IMAGES_DIR))
masksSet  = set(os.listdir(MASKS_DIR))

print("Im√°genes sin m√°scara:", len(imagesSet - masksSet))
print("M√°scaras sin imagen:", len(masksSet - imagesSet))

"""## 1.5 Particiones oficiales"""

printTree(PARTITION_DIR, level=3)

"""## 1.6 Verificamos qu√© contienen los Train.xlsx / Test.xlsx"""

import pandas as pd

samplePartition = pd.read_excel(
    os.path.join(PARTITION_DIR, "Validation", "Val1", "Train.xlsx")
)

samplePartition.head()

"""## 1.7 Verificamos si hay solapamiento entre folds (anti data leakage)"""

trainVal1 = set(
    pd.read_excel(os.path.join(PARTITION_DIR, "Validation", "Val1", "Train.xlsx"))["image_name"].apply(lambda x: x.split('_Block')[0])
)

testVal1 = set(
    pd.read_excel(os.path.join(PARTITION_DIR, "Validation", "Val1", "Test.xlsx"))["image_name"].apply(lambda x: x.split('_Block')[0])
)

print("Solapamiento Train/Test:", len(trainVal1 & testVal1))

"""## 1.8 Conteo de WSIs por partici√≥n"""

def countWSIs(path):
    df = pd.read_excel(path)
    wsiIds = df["image_name"].apply(lambda x: x.split("_Block")[0])
    return wsiIds.nunique()

for fold in ["Val1", "Val2", "Val3", "Val4"]:
    trainPath = os.path.join(PARTITION_DIR, "Validation", fold, "Train.xlsx")
    testPath  = os.path.join(PARTITION_DIR, "Validation", fold, "Test.xlsx")

    print(
        fold,
        "Train WSIs:", countWSIs(trainPath),
        "\nTest WSIs:", countWSIs(testPath)
    )

import matplotlib.pyplot as plt

folds = ["Val1", "Val2", "Val3", "Val4"]
trainWsis = [95, 97, 94, 86]
testWsis  = [29, 27, 30, 38]

x = range(len(folds))

plt.bar(x, trainWsis, label="Train")
plt.bar(x, testWsis, bottom=trainWsis, label="Test")
plt.xticks(x, folds)
plt.ylabel("N√∫mero de WSIs")
plt.title("Distribuci√≥n de WSIs por fold (SICAPv2)")
plt.legend()
plt.show()

"""### **Nota**:
Las m√°scaras de segmentaci√≥n est√°n disponibles en SICAPv2, pero NO se utilizan para el entrenamiento del modelo MIL.

Se emplean √∫nicamente para an√°lisis exploratorio y validaci√≥n interpretativa.

El dataset SICAPv2 proporciona un conjunto de particiones oficiales para validaci√≥n cruzada (Val1‚ÄìVal4), las cuales fueron empleadas para el desarrollo y evaluaci√≥n de los modelos. Adicionalmente, el dataset incluye un conjunto de prueba independiente (partition/Test), el cual se reserva como test hold-out final y no se utiliza durante el proceso de ajuste ni selecci√≥n de modelos

# **Sprint 2: Construcci√≥n del dataset_manifest.csv**

### **Objetivo del Sprint 2**

Construir un manifest √∫nico, consistente y trazable, donde:

- 1 fila = 1 parche
- Cada parche queda asociado a:
  * su WSI
  * su fold (Val1‚ÄìVal4)
  * su split (train / test)
  * su ruta a imagen
  * su ruta a m√°scara
  * su label cl√≠nico global (WSI-level)

Este archivo ser√°:
- la base de los bags MIL
- la fuente para EDA, entrenamiento y evaluaci√≥n

## Esquema propuesto del `dataset_manifest.csv`

| Columna | Descripci√≥n |
| :--- | :--- |
| **imageName** | Nombre del parche |
| **imagePath** | Ruta absoluta o relativa a la imagen |
| **maskPath** | Ruta a la m√°scara correspondiente |
| **maskExists** | Indica si la m√°scara existe (True/False) |
| **wsiId** | Identificador √∫nico de la WSI |
| **fold** | Fold de validaci√≥n cruzada (Val1‚ÄìVal4) |
| **split** | Subconjunto: train / test |
| **gleasonPrimary** | Gleason Score primario a nivel WSI |
| **gleasonSecondary** | Gleason Score secundario a nivel WSI |
| **isup** | Grado ISUP derivado a partir de Gleason Primary + Secondary |
| **nc** | Parche no canceroso (Non-Cancerous) |
| **g3** | Gleason pattern 3 (patch-level) |
| **g4** | Gleason pattern 4 (patch-level) |
| **g5** | Gleason pattern 5 (patch-level) |
| **g4c** | Gleason pattern 4 cribriforme (subtipo agresivo) |


El grado ISUP se deriva una √∫nica vez a nivel WSI durante la carga de los labels cl√≠nicos, y posteriormente se propaga a todos los parches asociados a dicha WSI, garantizando coherencia WSI-level y evitando inconsistencias a nivel patch.

### Nota
Los labels NO son patch-level, solo se registran por trazabilidad.

## 2.1 Cargar labels globales (WSI-level)
"""

import pandas as pd
import os

wsiLabelsPath = os.path.join(DATASET_PATH, "wsi_labels.xlsx")
wsiLabelsDf = pd.read_excel(wsiLabelsPath)
wsiLabelsDf.head()

wsiLabelsDf.columns

wsiLabelsDf = wsiLabelsDf.rename(columns={
    "slide_id": "WSI_ID"
})

wsiLabelsDf.head()

wsiLabelsDf.columns

"""## 2.2 Funciones auxiliares clave"""

def extractWsiId(imageName: str) -> str:
    """
    Extrae el identificador de la WSI a partir del nombre del parche.
    """
    return imageName.split("_Block")[0]

def buildImagePath(imageName):
    return os.path.join(IMAGES_DIR, imageName)

def buildMaskPath(imageName):
    return os.path.join(MASKS_DIR, imageName)

def computeISUP(gleasonPrimary, gleasonSecondary):
    total = gleasonPrimary + gleasonSecondary

    if total <= 6:
        return 1
    elif total == 7 and gleasonPrimary == 3:
        return 2
    elif total == 7 and gleasonPrimary == 4:
        return 3
    elif total == 8:
        return 4
    else:
        return 5

"""## 2.3 Procesar un fold (funci√≥n central)

### precalcular ISUP
"""

wsiLabelsDf["ISUP"] = wsiLabelsDf.apply(
    lambda r: computeISUP(r["Gleason_primary"], r["Gleason_secondary"]),
    axis=1
)

"""### Construir el diccionario cl√≠nico (YA con ISUP)"""

wsiLabelDict = (
    wsiLabelsDf
    .set_index("WSI_ID")
    .to_dict(orient="index")
)

def processFold(foldName):
    foldRows = []
    foldPath = os.path.join(PARTITION_DIR, "Validation", foldName)

    for split in ["Train", "Test"]:
        splitDf = pd.read_excel(os.path.join(foldPath, f"{split}.xlsx"))

        for _, row in splitDf.iterrows():
            imageName = row["image_name"]
            wsiId = extractWsiId(imageName)

            foldRows.append({
                "imageName": imageName,
                "imagePath": buildImagePath(imageName),
                "maskPath": buildMaskPath(imageName),
                "maskExists": os.path.exists(buildMaskPath(imageName)),
                "wsiId": wsiId,
                "fold": foldName,
                "split": split.lower(),

                # üîí Labels WSI-level (constantes)
                "gleasonPrimary": int(wsiLabelDict[wsiId]["Gleason_primary"]),
                "gleasonSecondary": int(wsiLabelDict[wsiId]["Gleason_secondary"]),
                "isup": int(wsiLabelDict[wsiId]["ISUP"]),

                # üéØ Patch-level (desde partition)
                "nc": row["NC"],
                "g3": row["G3"],
                "g4": row["G4"],
                "g5": row["G5"],
                "g4c": row["G4C"],
            })

    return pd.DataFrame(foldRows)

"""## 2.4 Construcci√≥n completa del manifest"""

allFolds = []

for fold in ["Val1", "Val2", "Val3", "Val4"]:
    print(f"Procesando {fold}...")
    allFolds.append(processFold(fold))

manifestDf = pd.concat(allFolds, ignore_index=True)
manifestDf.head()

"""## 2.5 Validaciones cr√≠ticas"""

manifestDf["isup"].apply(type).value_counts()

manifestDf.groupby("wsiId")["isup"].nunique().max()

manifestDf.shape

"""### Integridad de m√°scaras"""

manifestDf["maskExists"].value_counts()

"""### Sin valores nulos"""

manifestDf.isna().sum()

"""### Coherencia WSI‚Äìfold"""

manifestDf.groupby(["fold", "split"])["wsiId"].nunique()

"""## 2.6 Guardar el manifest"""

manifestPath = os.path.join(META_PATH, "dataset_manifest.csv")
manifestDf.to_csv(manifestPath, index=False)

print("Manifest guardado en:", manifestPath)

"""Todas las validaciones cr√≠ticas del manifest fueron superadas, incluyendo coherencia WSI-level de los labels cl√≠nicos, integridad de m√°scaras, ausencia de valores nulos y correcta separaci√≥n por fold y split. El archivo dataset_manifest.csv se considera estable y se utiliza como √∫nica fuente de verdad para los sprints posteriores.

# **Sprint 3 Preparaci√≥n MIL (Bag Construction)**

## **Objetivo del Sprint 3**

Transformar el dataset_manifest.csv en una estructura de Bags MIL, donde:

- Bag = WSI
- Instancias = patches
- Label del bag = ISUP
- Separaci√≥n estricta por fold y split

Este sprint no entrena modelos. Solo prepara datos correctamente.
"""

import pandas as pd

MANIFEST_PATH = "/content/sicapv2_data/SICAPv2/metadata/dataset_manifest.csv"

manifestDf = pd.read_csv(MANIFEST_PATH)

manifestDf.shape
manifestDf.head()

"""## 3.2 Definici√≥n formal de un Bag MIL

A nivel conceptual:

- Un bag corresponde a una WSI y contiene m√∫ltiples instancias (***patches***).
- El bag hereda el label cl√≠nico de la WSI (ISUP), mientras que las instancias pueden tener etiquetas locales (***NC, G3, G4, G5, G4C***).

## 3.3 Construcci√≥n de Bags MIL
"""

from collections import defaultdict

def buildMilBags(manifestDf):
    bags = []

    grouped = manifestDf.groupby(["fold", "split", "wsiId"])

    for (fold, split, wsiId), group in grouped:

        bag = {
            "wsiId": wsiId,
            "fold": fold,
            "split": split,
            "isup": int(group["isup"].iloc[0]),
            "gleasonPrimary": int(group["gleasonPrimary"].iloc[0]),
            "gleasonSecondary": int(group["gleasonSecondary"].iloc[0]),
            "instances": []
        }

        for _, row in group.iterrows():
            instance = {
                "imagePath": row["imagePath"],
                "maskPath": row["maskPath"],
                "patchLabel": {
                    "nc": row["nc"],
                    "g3": row["g3"],
                    "g4": row["g4"],
                    "g5": row["g5"],
                    "g4c": row["g4c"]
                }
            }
            bag["instances"].append(instance)

        bags.append(bag)

    return bags

milBags = buildMilBags(manifestDf)

len(milBags)

"""El n√∫mero de bags debe coincidir con:"""

manifestDf[["fold", "split", "wsiId"]].drop_duplicates().shape[0]

"""## 3.5 Validaciones cr√≠ticas"""

assert all(len(bag["instances"]) > 0 for bag in milBags)

"""## 3.5.2 Un bag no mezcla splits"""

for bag in milBags:
    assert bag["split"] in ["train", "test"]

"""## 3.5.3 Un bag tiene un solo label ISUP"""

for bag in milBags:
    assert isinstance(bag["isup"], int)

"""## 3.6 Estad√≠sticas MIL"""

import numpy as np

bagSizes = [len(bag["instances"]) for bag in milBags]

print("N√∫mero de bags:", len(milBags))
print("Instancias por bag (min):", np.min(bagSizes))
print("Instancias por bag (max):", np.max(bagSizes))
print("Instancias por bag (mean):", np.mean(bagSizes))

"""## 3.7 Separaci√≥n final por fold y split"""

def splitBagsByFold(milBags, foldName):
    trainBags = [b for b in milBags if b["fold"] == foldName and b["split"] == "train"]
    testBags  = [b for b in milBags if b["fold"] == foldName and b["split"] == "test"]
    return trainBags, testBags

trainVal1, testVal1 = splitBagsByFold(milBags, "Val1")

len(trainVal1), len(testVal1)

bagSizes = [len(bag["instances"]) for bag in milBags]

plt.figure()
plt.hist(bagSizes, bins=30)
plt.xlabel("N√∫mero de instancias por bag")
plt.ylabel("Frecuencia")
plt.title("Distribuci√≥n de instancias por WSI (Bags MIL)")
plt.show()

bagsDf = pd.DataFrame([
    {"fold": b["fold"], "split": b["split"]}
    for b in milBags
])

bagsDf.groupby(["fold", "split"]).size().unstack().plot(kind="bar")
plt.ylabel("N√∫mero de bags")
plt.title("Distribuci√≥n de bags por fold y split")
plt.show()

"""## **Construcci√≥n y validaci√≥n de Bags MIL**

A partir del dataset_manifest.csv, se construy√≥ una representaci√≥n basada en Aprendizaje de Instancias M√∫ltiples (MIL), donde cada Whole Slide Image (WSI) corresponde a un bag y cada parche histopatol√≥gico a una instancia. El label cl√≠nico del bag se defini√≥ a partir del grado ISUP asociado a la WSI, mientras que las instancias conservan informaci√≥n local relacionada con la distribuci√≥n de patrones histol√≥gicos (NC, G3, G4, G5 y G4C).

Se valid√≥ que:

- Cada bag contiene exclusivamente instancias pertenecientes a una √∫nica WSI.
- No existe solapamiento entre conjuntos de entrenamiento y prueba.
- Todos los bags presentan un √∫nico label ISUP.
- No existen instancias hu√©rfanas ni bags vac√≠os.

El conjunto final contiene $496$ bags, con un n√∫mero variable de instancias por bag, lo cual refleja la heterogeneidad espacial propia de las im√°genes histopatol√≥gicas y justifica el uso de modelos MIL con mecanismos de agregaci√≥n basados en atenci√≥n.

# **Sprint 4 Feature Extraction (Backbone CNN)**

## **Objetivo**

Extraer representaciones profundas (embeddings) de cada parche histopatol√≥gico mediante una CNN preentrenada. Estos embeddings se utilizar√°n posteriormente como instancias en un modelo de Multiple Instance Learning (MIL).

En este sprint no se realiza MIL ni agregaci√≥n por WSI. Se trata de una etapa cl√°sica de computer vision con transfer learning, cuyo objetivo es obtener descriptores robustos a nivel de parche.


## **1. Decisiones metodol√≥gicas (justificaci√≥n t√©cnica)**

### 1.1 Backbone CNN

- Arquitectura: ResNet50 preentrenada en ImageNet
- Pesos: preentrenada en ImageNet

Justificaci√≥n (tesis-ready):
  - Arquitectura profunda con bloques residuales que facilitan el entrenamiento estable
  - Amplio uso en tareas de histopatolog√≠a digital
  - Buen equilibrio entre capacidad representacional y costo computacional
  - Frecuentemente utilizada como baseline en enfoques MIL

## **2. Estrategia de entrenamiento**

**Backbone congelado (feature extractor)**

La red convolucional se utiliza √∫nicamente como extractor de caracter√≠sticas:
  - No se realiza fine-tuning en esta etapa
  - Todos los par√°metros del backbone permanecen congelados

Razones:
  - Dataset relativamente peque√±o en n√∫mero de WSIs
  - Riesgo alto de sobreajuste
  - Pr√°ctica est√°ndar en pipelines MIL para histopatolog√≠a

## **Sprint 4.1 ‚Äî Implementaci√≥n paso a paso**

### 4.1.1 Imports y configuraci√≥n base
"""

import torch
import torch.nn as nn
from torchvision import models, transforms
from torch.utils.data import Dataset, DataLoader
from PIL import Image

import pandas as pd
import numpy as np
from tqdm import tqdm

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
print("Using device:", DEVICE)

manifestDf.head()

"""### 4.1.2 Transformaciones de imagen

Muy importante: consistentes con ImageNet.
"""

imageTransforms = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(
        mean=[0.485, 0.456, 0.406],
        std =[0.229, 0.224, 0.225]
    )
])

"""üìå Esta normalizaci√≥n es obligatoria para garantizar compatibilidad con los pesos de ImageNet.

### 4.1.3 Dataset de parches

Se utiliza directamente el archivo `dataset_manifest.csv`, que contiene la informaci√≥n de cada parche
"""

class PatchDataset(Dataset):
    def __init__(self, manifestDf, transform=None):
        self.df = manifestDf.reset_index(drop=True)
        self.transform = transform

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        image = Image.open(row["imagePath"]).convert("RGB")

        if self.transform:
            image = self.transform(image)

        return {
            "image": image,
            "wsiId": row["wsiId"],
            "isup": int(row["isup"]),
            "fold": row["fold"],
            "split": row["split"]
        }

"""### 4.1.4 DataLoader"""

patchDataset = PatchDataset(manifestDf, transform=imageTransforms)

patchLoader = DataLoader(
    patchDataset,
    batch_size=32,
    shuffle=False,
    num_workers=2,
    pin_memory=True
)

"""### 4.1.5 Backbone ResNet50 (feature extractor)"""

resnet = models.resnet50(
    weights=models.ResNet50_Weights.IMAGENET1K_V1
)

featureExtractor = nn.Sequential(*list(resnet.children())[:-1])

for param in featureExtractor.parameters():
    param.requires_grad = False

featureExtractor = featureExtractor.to(DEVICE)
featureExtractor.eval()

"""### 4.1.6 Validaci√≥n de dimensionalidad"""

batch = next(iter(patchLoader))
imgs = batch["image"].to(DEVICE)

with torch.no_grad():
    out = featureExtractor(imgs)

print("Salida backbone:", out.shape)

out = out.squeeze(-1).squeeze(-1)
print("Salida flatten:", out.shape)

"""### 4.1.7 Extracci√≥n de embeddings (PATCH-LEVEL)"""

def extractFeatures(dataloader, model, device):
    features = []

    with torch.no_grad():
        for batch in tqdm(dataloader, desc="Extracting features"):
            imgs = batch["image"].to(device)
            out = model(imgs).squeeze(-1).squeeze(-1)

            for i in range(out.shape[0]):
                features.append({
                    "embedding": out[i].cpu().numpy(),
                    "wsiId": batch["wsiId"][i],
                    "isup": int(batch["isup"][i].item()),
                    "fold": batch["fold"][i],
                    "split": batch["split"][i],
                })

    return features

features = extractFeatures(patchLoader, featureExtractor, DEVICE)

"""### 4.1.8 Validaciones finales"""

featuresDf = pd.DataFrame(features)

assert len(featuresDf) == len(manifestDf)
assert featuresDf.groupby("wsiId")["isup"].nunique().max() == 1
assert featuresDf.groupby("wsiId")["isup"].nunique().min() == 1

np.isnan(np.vstack(featuresDf["embedding"].values)).sum()

print("Embeddings:", len(features))
print("Manifest rows:", len(manifestDf))
assert len(features) == len(manifestDf)

"""#### Dimensi√≥n del embedding"""

features[0]["embedding"].shape
# (2048,)

"""#### Coherencia WSI‚ÄìISUP"""

featuresDf.groupby("wsiId")["isup"].nunique().max()
# ‚úÖ 1

featuresDf.groupby("wsiId")["isup"].nunique().min()
# ‚úÖ 1

FEATURES_PATH = "/content/sicapv2_data/SICAPv2/metadata/patch_embeddings.pkl"
pd.to_pickle(featuresDf, FEATURES_PATH)
print("Patch embeddings guardados en:", FEATURES_PATH)

"""Debido al n√∫mero de instancias por WSI y a la dimensionalidad de los embeddings, los bags MIL no se materializan completamente en memoria en esta etapa. En su lugar, los embeddings se almacenan a nivel patch y los bags se construyen din√°micamente durante el entrenamiento, siguiendo pr√°cticas est√°ndar en aprendizaje por instancias m√∫ltiples para histopatolog√≠a digital.

Los embeddings se almacenan a nivel de parche, junto con sus metadatos (WSI, ISUP, fold y split).

### 4.1.9 Construcci√≥n l√≥gica de bags MIL (conceptual)

Debido al tama√±o de los embeddings y al n√∫mero de instancias por WSI, los bags MIL no se materializan completamente en memoria. En su lugar, los embeddings se almacenan a nivel patch y los bags se construyen din√°micamente durante el entrenamiento, siguiendo pr√°cticas est√°ndar en MIL para histopatolog√≠a.

### 4.1.10 Construcci√≥n l√≥gica de bags MIL (conceptual)

En esta etapa no se materializan expl√≠citamente los bags MIL en memoria.

Conceptualmente, cada bag corresponde al conjunto de embeddings de parches asociados a un mismo wsiId, junto con su etiqueta ISUP. Durante el entrenamiento del modelo MIL (Sprint 5), los bags se construyen din√°micamente agrupando los embeddings almacenados a nivel de parche.

Esta estrategia:

- Reduce significativamente el consumo de memoria
- Escala mejor a WSIs con miles de parches
- Sigue pr√°cticas est√°ndar en MIL aplicado a histopatolog√≠a digital

### 4.1.11 An√°lisis exploratorio opcional (PCA)
"""

from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

X = np.vstack(featuresDf["embedding"].values)
y = featuresDf["isup"].values

pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)

plt.figure(figsize=(7,6))
scatter = plt.scatter(
    X_pca[:,0],
    X_pca[:,1],
    c=y,
    cmap="tab10",
    s=5,
    alpha=0.6
)
plt.colorbar(scatter, label="ISUP")
plt.xlabel("PC1")
plt.ylabel("PC2")
plt.title("Proyecci√≥n PCA de embeddings (ResNet50)")
plt.show()

"""Este an√°lisis tiene fines exclusivamente exploratorios. No se espera una separaci√≥n clara entre clases a nivel de parche; sin embargo, la proyecci√≥n sugiere que las representaciones extra√≠das capturan informaci√≥n histopatol√≥gica relevante que ser√° explotada por el modelo MIL en etapas posteriores.

# **Sprint 5 ‚Äî MIL Dataset + Modelos**

Construir un Dataset MIL que:

- No materialice bags en memoria
- Agrupe din√°micamente patches ‚Üí WSI
- Devuelva una bag por iteraci√≥n
- Sea escalable y estable en RAM

## Decisi√≥n metodol√≥gica clave (para el documento)
- Los bags MIL no se preconstruyen.
- Se generan din√°micamente a partir de embeddings almacenados a nivel de parche.

## **Justificaci√≥n:**

- WSIs con cientos/miles de patches
- Embeddings de 2048 dimensiones
- Pr√°ctica est√°ndar en MIL histopatol√≥gico moderno

## 5.1 Dataset MIL din√°mico
"""

class MILDataset(torch.utils.data.Dataset):
    def __init__(self, featuresDf, split="train", fold=None):
        """
        Dataset MIL din√°mico.
        Cada __getitem__ devuelve un bag completo (WSI).
        """
        self.df = featuresDf.copy()

        if fold is not None:
            self.df = self.df[self.df["fold"] == fold]

        self.df = self.df[self.df["split"] == split]

        # Lista √∫nica de WSIs
        self.wsiIds = self.df["wsiId"].unique()

        # Agrupaci√≥n lazy
        self.grouped = self.df.groupby("wsiId")

    def __len__(self):
        return len(self.wsiIds)

    def __getitem__(self, idx):
        wsiId = self.wsiIds[idx]
        group = self.grouped.get_group(wsiId)

        embeddings = torch.tensor(
            np.vstack(group["embedding"].values),
            dtype=torch.float32
        )

        label = int(group["isup"].iloc[0])

        return {
            "wsiId": wsiId,
            "embeddings": embeddings,
            "label": label
        }

"""### 5.1.2 Validaciones

Esperado:

- embeddings.shape ‚Üí (N_i, 2048)
- label ‚àà {1,2,3,4,5}
- N_i variable entre WSIs
"""

trainDataset = MILDataset(
    featuresDf,
    split="train",
    fold="Val1"
)

sample = trainDataset[0]

print("WSI:", sample["wsiId"])
print("Embeddings shape:", sample["embeddings"].shape)
print("Label:", sample["label"])

"""### 5.1.3 DataLoader MIL"""

trainLoader = torch.utils.data.DataLoader(
    trainDataset,
    batch_size=1,
    shuffle=True
)

"""## 5.2 Modelo MIL ‚Äì Mean Pooling

Se implementa el primer modelo MIL real del pipeline
Es obligatorio como baseline antes de Attention MIL.


Implementar un modelo Multiple Instance Learning con Mean Pooling, donde:

- Cada parche ‚Üí embedding (ya extra√≠do)
- Cada WSI ‚Üí bag de embeddings
- La agregaci√≥n se hace mediante promedio
- Se predice ISUP a nivel WSI

Este modelo servir√° como:
- Baseline cuantitativo
- Punto de comparaci√≥n contra Attention MIL
- Control experimental (¬øla atenci√≥n realmente aporta?)
"""

class MeanMIL(nn.Module):
    def __init__(self, inputDim=2048, numClasses=6):
        super().__init__()

        self.classifier = nn.Linear(inputDim, numClasses)

    def forward(self, embeddings):
        """
        embeddings: Tensor de forma (N, D)
        """
        bagEmbedding = embeddings.mean(dim=0)      # (D,)
        logits = self.classifier(bagEmbedding)     # (numClasses,)

        return logits, None

"""### 5.2.2 Inicializaci√≥n del modelo"""

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

modelMean = MeanMIL(
    inputDim=2048,
    numClasses=6
).to(DEVICE)

modelMean

"""### 5.2.3 Forward pass por bag (validaci√≥n cr√≠tica)

Logits shape: torch.Size([6])

Label: int
"""

sample = trainDataset[0]

embeddings = sample["embeddings"].to(DEVICE)
label = sample["label"]

logits, _ = modelMean(embeddings)

print("Logits shape:", logits.shape)
print("Label:", label)

"""### 5.2.4 Validaci√≥n sobre varios bags"""

for i in range(5):
    bag = trainDataset[i]
    logits, _ = modelMean(bag["embeddings"].to(DEVICE))

    assert logits.shape == (6,)

"""NOTA:
El modelo Mean MIL utiliza una agregaci√≥n uniforme sobre todas las instancias del bag. Aunque simple, este enfoque constituye un baseline fuerte y ampliamente utilizado en MIL, permitiendo evaluar si mecanismos m√°s complejos (e.g., atenci√≥n) aportan mejoras reales en desempe√±o.

## 5.3 Modelo MIL ‚Äì Attention-based MIL (ABMIL)

Implementar un modelo Multiple Instance Learning con mecanismo de atenci√≥n, donde:

- Cada parche aporta de forma no uniforme a la predicci√≥n del WSI
- El modelo aprende qu√© parches son relevantes

Se obtiene:

- Predicci√≥n ISUP por WSI
- Pesos de atenci√≥n por parche (interpretabilidad)

## Objetivo
Implementar un modelo Multiple Instance Learning con mecanismo de atenci√≥n, donde:

- Cada parche aporta de forma no uniforme a la predicci√≥n del WSI
- El modelo aprende qu√© parches son relevantes

Se obtiene:

- Predicci√≥n ISUP por WSI
- Pesos de atenci√≥n por parche (interpretabilidad)

***Este ser√° el modelo principal del proyecto.***

### Formulaci√≥n matem√°tica (tesis-ready)

Dado un bag $B = \{x_1, \ldots, x_N\}$, con $x_i \in \mathbb{R}^{2048}$:

#### Atenci√≥n (Ilse et al., 2018)

$$
a_i = \frac{\exp(w^\top \tanh(Vx_i))}{\sum_j \exp(w^\top \tanh(Vx_j))}
$$

$$z = \sum_{i=1}^N a_i x_i$$

$$\hat{y} = f(z)$$


Donde:

$a_i$: peso de atenci√≥n del parche $i$

$z$: representaci√≥n agregada del bag

$f(\cdot)$: clasificador
"""

class AttentionMIL(nn.Module):
    def __init__(self, inputDim=2048, hiddenDim=256, numClasses=6):
        super().__init__()

        self.attention = nn.Sequential(
            nn.Linear(inputDim, hiddenDim),
            nn.Tanh(),
            nn.Linear(hiddenDim, 1)
        )

        self.classifier = nn.Linear(inputDim, numClasses)

    def forward(self, embeddings):
        """
        embeddings: Tensor (N, D)
        """
        # Atenci√≥n
        A = self.attention(embeddings)          # (N, 1)
        A = torch.softmax(A, dim=0)             # normalizaci√≥n sobre instancias

        # Agregaci√≥n ponderada
        bagEmbedding = torch.sum(A * embeddings, dim=0)  # (D,)

        # Clasificaci√≥n
        logits = self.classifier(bagEmbedding)  # (numClasses,)

        return logits, A

"""#### Inicializaci√≥n del modelo"""

modelAtt = AttentionMIL(
    inputDim=2048,
    hiddenDim=256,
    numClasses=6
).to(DEVICE)

modelAtt

"""#### Forward pass por bag"""

sample = trainDataset[0]

embeddings = sample["embeddings"].to(DEVICE)
label = sample["label"]

logits, A = modelAtt(embeddings)

print("Logits shape:", logits.shape)
print("Attention shape:", A.shape)
print("Attention sum:", A.sum().item())

for i in range(5):
    bag = trainDataset[i]
    logits, A = modelAtt(bag["embeddings"].to(DEVICE))

    assert logits.shape == (6,)
    assert abs(A.sum().item() - 1.0) < 1e-4

"""NOTA

A diferencia del Mean MIL, el modelo ABMIL asigna pesos de atenci√≥n aprendidos a cada parche, permitiendo al modelo enfocarse en regiones histopatol√≥gicas relevantes y atenuar la contribuci√≥n de parches no informativos. Este mecanismo introduce interpretabilidad expl√≠cita a nivel de instancia, aspecto clave en aplicaciones cl√≠nicas.

üß† Ventajas frente a Mean MIL

| Aspecto | Mean MIL | Attention MIL |
| :--- | :---: | :---: |
| Pesos por parche | Uniformes | Aprendidos |
| Manejo de ruido | Limitado | Mejor |
| Interpretabilidad | ‚ùå | ‚úÖ |
| Capacidad expresiva | Baja | Alta |

## 5.4 Funci√≥n de p√©rdida y setup de entrenamiento

### 5.4.1 Loss function

Clasificaci√≥n multiclase ISUP (1‚Äì5, opcionalmente 0):
"""

criterion = nn.CrossEntropyLoss()

"""### 5.4.2 Optimizer
Solo se entrenan par√°metros del modelo MIL (no el backbone):
"""

optimizer = torch.optim.Adam(
    modelAtt.parameters(),
    lr=1e-4,
    weight_decay=1e-5
)

"""### 5.4.3 Training step (una WSI)"""

def trainStep(model, bag, optimizer, criterion, device):
    model.train()

    embeddings = bag["embeddings"].to(device)
    label = torch.tensor([bag["label"]], device=device)

    optimizer.zero_grad()

    logits, _ = model(embeddings)
    loss = criterion(logits.unsqueeze(0), label)

    loss.backward()
    optimizer.step()

    pred = logits.argmax().item()

    return loss.item(), pred

"""### 5.4.4 Validation step"""

@torch.no_grad()
def valStep(model, bag, criterion, device):
    model.eval()

    embeddings = bag["embeddings"].to(device)
    label = torch.tensor([bag["label"]], device=device)

    logits, _ = model(embeddings)
    loss = criterion(logits.unsqueeze(0), label)

    pred = logits.argmax().item()

    return loss.item(), pred

"""### 5.4.5 Epoch loop"""

def runEpoch(model, dataset, optimizer, criterion, device, train=True):
    totalLoss = 0
    yTrue, yPred = [], []

    for bag in dataset:
        if train:
            loss, pred = trainStep(model, bag, optimizer, criterion, device)
        else:
            loss, pred = valStep(model, bag, criterion, device)

        totalLoss += loss
        yTrue.append(bag["label"])
        yPred.append(pred)

    avgLoss = totalLoss / len(dataset)
    accuracy = (np.array(yTrue) == np.array(yPred)).mean()

    return avgLoss, accuracy

"""### 5.4.6 Entrenamiento completo (por fold)"""

FEATURES_PATH = "/content/sicapv2_data/SICAPv2/metadata/patch_embeddings.pkl"
featuresDf = pd.read_pickle(FEATURES_PATH)

trainDataset = MILDataset(
    featuresDf,
    split="train",
    fold="Val1"
)

valDataset = MILDataset(
    featuresDf,
    split="test",
    fold="Val1"
)

EPOCHS = 30

for epoch in range(EPOCHS):
    trainLoss, trainAcc = runEpoch(
        modelAtt,
        trainDataset,
        optimizer,
        criterion,
        DEVICE,
        train=True
    )

    valLoss, valAcc = runEpoch(
        modelAtt,
        valDataset,
        optimizer,
        criterion,
        DEVICE,
        train=False
    )

    print(
        f"Epoch {epoch+1:02d} | "
        f"Train Loss: {trainLoss:.4f}, Acc: {trainAcc:.3f} | "
        f"Val Loss: {valLoss:.4f}, Acc: {valAcc:.3f}"
    )

"""## üìä **An√°lisis del Entrenamiento y Diagn√≥stico de Desempe√±o**

### 1. Resumen de M√©tricas Finales (√âpoca 30)
| Conjunto | Loss | Accuracy | Estado |
| :--- | :--- | :--- | :--- |
| **Entrenamiento** | $0.2807 \downarrow$ | $95.8\% \uparrow$ | Convergencia √≥ptima |
| **Validaci√≥n** | $1.9453 \uparrow$ | $31.0\% \downarrow$ | Divergencia (Overfitting) |

### 2. Diagn√≥stico T√©cnico
Se observa una mejora progresiva y robusta en las m√©tricas de entrenamiento; sin embargo, el desempe√±o en validaci√≥n alcanza su punto √≥ptimo prematuramente en la **√âpoca 11 ($Loss: 1.4183$)**, para luego degradarse de forma sostenida.



Esta divergencia entre ambas curvas es un indicador claro de **sobreajuste (overfitting)**. El comportamiento es consistente con la alta capacidad del modelo de atenci√≥n frente al tama√±o reducido del conjunto de entrenamiento (~95 WSIs). Los resultados sugieren que el modelo est√° memorizando ruido o caracter√≠sticas espec√≠ficas del set de entrenamiento en lugar de generalizar patrones subyacentes.

### 3. Observaciones Clave
* **Capacidad del Modelo:** El hecho de que el *Train Accuracy* alcance el $96\%$ confirma que el mecanismo de atenci√≥n y los embeddings (ResNet50) son capaces de capturar la informaci√≥n necesaria para la tarea. **No es un bug**, es una se√±al de que el modelo "tiene potencia".
* **Limitaciones del Dataset:** El volumen de datos es cr√≠tico. Con bags peque√±os, el modelo asocia configuraciones espec√≠ficas de instancias con la etiqueta de la WSI con demasiada facilidad.
* **Estado de la Arquitectura:** Los embeddings fijos (congelados) limitan la adaptaci√≥n de las caracter√≠sticas, mientras que la atenci√≥n (alta capacidad) intenta compensar sobreajustando los pesos.

### 4. Pr√≥ximos Pasos (Propuesta)
Para mitigar este comportamiento y mejorar la generalizaci√≥n, se justifica la implementaci√≥n de:
1.  **Regularizaci√≥n Fuerte:** Incorporar *Dropout* en las capas de atenci√≥n y aumentar el *Weight Decay*.
2.  **Early Stopping:** Interrumpir el entrenamiento cerca de la √©poca 11-13 para conservar el mejor estado de validaci√≥n.
3.  **Comparativa:** Evaluar este modelo contra un agregador simple (*Mean/Max Pooling*) para cuantificar el beneficio real de la atenci√≥n en este volumen de datos.

## Sprint 5.5 ‚Äî Cross-Validation + Comparaci√≥n Mean MIL vs Attention MIL

### Objetivo:
Evaluar de forma rigurosa dos estrategias de agregaci√≥n en Multiple Instance Learning a nivel WSI:

- Mean MIL (baseline)
- Attention MIL (ABMIL)

utilizando cross-validation por WSI, reportando m√©tricas agregadas y controlando el sobreajuste.

### 5.5.1 Configuraci√≥n global

Esta secci√≥n define el experimental setup.
Todo lo que est√© aqu√≠ afecta todos los folds y modelos.
"""

import random
import torch
import numpy as np

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

EPOCHS = 30
LR = 1e-4
NUM_CLASSES = 6

SEED = 42

torch.manual_seed(SEED)
np.random.seed(SEED)
random.seed(SEED)

if torch.cuda.is_available():
    torch.cuda.manual_seed_all(SEED)

"""### 5.5.2 Folds de validaci√≥n"""

FOLDS = featuresDf["fold"].unique()
print("Folds:", FOLDS)

"""### 5.5.3 M√©tricas de evaluaci√≥n"""

from sklearn.metrics import accuracy_score, f1_score

"""### 5.5.4 Funci√≥n de evaluaci√≥n (WSI-level)"""

@torch.no_grad()
def evaluateModel(model, dataset, device):
    model.eval()

    allPreds = []
    allLabels = []

    for i in range(len(dataset)):
        sample = dataset[i]

        embeddings = sample["embeddings"].to(device)
        label = sample["label"]

        logits, _ = model(embeddings)
        pred = logits.argmax(dim=0).item()

        allPreds.append(pred)
        allLabels.append(label)

    acc = accuracy_score(allLabels, allPreds)
    f1 = f1_score(allLabels, allPreds, average="macro")

    return acc, f1

"""**La evaluaci√≥n se realiza por bag (WSI), no por parche, coherente con el setting MIL.**

### 5.5.5 Loop de entrenamiento y evaluaci√≥n por fold
"""

results = []

# Create the checkpoints directory if it doesn't exist
os.makedirs("checkpoints", exist_ok=True)

for fold in FOLDS:
    print(f"\n===== Fold {fold} ====")

    trainDataset = MILDataset(
        featuresDf,
        split="train",
        fold=fold
    )

    valDataset = MILDataset(
        featuresDf,
        split="test",
        fold=fold
    )

    for modelName in ["mean", "attention"]:
        print(f"\n--- Model: {modelName} ---")

        if modelName == "mean":
            model = MeanMIL(
                inputDim=2048,
                numClasses=NUM_CLASSES
            )
        else:
            model = AttentionMIL(
                inputDim=2048,
                hiddenDim=256,
                numClasses=NUM_CLASSES
            )

        model.to(DEVICE)

        optimizer = torch.optim.Adam(
            model.parameters(),
            lr=LR,
            weight_decay=1e-4
        )

        criterion = torch.nn.CrossEntropyLoss()

        # Entrenamiento
        for epoch in range(EPOCHS):
            trainLoss, trainAcc = runEpoch(
                model,
                trainDataset,
                optimizer,
                criterion,
                DEVICE,
                train=True
            )

        # Evaluaci√≥n
        valAcc, valF1 = evaluateModel(
            model,
            valDataset,
            DEVICE
        )

        results.append({
            "fold": fold,
            "model": modelName,
            "valAccuracy": valAcc,
            "valF1": valF1
        })

        print(
            f"Fold {fold} | "
            f"Acc: {valAcc:.3f} | "
            f"F1-macro: {valF1:.3f}"
        )

        # Save the attention model checkpoint after training for the current fold
        if modelName == "attention":
            checkpointPath = f"checkpoints/attention_fold_{fold}.pt"
            torch.save(model.state_dict(), checkpointPath)
            print(f"  Attention model for {fold} saved to {checkpointPath}")

"""***No se realiza early stopping ni hyperparameter tuning en esta etapa para garantizar una comparaci√≥n justa entre agregadores.***

### 5.5.6 Resultados agregados (Cross-Validation)
"""

resultsDf = pd.DataFrame(results)

print("Resultados por fold:")
display(resultsDf)

"""### Resultados promedio por modelo"""

summaryDf = (
    resultsDf
    .groupby("model")[["valAccuracy", "valF1"]]
    .mean()
    .reset_index()
)

print("Resultados promedio (cross-validation):")
display(summaryDf)

"""## **Discusi√≥n de resultados ‚Äî Cross-Validation MIL**

Se evaluaron dos estrategias de agregaci√≥n MIL (Mean Pooling y Attention-based MIL)
mediante validaci√≥n cruzada a nivel WSI, utilizando m√©tricas cl√≠nicas relevantes
(Accuracy y F1-score macro).

### Resultados clave

- El modelo **Mean MIL** obtuvo una mayor accuracy promedio, reflejando un buen
  desempe√±o en clases dominantes.
- El modelo **Attention MIL** alcanz√≥ un **F1-score macro superior**, indicando
  una mejor capacidad para modelar clases minoritarias y un comportamiento m√°s
  equilibrado entre grados ISUP.

### Interpretaci√≥n cl√≠nica

Dado el car√°cter desbalanceado del problema y la relevancia cl√≠nica de detectar
grados ISUP altos, el F1-score macro constituye una m√©trica m√°s adecuada que la
accuracy global. Bajo este criterio, el modelo Attention MIL demuestra una ventaja
clara frente al baseline Mean MIL.

### Conclusi√≥n

Estos resultados sugieren que el mecanismo de atenci√≥n permite al modelo enfocar
su decisi√≥n en parches histopatol√≥gicos relevantes, mejorando la discriminaci√≥n
entre clases, aun cuando la accuracy global no siempre se incremente.

En consecuencia, el modelo Attention MIL se selecciona como arquitectura principal
para los an√°lisis de interpretabilidad desarrollados en el Sprint 6.

# **Sprint 6 ‚Äî Interpretabilidad (Attention-based MIL)**

Interpretar c√≥mo y por qu√© el modelo MIL con atenci√≥n toma decisiones a nivel WSI, analizando:

- La distribuci√≥n de pesos de atenci√≥n
- La importancia relativa de los parches
- La coherencia cl√≠nica de las regiones m√°s atendidas
- La relaci√≥n entre atenci√≥n y grado ISUP

‚ö†Ô∏è Importante:
* Este sprint no modifica ni reentrena el modelo.
* Es post-hoc interpretability.

## 6.1 Preparaci√≥n del modelo para inferencia interpretativa -- Seleccionar fold y cargar modelo

Usamos el fold con mejor F1 en Attention (Val2)
"""

INTERPRET_FOLD = "Val2"
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

"""### Cargar modelo entrenado"""

modelAtt = AttentionMIL(
    inputDim=2048,
    hiddenDim=256,
    numClasses=NUM_CLASSES
)

checkpointPath = f"checkpoints/attention_fold_{INTERPRET_FOLD}.pt"
modelAtt.load_state_dict(torch.load(checkpointPath,
                                    map_location=DEVICE))

modelAtt.to(DEVICE)
modelAtt.eval()

"""### Verificaci√≥n r√°pida (sanity check)"""

sample = interpretDataset[0]

print(sample["wsiId"])
print(sample["embeddings"].shape)   # (N, 2048)
print(sample["label"])

"""## 6.2 ‚Äî Extracci√≥n de atenci√≥n por WSI (c√≥digo completo)

### Estructura de almacenamiento

Dise√±ada para:

- an√°lisis estad√≠stico
- visualizaci√≥n
- trazabilidad cient√≠fica

### Extracci√≥n de pesos de atenci√≥n por WSI

Usamos el mismo MILDataset, sin cambios:
"""

interpretDataset = MILDataset(
    featuresDf,
    split="test",
    fold=INTERPRET_FOLD
)

"""### Loop de inferencia interpretativa (core del sprint)"""

import torch.nn.functional as F

def extractAttentionOutputs(model, dataset, device):
    outputs = []

    with torch.no_grad():
        for i in range(len(dataset)):
            sample = dataset[i]

            embeddings = sample["embeddings"].to(device)
            label = sample["label"]
            wsiId = sample["wsiId"]

            logits, att = model(embeddings)

            att = att.squeeze(1)          # (N,)
            att = att.cpu().numpy()

            outputs.append({
                "wsiId": wsiId,
                "label": label,
                "logits": logits.cpu().numpy(),
                "attention": att,
                "numPatches": len(att)
            })

    return outputs

attentionOutputs = extractAttentionOutputs(
    modelAtt,
    interpretDataset,
    DEVICE
)

"""## 6.3 An√°lisis cuantitativo de la atenci√≥n

Check 1 ‚Äî Verificaci√≥n b√°sica (sanity checks)

Confirma:

- Atenci√≥n normalizada (softmax)
- Interpretabilidad v√°lida
"""

sample = attentionOutputs[0]

assert np.isclose(sample["attention"].sum(), 1.0, atol=1e-4)
assert sample["attention"].min() >= 0

"""### Check 2 ‚Äî Distribuci√≥n de atenci√≥n por WSI"""

att = sample["attention"]

print("Min attention:", att.min())
print("Max attention:", att.max())
print("Top 10% mass:", np.sort(att)[-int(0.1*len(att)):].sum())

plt.hist(sample["attention"], bins=50)
plt.xlabel("Peso de atenci√≥n")
plt.ylabel("Frecuencia")
plt.title(f"Distribuci√≥n de atenci√≥n ‚Äì WSI {sample['wsiId']}")
plt.show()

"""Interpretaci√≥n t√≠pica:

- Distribuci√≥n altamente sesgada
- Pocos parches concentran la mayor atenci√≥n
- Comportamiento esperado en MIL histopatol√≥gico

## 6.4 Identificaci√≥n de parches m√°s relevantes (Top-K)

Selecci√≥n de top-k parches
"""

def getTopKPatches(sample, k=10):
    att = sample["attention"]
    topIdx = np.argsort(att)[-k:][::-1]

    return topIdx, att[topIdx]

topIdx, topAtt = getTopKPatches(sample, k=10)

topAtt

"""Estos parches son:

- Los que m√°s influyen en la predicci√≥n del WSI
- Candidatos directos a inspecci√≥n cl√≠nica

### Asociaci√≥n con rutas de imagen (opcional visual)
"""

# ================================
# FIX Sprint 6.4.2 ‚Äì imagePath
# ================================

# Re-asociar imagePath desde manifestDf (misma longitud y orden)
featuresDf = featuresDf.copy()
featuresDf["imagePath"] = manifestDf["imagePath"].values

# ================================
# 6.4.2 Asociaci√≥n con rutas de imagen
# ================================

# Subset del WSI analizado
wsiDf = featuresDf[
    featuresDf["wsiId"] == sample["wsiId"]
].reset_index(drop=True)

# Top-K parches seg√∫n atenci√≥n
topPatchesDf = wsiDf.iloc[topIdx]

# Verificaci√≥n
topPatchesDf[["imagePath", "isup"]]

"""Esto permite:

- Visualizaci√≥n directa
- Validaci√≥n con pat√≥logo
- Figuras interpretables para tesis

## 6.5 An√°lisis estad√≠stico global de atenci√≥n

6.5.1 Entrop√≠a de atenci√≥n (concentraci√≥n)
"""

from scipy.stats import entropy
import matplotlib.pyplot as plt
import numpy as np

def attentionEntropy(att):
    att = np.asarray(att)
    return entropy(att + 1e-8)

entropies = [
    attentionEntropy(o["attention"])
    for o in attentionOutputs
]

"""### Histograma global"""

plt.figure(figsize=(6,4))
plt.hist(entropies, bins=30)
plt.xlabel("Entrop√≠a de atenci√≥n")
plt.ylabel("N√∫mero de WSIs")
plt.title("Concentraci√≥n de atenci√≥n por WSI")
plt.show()

"""### WSIs con baja entrop√≠a
- El modelo concentra casi toda la atenci√≥n en muy pocos parches
- Indica foco morfol√≥gico claro, t√≠pico de lesiones tumorales bien definidas

WSIs con alta entrop√≠a
- Atenci√≥n distribuida en muchos parches
- Puede corresponder a:
  * Tumores difusos
  * Casos borderline
  * WSIs con alto ruido / heterogeneidad

## 6.6 Atenci√≥n vs ISUP (an√°lisis cl√≠nico)
"""

import pandas as pd

analysisDf = pd.DataFrame({
    "wsiId": [o["wsiId"] for o in attentionOutputs],
    "isup": [o["label"] for o in attentionOutputs],
    "entropy": entropies,
    "numPatches": [o["numPatches"] for o in attentionOutputs]
})

analysisDf.groupby("isup")["entropy"].mean()

"""Lectura cl√≠nica correcta

* ISUP 5 (alto grado)
  - Menor entrop√≠a
  - El modelo se enfoca en regiones muy espec√≠ficas
‚úîÔ∏è Coherente con patrones morfol√≥gicos agresivos

* ISUP 2
  - Entrop√≠a muy alta
  - Casos ambiguos, heterog√©neos, dif√≠ciles incluso para humanos
‚úîÔ∏è Muy realista

* Tendencia general:
  - A mayor agresividad tumoral ‚Üí atenci√≥n m√°s focalizada

  El NaN en ISUP 2 es solo por bajo N, no es un error.
"""

analysisDf.groupby("isup")[["entropy", "numPatches"]].agg(["mean", "std"])

"""### Relaci√≥n entrop√≠a ‚Äì n√∫mero de parches

Tambi√©n tiene sentido:

- ISUP altos ‚Üí menos parches relevantes
- ISUP bajos ‚Üí m√°s tejido benigno / variabilidad

Esto refuerza que el modelo:

- No depende del n√∫mero de patches
- Aprende patrones discriminativos reales

# **Sprint 7 ‚Äî Binarizaci√≥n ISUP + Reentrenamiento MIL**
## Objetivo del Sprint 7

Reformular el problema de clasificaci√≥n multiclase ISUP (0‚Äì5) a un escenario binario cl√≠nicamente relevante, y evaluar:

- Si la binarizaci√≥n mejora la generalizaci√≥n
- Si el modelo Attention MIL mantiene ventaja frente a Mean MIL
- Si la atenci√≥n se vuelve a√∫n m√°s focalizada

## 7.1 Decisi√≥n cl√≠nica y metodol√≥gica

### Definici√≥n de binarizaci√≥n

Usaremos el criterio est√°ndar en literatura prost√°tica:

| Grado ISUP Original | Categor√≠a Binaria | Etiqueta (*Label*) |
| :--- | :--- | :---: |
| **ISUP $\leq$ 2** | Bajo Grado (*Low-grade*) | `0` |
| **ISUP $\geq$ 3** | Alto Grado (*High-grade*) | `1` |

 **Contexto Cl√≠nico:** La elecci√≥n de este umbral ($ISUP \ 3$ como punto de corte) es est√°ndar en diversas tareas de patolog√≠a computacional, ya que suele marcar la transici√≥n hacia un comportamiento cl√≠nico m√°s agresivo en el c√°ncer de pr√≥stata.

üëâ Justificaci√≥n:

- ISUP ‚â• 3 implica riesgo cl√≠nico significativo
- Separaci√≥n usada en trabajos MIL SOTA
- Reduce ambig√ºedad intermedia
- Aumenta estabilidad estad√≠stica

Esto NO invalida el an√°lisis multiclase previo.
Es un experimento complementario, no un reemplazo.

## 7.2 Estrategia general del Sprint 7

NO se cambia nada del pipeline estructural:

- Mismos embeddings (Sprint 4)
- Mismo Dataset MIL din√°mico (Sprint 5)
- Mismas arquitecturas (Mean / Attention)
- Mismo cross-validation por fold

√önico cambio:
- Re-etiquetado binario
- Ajuste de numClasses = 2
- Nuevas m√©tricas

## 7.3 Paso 1 ‚Äî Crear etiqueta binaria (sin tocar datos originales)
"""

# =========================
# Sprint 7.3 ‚Äì Binarizaci√≥n ISUP
# =========================

featuresDf_bin = featuresDf.copy()

def binarizeIsup(isup):
    return 0 if isup <= 2 else 1

featuresDf_bin["labelBin"] = featuresDf_bin["isup"].apply(binarizeIsup)

# Sanity check
featuresDf_bin["labelBin"].value_counts()

"""Nunca sobreescribimos isup

Esto es clave para trazabilidad cient√≠fica.

## 7.4 Paso 2 ‚Äî Dataset MIL binario (m√≠nimo cambio)

Extendemos el dataset sin duplicar l√≥gica:
"""

class MILDatasetBinary(torch.utils.data.Dataset):
    def __init__(self, featuresDf, split="train", fold=None):
        self.df = featuresDf.copy()

        if fold is not None:
            self.df = self.df[self.df["fold"] == fold]

        self.df = self.df[self.df["split"] == split]

        self.wsiIds = self.df["wsiId"].unique()
        self.grouped = self.df.groupby("wsiId")

    def __len__(self):
        return len(self.wsiIds)

    def __getitem__(self, idx):
        wsiId = self.wsiIds[idx]
        group = self.grouped.get_group(wsiId)

        embeddings = torch.tensor(
            np.vstack(group["embedding"].values),
            dtype=torch.float32
        )

        label = int(group["labelBin"].iloc[0])

        return {
            "wsiId": wsiId,
            "embeddings": embeddings,
            "label": label
        }

"""Misma estructura ‚Üí cero riesgo de bugs nuevos

## 7.5 Paso 3 ‚Äî Modelos MIL binarios

### 7.5.1 Mean MIL binario
"""

modelMeanBin = MeanMIL(
    inputDim=2048,
    numClasses=2
).to(DEVICE)

"""### 7.5.2 Attention MIL binario"""

modelAttBin = AttentionMIL(
    inputDim=2048,
    hiddenDim=256,
    numClasses=2
).to(DEVICE)

"""## 7.6 Paso 4 ‚Äî Setup de entrenamiento (binario)"""

criterion = nn.CrossEntropyLoss()

optimizer = torch.optim.Adam(
    modelAttBin.parameters(),
    lr=1e-4,
    weight_decay=1e-4
)

"""Mantenemos hiperpar√°metros para comparaci√≥n justa

## 7.7 Paso 5 ‚Äî M√©tricas (cl√≠nicamente adecuadas)

En binario NO basta accuracy. Evaluaremos:

- Accuracy
- F1-score
- ROC-AUC
"""

from sklearn.metrics import accuracy_score, f1_score, roc_auc_score

"""## 7.8 Paso 6 ‚Äî Cross-validation binaria (core del sprint)"""

resultsBin = []

torch.manual_seed(42)
np.random.seed(42)

for fold in FOLDS:
    print(f"\n===== Fold {fold} =====")

    trainDataset = MILDatasetBinary(
        featuresDf_bin,
        split="train",
        fold=fold
    )

    valDataset = MILDatasetBinary(
        featuresDf_bin,
        split="test",
        fold=fold
    )

    for modelName in ["mean", "attention"]:
        print(f"\n--- Model: {modelName} ---")

        if modelName == "mean":
            model = MeanMIL(inputDim=2048, numClasses=2)
        else:
            model = AttentionMIL(inputDim=2048, hiddenDim=256, numClasses=2)

        model.to(DEVICE)

        optimizer = torch.optim.Adam(
            model.parameters(),
            lr=1e-4,
            weight_decay=1e-4
        )

        criterion = nn.CrossEntropyLoss()

        # Entrenamiento
        for epoch in range(EPOCHS):
            runEpoch(
                model,
                trainDataset,
                optimizer,
                criterion,
                DEVICE,
                train=True
            )

        # Evaluaci√≥n
        acc, f1 = evaluateModel(model, valDataset, DEVICE)

        resultsBin.append({
            "fold": fold,
            "model": modelName,
            "valAccuracy": acc,
            "valF1": f1
        })

        print(f"Acc: {acc:.3f} | F1: {f1:.3f}")

"""## 7.9 Paso 7 ‚Äî Resultados agregados"""

resultsBinDf = pd.DataFrame(resultsBin)

resultsBinDf.groupby("model")[["valAccuracy", "valF1"]].mean()

"""## Implementaci√≥n de Kappa


"""

from sklearn.metrics import cohen_kappa_score

def evaluateModel(model, dataset, device):
    model.eval()

    yTrue = []
    yPred = []

    with torch.no_grad():
        for i in range(len(dataset)):
            sample = dataset[i]

            embeddings = sample["embeddings"].to(device)
            label = sample["label"]

            logits, _ = model(embeddings)
            pred = torch.argmax(logits).item()

            yTrue.append(label)
            yPred.append(pred)

    acc = accuracy_score(yTrue, yPred)
    f1 = f1_score(yTrue, yPred)
    kappa = cohen_kappa_score(yTrue, yPred)

    return acc, f1, kappa

"""### Guardar resultados por fold"""

results = []

for foldName in FOLDS: # Iterate over fold names

    print(f"\n===== Fold {foldName} ====")

    # Datasets del fold
    trainDataset = MILDatasetBinary(
        featuresDf_bin,
        split="train",
        fold=foldName
    )

    valDataset = MILDatasetBinary(
        featuresDf_bin,
        split="test",
        fold=foldName
    )

    for modelName in ["mean", "attention"]:
        print(f"\n--- Model: {modelName} ---")

        if modelName == "mean":
            model = MeanMIL(inputDim=2048, numClasses=2)
        else:
            model = AttentionMIL(
                inputDim=2048,
                hiddenDim=256,
                numClasses=2
            )

        model.to(DEVICE)

        optimizer = torch.optim.Adam(
            model.parameters(),
            lr=LR,
            weight_decay=1e-4
        )

        criterion = nn.CrossEntropyLoss()

        # Entrenamiento
        for epoch in range(EPOCHS):
            runEpoch(
                model,
                trainDataset,
                optimizer,
                criterion,
                DEVICE,
                train=True
            )

        # Evaluaci√≥n
        acc, f1, kappa = evaluateModel(
            model,
            valDataset,
            DEVICE
        )

        print(f"Acc: {acc:.3f} | F1: {f1:.3f} | Kappa: {kappa:.3f}")

        results.append({
            "fold": foldName,
            "model": modelName,
            "valAccuracy": acc,
            "valF1": f1,
            "valKappa": kappa
        })

resultsDf2 = pd.DataFrame(results)

summaryDf2 = (
    resultsDf2
    .groupby("model")[["valAccuracy", "valF1", "valKappa"]]
    .agg(["mean", "std"])
)

summaryDf2

import seaborn as sns
import matplotlib.pyplot as plt

sns.set(style="whitegrid")

# Accuracy por fold
plt.figure(figsize=(8,4))
sns.barplot(
    data=resultsDf2,
    x="fold",
    y="valAccuracy",
    hue="model"
)
plt.title("Accuracy por fold (ISUP binario)")
plt.ylabel("Accuracy")
plt.xlabel("Fold")
plt.legend(title="Modelo")
plt.show()

# Kappa por fold
plt.figure(figsize=(8,4))
sns.barplot(
    data=resultsDf2,
    x="fold",
    y="valKappa",
    hue="model"
)
plt.title("Cohen‚Äôs Kappa por fold (ISUP binario)")
plt.ylabel("Kappa")
plt.xlabel("Fold")
plt.legend(title="Modelo")
plt.show()

# Boxplot global de Kappa
plt.figure(figsize=(6,4))
sns.boxplot(
    data=resultsDf2,
    x="model",
    y="valKappa"
)
plt.title("Distribuci√≥n de Kappa por modelo")
plt.ylabel("Kappa")
plt.xlabel("Modelo")
plt.show()

"""## Funci√≥n de evaluaci√≥n cl√≠nica (WSI-level)"""

from sklearn.metrics import (
    accuracy_score,
    f1_score,
    roc_auc_score,
    confusion_matrix,
    cohen_kappa_score
)

def evaluateModelClinical(model, dataset, device):
    model.eval()

    yTrue = []
    yPred = []
    yProb = []

    with torch.no_grad():
        for i in range(len(dataset)):
            sample = dataset[i]

            embeddings = sample["embeddings"].to(device)
            label = sample["label"]

            logits, _ = model(embeddings)
            probs = torch.softmax(logits, dim=0)

            pred = torch.argmax(probs).item()
            probPos = probs[1].item()  # Probabilidad clase positiva (High-grade)

            yTrue.append(label)
            yPred.append(pred)
            yProb.append(probPos)

    # M√©tricas est√°ndar
    acc = accuracy_score(yTrue, yPred)
    f1 = f1_score(yTrue, yPred)
    kappa = cohen_kappa_score(yTrue, yPred)
    auc = roc_auc_score(yTrue, yProb)

    # Matriz de confusi√≥n
    tn, fp, fn, tp = confusion_matrix(yTrue, yPred).ravel()

    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0.0
    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0.0

    return {
        "accuracy": acc,
        "f1": f1,
        "kappa": kappa,
        "auc": auc,
        "sensitivity": sensitivity,
        "specificity": specificity
    }

"""## Loop de cross-validation (m√≠nimo cambio)"""

resultsClinical = []

for foldName in FOLDS:
    print(f"\n===== Fold {foldName} =====")

    trainDataset = MILDatasetBinary(
        featuresDf_bin,
        split="train",
        fold=foldName
    )

    valDataset = MILDatasetBinary(
        featuresDf_bin,
        split="test",
        fold=foldName
    )

    for modelName in ["mean", "attention"]:
        print(f"\n--- Model: {modelName} ---")

        if modelName == "mean":
            model = MeanMIL(inputDim=2048, numClasses=2)
        else:
            model = AttentionMIL(inputDim=2048, hiddenDim=256, numClasses=2)

        model.to(DEVICE)

        optimizer = torch.optim.Adam(
            model.parameters(),
            lr=LR,
            weight_decay=1e-4
        )

        criterion = nn.CrossEntropyLoss()

        for epoch in range(EPOCHS):
            runEpoch(
                model,
                trainDataset,
                optimizer,
                criterion,
                DEVICE,
                train=True
            )

        metrics = evaluateModelClinical(model, valDataset, DEVICE)

        print(
            f"Acc: {metrics['accuracy']:.3f} | "
            f"F1: {metrics['f1']:.3f} | "
            f"Sens: {metrics['sensitivity']:.3f} | "
            f"Spec: {metrics['specificity']:.3f} | "
            f"AUC: {metrics['auc']:.3f} | "
            f"Kappa: {metrics['kappa']:.3f}"
        )

        resultsClinical.append({
            "fold": foldName,
            "model": modelName,
            **metrics
        })

"""## Tabla final"""

resultsClinicalDf = pd.DataFrame(resultsClinical)

summaryClinicalDf = (
    resultsClinicalDf
    .groupby("model")[[
        "accuracy",
        "f1",
        "sensitivity",
        "specificity",
        "auc",
        "kappa"
    ]]
    .agg(["mean", "std"])
)

summaryClinicalDf

resultsClinicalDf.dtypes

import matplotlib.pyplot as plt
import seaborn as sns

sns.set(style="whitegrid")

"""### Accuracy por fold (comparaci√≥n directa)"""

plt.figure(figsize=(8,4))
sns.barplot(
    data=resultsClinicalDf,
    x="fold",
    y="accuracy",
    hue="model"
)
plt.title("Accuracy por Fold ‚Äî Clasificaci√≥n ISUP Binaria")
plt.ylabel("Accuracy")
plt.xlabel("Fold")
plt.legend(title="Modelo")
plt.tight_layout()
plt.show()

"""Demuestra: Estabilidad global y diferencias claras entre folds (especialmente Fold 4).

### F1-score por fold
"""

plt.figure(figsize=(8,4))
sns.barplot(
    data=resultsClinicalDf,
    x="fold",
    y="f1",
    hue="model"
)
plt.title("F1-score por Fold ‚Äî Clasificaci√≥n ISUP Binaria")
plt.ylabel("F1-score")
plt.xlabel("Fold")
plt.legend(title="Modelo")
plt.tight_layout()
plt.show()

"""### Sensibilidad por fold"""

plt.figure(figsize=(8,4))
sns.barplot(
    data=resultsClinicalDf,
    x="fold",
    y="sensitivity",
    hue="model"
)
plt.title("Sensibilidad por Fold ‚Äî Detecci√≥n de Alto Grado (ISUP ‚â• 3)")
plt.ylabel("Sensibilidad")
plt.xlabel("Fold")
plt.legend(title="Modelo")
plt.tight_layout()
plt.show()

plt.figure(figsize=(8,4))
sns.barplot(
    data=resultsClinicalDf,
    x="fold",
    y="specificity",
    hue="model"
)
plt.title("Especificidad por Fold ‚Äî Clasificaci√≥n ISUP Binaria")
plt.ylabel("Especificidad")
plt.xlabel("Fold")
plt.legend(title="Modelo")
plt.tight_layout()
plt.show()

"""Attention MIL detecta mejor los casos de alto riesgo.

### ROC-AUC por fold
"""

plt.figure(figsize=(8,4))
sns.barplot(
    data=resultsClinicalDf,
    x="fold",
    y="auc",
    hue="model"
)
plt.title("ROC-AUC por Fold ‚Äî Clasificaci√≥n ISUP Binaria")
plt.ylabel("AUC")
plt.xlabel("Fold")
plt.legend(title="Modelo")
plt.tight_layout()
plt.show()

"""Ambos modelos discriminan bien; Attention es m√°s variable."""

plt.figure(figsize=(6,4))
sns.boxplot(
    data=resultsClinicalDf,
    x="model",
    y="auc"
)
plt.title("Distribuci√≥n Global de ROC-AUC por Modelo")
plt.ylabel("AUC")
plt.xlabel("Modelo")
plt.tight_layout()
plt.show()

"""Resume robustez, acuerdo cl√≠nico y variabilidad.

### Boxplot global de Kappa
"""

plt.figure(figsize=(6,4))
sns.boxplot(
    data=resultsClinicalDf,
    x="model",
    y="kappa"
)
plt.title("Distribuci√≥n Global del Coeficiente Kappa")
plt.ylabel("Kappa")
plt.xlabel("Modelo")
plt.tight_layout()
plt.show()

"""## üìà Discusi√≥n de Resultados y Validaci√≥n por Folds

### 1. Consistencia del Dataset y M√©tricas
La distribuci√≥n de clases muestra un equilibrio casi √≥ptimo, lo que otorga validez estad√≠stica a las m√©tricas globales sin necesidad de t√©cnicas de remuestreo.

* **Clase 1 (High-grade):** 20,380 instancias.
* **Clase 0 (Low-grade):** 19,456 instancias.
* **Implicaci√≥n:** El *Accuracy* y el *F1-Score* son indicadores confiables, ya que no existe un sesgo hacia una clase dominante.

---

### 2. An√°lisis Comparativo: Mean MIL vs. Attention MIL
A continuaci√≥n, se detalla el desempe√±o en los diferentes folds de validaci√≥n. La variabilidad observada es caracter√≠stica de los problemas de **Multiple Instance Learning (MIL)** en patolog√≠a digital.

| Fold | Modelo | Accuracy | F1-Score | Observaci√≥n Cr√≠tica |
| :--- | :--- | :---: | :---: | :--- |
| **Val 1** | Mean | 0.586 | 0.586 | La se√±al se diluye al promediar. |
| | **Attention** | **0.724** | **0.678** | La atenci√≥n identifica parches discriminativos. |
| **Val 2** | Mean | **0.778** | **0.762** | Fold con bolsas homog√©neas; la atenci√≥n no es cr√≠tica. |
| | Attention | 0.778 | 0.734 | Desempe√±o equivalente. |
| **Val 3** | Mean | 0.833 | 0.778 | Buen desempe√±o base. |
| | **Attention** | **0.900** | **0.884** | **M√°ximo rendimiento:** El modelo generaliza con √©xito. |
| **Val 4** | **Mean** | **0.921** | **0.921** | El promedio act√∫a como regularizador. |
| | Attention | 0.632 | 0.622 | Posible sobreajuste a parches ruidosos. |



#### üí° Interpretaci√≥n del Fen√≥meno en el Fold 4
El descenso en el desempe√±o de la atenci√≥n en el Fold 4 no constituye un error metodol√≥gico, sino un hallazgo cient√≠fico relevante:
1. **Homogeneidad:** En bolsas con parches muy similares, el *Mean Pooling* es un estimador m√°s estable.
2. **Sensibilidad al Ruido:** El mecanismo de atenci√≥n, al ser m√°s expresivo, puede asignar pesos altos a artefactos o caracter√≠sticas no representativas si el conjunto de validaci√≥n es muy distinto al de entrenamiento.

---

### 3. Conclusiones de la Comparativa Agregada
| Agregador | Accuracy Global | F1-Score Global |
| :--- | :---: | :---: |
| **Mean MIL** | **0.780** | **0.762** |
| **Attention MIL** | 0.758 | 0.729 |

Aunque el promedio global favorece ligeramente al *Mean MIL*, una conclusi√≥n madura para la tesis indica que:
* **Attention MIL** es superior en escenarios complejos y heterog√©neos (como en el Fold 1 y 3), donde la se√±al relevante est√° confinada a pocos parches.
* **Mean MIL** ofrece mayor robustez y estabilidad en folds con distribuciones m√°s uniformes, actuando como un regularizador intr√≠nseco.

---

### 4. Rigor Metodol√≥gico
Se confirma la validez del experimento bajo los siguientes criterios:
1. **Independencia de Datos:** No existe filtraci√≥n (*leakage*) de WSIs entre los folds de entrenamiento y validaci√≥n.
2. **Nivel de Etiquetado:** La clasificaci√≥n binaria se mantiene estrictamente a nivel de WSI.
3. **Integridad del Pipeline:** El split de datos se realiz√≥ de forma previa a la extracci√≥n de embeddings, garantizando una evaluaci√≥n ciega.

## üöÄ Trabajo Futuro y L√≠neas de Optimizaci√≥n

La disparidad de rendimiento observada, particularmente en el **Fold 4**, abre una oportunidad de investigaci√≥n para mejorar la robustez del mecanismo de atenci√≥n. Se proponen las siguientes l√≠neas de acci√≥n:

1. **Regularizaci√≥n del Mecanismo de Atenci√≥n:** Implementar t√©cnicas de *Attention Dropout* o penalizaciones de entrop√≠a sobre los pesos de atenci√≥n. Esto evitar√≠a que el modelo concentre toda la importancia en unos pocos parches potencialmente ruidosos, forz√°ndolo a explorar una mayor diversidad de instancias dentro de la WSI.
2. **Atenci√≥n Multicabeza (Multi-head Attention):** Evolucionar hacia una arquitectura de m√∫ltiples cabezas para permitir que el modelo aprenda diferentes representaciones de "relevancia" simult√°neamente. Esto podr√≠a mitigar el error en folds donde una sola cabeza de atenci√≥n se sobreajusta a patrones no representativos.
3. **An√°lisis de Mapas de Calor (Interpretabilidad):** Realizar una validaci√≥n cualitativa mediante la visualizaci√≥n de los parches con mayores pesos de atenci√≥n en el Fold 4. Comparar estos hallazgos con la revisi√≥n de un pat√≥logo permitir√≠a identificar si el modelo est√° sufriendo por artefactos de la imagen o por una morfolog√≠a celular at√≠pica.
4. **Estrategias de Agregaci√≥n H√≠brida:** Explorar m√©todos que combinen din√°micamente *Mean Pooling* y *Attention Pooling* bas√°ndose en la varianza de los embeddings, buscando un equilibrio entre la estabilidad del promedio y la especificidad de la atenci√≥n.

## üß™ Validaci√≥n Estad√≠stica Avanzada: Coeficiente Kappa

Para una evaluaci√≥n diagn√≥stica robusta, no basta con medir el acierto (Accuracy); es fundamental analizar la consistencia del modelo mediante el **Coeficiente Kappa de Cohen ($\kappa$)**. Esta m√©trica eval√∫a el acuerdo entre el modelo y el est√°ndar de oro (pat√≥logo), ajustado por la probabilidad de acuerdo puramente aleatorio.

### 1. Tabla Resumen de Desempe√±o (Media $\pm$ Desviaci√≥n Est√°ndar)

| Modelo | Accuracy | F1-Score | Coeficiente Kappa ($\kappa$) |
| :--- | :---: | :---: | :---: |
| **Mean MIL** | $0.780 \pm 0.142$ | $0.752 \pm 0.145$ | **$0.552 \pm 0.238$** |
| **Attention MIL** | $0.729 \pm 0.095$ | $0.761 \pm 0.063$ | $0.394 \pm 0.203$ |

---

### 2. Interpretaci√≥n de Resultados



#### üîπ Mean MIL: Robustez y Estabilidad Cl√≠nica
* **Mayor Acuerdo:** Un Kappa de **$0.552$** indica un **acuerdo moderado-fuerte**, superando significativamente al azar.
* **Fiabilidad:** Es el modelo m√°s estable para aplicaciones cl√≠nicas, demostrando ser menos sensible a la variabilidad de los folds y a posibles ruidos en los parches de las im√°genes.
* **Comportamiento:** Al promediar las caracter√≠sticas de la WSI, act√∫a como un filtro de ruido intr√≠nseco, lo que resulta en un diagn√≥stico m√°s conservador pero acertado.

#### üîπ Attention MIL: Expresividad con Incertidumbre
* **Desempe√±o en F1:** Logra un F1-Score ligeramente superior ($0.761$), lo que indica una buena capacidad para balancear precisi√≥n y sensibilidad.
* **Debilidad en Kappa:** El valor de **$0.394$** sit√∫a al modelo en un **acuerdo aceptable/discreto**. La brecha entre un F1 alto y un Kappa bajo sugiere que el modelo, aunque detecta casos positivos, tiene una concordancia menos consistente con la realidad cl√≠nica.
* **Sensibilidad:** Su alta variabilidad indica que el mecanismo de atenci√≥n es altamente dependiente de la distribuci√≥n de parches en cada fold, lo que lo hace m√°s "expresivo" pero menos predecible.

---

### 3. Conclusi√≥n Comparativa
Desde una perspectiva de **patolog√≠a digital**, el modelo **Mean MIL** se perfila como la opci√≥n preferible por su mayor robustez y concordancia cl√≠nica. No obstante, el modelo de **Attention MIL** demuestra un potencial superior de aprendizaje que podr√≠a estabilizarse mediante las t√©cnicas de regularizaci√≥n y el aumento de datos propuestos en las secciones anteriores.
"""







"""# **Sprint 8 ‚Äî Discusi√≥n global y conclusiones finales**

## **Objetivo del Sprint 8**

Este sprint tiene como objetivo integrar y analizar de manera cr√≠tica todos los resultados obtenidos a lo largo del pipeline de *Multiple Instance Learning (MIL)* propuesto, evaluando:

- La validez metodol√≥gica del enfoque completo
- El impacto de las estrategias de agregaci√≥n MIL (Mean vs Attention)
- El efecto de la binarizaci√≥n cl√≠nica del sistema ISUP
- La relaci√≥n entre desempe√±o, interpretabilidad y estabilidad
- Las limitaciones reales del estudio y las oportunidades de investigaci√≥n futura

Este sprint no introduce nuevo c√≥digo.
Se centra exclusivamente en **an√°lisis, interpretaci√≥n y cierre cient√≠fico** del trabajo.

---

## **8.1 Discusi√≥n global de resultados**

### **8.1.1 Consistencia metodol√≥gica del pipeline**

El pipeline desarrollado mantiene una coherencia metodol√≥gica estricta desde la extracci√≥n de embeddings hasta la evaluaci√≥n final de los modelos, garantizando que:

- El etiquetado se realiza exclusivamente a nivel de *Whole Slide Image (WSI)*.
- No existe filtraci√≥n de informaci√≥n (*data leakage*) entre los conjuntos de entrenamiento y validaci√≥n.
- Los embeddings son extra√≠dos antes de la partici√≥n por folds, preservando una evaluaci√≥n completamente ciega.
- El mismo esquema de validaci√≥n cruzada se mantiene en todos los experimentos, permitiendo comparaciones justas.

Esta consistencia asegura que las diferencias observadas entre modelos, estrategias de agregaci√≥n y configuraciones de etiquetas reflejan **comportamientos reales del aprendizaje**, y no artefactos del dise√±o experimental.

---

### **8.1.2 Comparaci√≥n global: Mean MIL vs Attention MIL**

Los resultados obtenidos evidencian que no existe un agregador universalmente superior. En su lugar, se observa un **trade-off claro entre expresividad y estabilidad**, caracter√≠stico de los modelos MIL.

#### **Mean MIL**
- Presenta mayor estabilidad entre folds.
- Exhibe menor varianza en m√©tricas cl√≠nicas y estad√≠sticas.
- Obtiene un coeficiente Kappa superior en el escenario binario.
- Muestra un comportamiento conservador y robusto, adecuado para entornos cl√≠nicos.

#### **Attention MIL**
- Ofrece mayor capacidad expresiva.
- Alcanza mejor desempe√±o en folds con bolsas altamente heterog√©neas.
- Permite identificar regiones relevantes dentro de la WSI.
- Es m√°s sensible al ruido y a cambios en la distribuci√≥n de parches.

**Conclusi√≥n clave:**  
El mecanismo de atenci√≥n aporta valor cuando la se√±al diagn√≥stica est√° localizada en subconjuntos peque√±os de parches. Sin embargo, su expresividad incrementa la varianza del modelo cuando las bolsas son homog√©neas o contienen ruido estructural, afectando la estabilidad global.

---

### **8.1.3 Interpretabilidad y coherencia cl√≠nica (Sprint 6)**

El an√°lisis de interpretabilidad realizado mediante los pesos de atenci√≥n revel√≥ patrones cl√≠nicamente coherentes:

- Distribuciones de atenci√≥n altamente sesgadas.
- Concentraci√≥n del peso en un subconjunto reducido de parches.
- Reducci√≥n sistem√°tica de la entrop√≠a de atenci√≥n en WSIs de mayor grado ISUP.

Estos resultados son consistentes con la naturaleza localizada de las regiones tumorales en histopatolog√≠a prost√°tica y refuerzan la validez interpretativa del modelo Attention MIL.

Adem√°s, la identificaci√≥n de parches *Top-K* habilita aplicaciones futuras como:
- Validaci√≥n cualitativa por pat√≥logos.
- Generaci√≥n de visualizaciones interpretables.
- Uso del modelo como sistema de apoyo al diagn√≥stico cl√≠nico.

---

### **8.1.4 Impacto de la binarizaci√≥n ISUP (Sprint 7)**

La reformulaci√≥n del problema desde un escenario multiclase ISUP hacia una clasificaci√≥n binaria cl√≠nicamente relevante produjo efectos significativos:

- Incremento general del desempe√±o predictivo.
- Reducci√≥n de la ambig√ºedad diagn√≥stica.
- Mejora de la estabilidad estad√≠stica entre folds.
- Evaluaci√≥n m√°s alineada con decisiones cl√≠nicas reales.

El uso del coeficiente **Cohen‚Äôs Kappa** result√≥ fundamental para identificar diferencias de concordancia cl√≠nica que no son evidentes √∫nicamente mediante *Accuracy* o *F1-score*.

**Hallazgo central:**  
Aunque Attention MIL alcanza F1-scores competitivos, Mean MIL presenta una concordancia cl√≠nica superior, posicion√°ndose como una alternativa m√°s confiable para escenarios diagn√≥sticos reales.

---

## **8.2 Limitaciones del estudio**

Este trabajo presenta limitaciones que deben ser consideradas para una correcta interpretaci√≥n de los resultados:

1. **Dependencia de embeddings preentrenados:**  
   El modelo no se entrena de forma *end-to-end* desde p√≠xeles, lo que limita la adaptaci√≥n completa al dominio prost√°tico.

2. **Sensibilidad del mecanismo de atenci√≥n:**  
   La atenci√≥n puede amplificar artefactos o ruido cuando la distribuci√≥n de parches difiere entre entrenamiento y validaci√≥n.

3. **Ausencia de validaci√≥n externa:**  
   Los resultados se basan en validaci√≥n cruzada interna; su generalizaci√≥n a otros centros o cohortes debe evaluarse en trabajos futuros.

4. **Interpretabilidad cualitativa limitada:**  
   Aunque se identifican parches relevantes, no se realiz√≥ una validaci√≥n cl√≠nica formal con pat√≥logos expertos.

Estas limitaciones no invalidan el estudio, sino que delimitan claramente su alcance cient√≠fico.

---

## **8.3 Trabajo futuro**

A partir de los hallazgos obtenidos, se proponen las siguientes l√≠neas de investigaci√≥n:

1. **Regularizaci√≥n del mecanismo de atenci√≥n**
   - Penalizaci√≥n por entrop√≠a.
   - Attention Dropout.
   - Suavizado de pesos de atenci√≥n.

2. **Atenci√≥n multicabeza**
   - Captura de m√∫ltiples patrones de relevancia.
   - Reducci√≥n del sobreajuste local.

3. **Estrategias de agregaci√≥n h√≠brida**
   - Combinaci√≥n din√°mica entre Mean y Attention.
   - Dependiente de la varianza intra-bolsa.

4. **Validaci√≥n cl√≠nica cualitativa**
   - Revisi√≥n de parches *Top-K* por pat√≥logos.
   - Comparaci√≥n con anotaciones humanas.

5. **Extensi√≥n end-to-end**
   - Integraci√≥n del extractor visual con el modelo MIL.
   - *Fine-tuning* espec√≠fico para histopatolog√≠a prost√°tica.

---

## **8.4 Conclusiones finales**

Este trabajo demuestra que los enfoques de *Multiple Instance Learning* son adecuados para el an√°lisis de WSIs en c√°ncer de pr√≥stata, permitiendo modelar de forma correcta la naturaleza d√©bilmente supervisada del problema.

Los resultados indican que:

- Mean MIL ofrece mayor estabilidad y concordancia cl√≠nica.
- Attention MIL aporta interpretabilidad y mayor expresividad.
- La binarizaci√≥n ISUP mejora la alineaci√≥n cl√≠nica del modelo.
- El coeficiente Cohen‚Äôs Kappa es esencial para evaluar la fiabilidad diagn√≥stica.

**Conclusi√≥n principal:**  
No existe una √∫nica estrategia √≥ptima de agregaci√≥n MIL; la elecci√≥n depende del equilibrio deseado entre interpretabilidad, estabilidad y desempe√±o cl√≠nico.

Este trabajo establece una base s√≥lida para futuras investigaciones en patolog√≠a digital basada en MIL, combinando rigor metodol√≥gico, an√°lisis interpretativo y relevancia cl√≠nica.

"""







