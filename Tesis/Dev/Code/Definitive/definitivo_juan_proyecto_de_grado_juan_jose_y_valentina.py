# -*- coding: utf-8 -*-
"""Definitivo_Juan_Proyecto_de_Grado_Juan_Jose_y_Valentina.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MVecYuB9uj3JcFwHH7GrIHgBlptLfABM

# ***Weakly Supervised Multiple Instance Learning for Prostate Cancer Histopathology (SICAPv2)***

# **Sprint 0: Descarga / importaciÃ³n del dataset**

Este bloque deja evidencia de origen de datos, trazabilidad y organizaciÃ³n reproducible.

## 0.1 DefiniciÃ³n de rutas base
"""

import os
import shutil

BASE_PATH = "/content"
DATA_ROOT = os.path.join(BASE_PATH, "sicapv2_data")
DATASET_PATH = os.path.join(DATA_ROOT, "SICAPv2")
META_PATH = os.path.join(DATASET_PATH, "metadata")

os.makedirs(META_PATH, exist_ok=True)

"""## 0.2 ImportaciÃ³n del dataset"""

ZIP_MAIN = "/content/sicapv2.zip"

if not os.path.exists(ZIP_MAIN):
    print("Archivo ZIP principal NO existe. Descargando...")

    # Enlace de descarga para el archivo ZIP
    download_url = "https://data.mendeley.com/public-api/zip/9xxm58dvs3/download/1"

    # Descargar el archivo ZIP y guardarlo como sicapv2.zip
    !wget -O $ZIP_MAIN $download_url
else:
    print("Archivo ZIP principal ya existe â†’ NO se descarga.")

"""### Descomprimimos el Archivo ZIP principal"""

INTERMEDIATE_DIR = "/content/intermediate_data"
!unzip -q $ZIP_MAIN -d $INTERMEDIATE_DIR

# Listar los contenidos para verificar
!ls /content/intermediate_data

"""## 0.3 Descomprimir el Archivo ZIP Anidado (SICAPv2.zip)

####"/content/intermediate_data/SICAPv2.../SICAPv2.zip"
"""

nested_zip = None
for root, _, files in os.walk(INTERMEDIATE_DIR):
    for f in files:
        if f.endswith(".zip"):
            nested_zip = os.path.join(root, f)
            break

assert nested_zip is not None, "ZIP SICAPv2 no encontrado"

!unzip -q "$nested_zip" -d $DATA_ROOT

assert os.path.exists(DATASET_PATH)
assert len(os.listdir(DATASET_PATH)) > 0

"""## 0.4. Limpiar y Organizar el Espacio"""

if os.path.exists(ZIP_MAIN):
    print("Eliminando ZIP principal...")

    # Eliminar el archivo ZIP principal
    !rm $ZIP_MAIN

if os.path.exists(INTERMEDIATE_DIR):
    print("Eliminando carpeta intermedia...")

    # Eliminar el directorio intermedio y su contenido de forma recursiva
    !rm -r $INTERMEDIATE_DIR

print("\nContenido de /content:")
!ls /content



"""# **Sprint 1: Entendimiento y organizaciÃ³n del dataset SICAPv2**

PropÃ³sito del sprint

1. Verificar la estructura real del dataset descargado
2. Identificar quÃ© informaciÃ³n tenemos (imÃ¡genes, mÃ¡scaras, labels WSI)
3. Preparar una organizaciÃ³n coherente para Aprendizaje con MÃºltiples Instancias (MIL)
4. Dejar trazabilidad clara para el documento de tesis

## 1.1 VerificaciÃ³n de la estructura del dataset

Primero, inspeccionamos quÃ© hay realmente en disco. Esto es clave porque SICAPv2 no es homogÃ©neo entre Kaggle y Mendeley.
"""

def printTree(path, level=2):
    for root, dirs, files in os.walk(path):
        depth = root.replace(path, "").count(os.sep)
        if depth > level:
            continue
        indent = " " * 4 * depth
        print(f"{indent}{os.path.basename(root)}/")
        for f in files[:5]:
            print(f"{indent}    {f}")

printTree(DATASET_PATH, level=3)

"""## 1.2 DefiniciÃ³n de Paths"""

IMAGES_DIR = os.path.join(DATASET_PATH, "images")
MASKS_DIR  = os.path.join(DATASET_PATH, "masks")
PARTITION_DIR = os.path.join(DATASET_PATH, "partition")

for p in [IMAGES_DIR, MASKS_DIR, PARTITION_DIR]:
    print(p, "â†’", os.path.exists(p))

"""## 1.3 Conteo y verificaciÃ³n bÃ¡sica de datos"""

def countFiles(path, ext=".jpg"):
    return len([f for f in os.listdir(path) if f.lower().endswith(ext)])

print("Conteo global de patches")
stats = {
    "total_images": countFiles(IMAGES_DIR),
    "total_masks": countFiles(MASKS_DIR)
}

stats

"""## 1.4 Consistencia imagenâ€“mÃ¡scara"""

imagesSet = set(os.listdir(IMAGES_DIR))
masksSet  = set(os.listdir(MASKS_DIR))

print("ImÃ¡genes sin mÃ¡scara:", len(imagesSet - masksSet))
print("MÃ¡scaras sin imagen:", len(masksSet - imagesSet))

"""## 1.5 Particiones oficiales"""

printTree(PARTITION_DIR, level=3)

"""## 1.6 Verificamos quÃ© contienen los Train.xlsx / Test.xlsx"""

import pandas as pd

samplePartition = pd.read_excel(
    os.path.join(PARTITION_DIR, "Validation", "Val1", "Train.xlsx")
)

samplePartition.head()

"""## 1.7 Verificamos si hay solapamiento entre folds (anti data leakage)"""

trainVal1 = set(
    pd.read_excel(os.path.join(PARTITION_DIR, "Validation", "Val1", "Train.xlsx"))["image_name"].apply(lambda x: x.split('_Block')[0])
)

testVal1 = set(
    pd.read_excel(os.path.join(PARTITION_DIR, "Validation", "Val1", "Test.xlsx"))["image_name"].apply(lambda x: x.split('_Block')[0])
)

print("Solapamiento Train/Test:", len(trainVal1 & testVal1))

"""## 1.8 Conteo de WSIs por particiÃ³n"""

def countWSIs(path):
    df = pd.read_excel(path)
    wsiIds = df["image_name"].apply(lambda x: x.split("_Block")[0])
    return wsiIds.nunique()

for fold in ["Val1", "Val2", "Val3", "Val4"]:
    trainPath = os.path.join(PARTITION_DIR, "Validation", fold, "Train.xlsx")
    testPath  = os.path.join(PARTITION_DIR, "Validation", fold, "Test.xlsx")

    print(
        fold,
        "Train WSIs:", countWSIs(trainPath),
        "\nTest WSIs:", countWSIs(testPath)
    )

import matplotlib.pyplot as plt

folds = ["Val1", "Val2", "Val3", "Val4"]
trainWsis = [95, 97, 94, 86]
testWsis  = [29, 27, 30, 38]

x = range(len(folds))

plt.bar(x, trainWsis, label="Train")
plt.bar(x, testWsis, bottom=trainWsis, label="Test")
plt.xticks(x, folds)
plt.ylabel("NÃºmero de WSIs")
plt.title("DistribuciÃ³n de WSIs por fold (SICAPv2)")
plt.legend()
plt.show()

"""### **Nota**:
Las mÃ¡scaras de segmentaciÃ³n estÃ¡n disponibles en SICAPv2, pero NO se utilizan para el entrenamiento del modelo MIL.

Se emplean Ãºnicamente para anÃ¡lisis exploratorio y validaciÃ³n interpretativa.

El dataset SICAPv2 proporciona un conjunto de particiones oficiales para validaciÃ³n cruzada (Val1â€“Val4), las cuales fueron empleadas para el desarrollo y evaluaciÃ³n de los modelos. Adicionalmente, el dataset incluye un conjunto de prueba independiente (partition/Test), el cual se reserva como test hold-out final y no se utiliza durante el proceso de ajuste ni selecciÃ³n de modelos

# **Sprint 2: ConstrucciÃ³n del dataset_manifest.csv**

### **Objetivo del Sprint 2**

Construir un manifest Ãºnico, consistente y trazable, donde:

- 1 fila = 1 parche
- Cada parche queda asociado a:
  * su WSI
  * su fold (Val1â€“Val4)
  * su split (train / test)
  * su ruta a imagen
  * su ruta a mÃ¡scara
  * su label clÃ­nico global (WSI-level)

Este archivo serÃ¡:
- la base de los bags MIL
- la fuente para EDA, entrenamiento y evaluaciÃ³n

## Esquema propuesto del `dataset_manifest.csv`

| Columna | DescripciÃ³n |
| :--- | :--- |
| **imageName** | Nombre del parche |
| **imagePath** | Ruta absoluta o relativa a la imagen |
| **maskPath** | Ruta a la mÃ¡scara correspondiente |
| **maskExists** | Indica si la mÃ¡scara existe (True/False) |
| **wsiId** | Identificador Ãºnico de la WSI |
| **fold** | Fold de validaciÃ³n cruzada (Val1â€“Val4) |
| **split** | Subconjunto: train / test |
| **gleasonPrimary** | Gleason Score primario a nivel WSI |
| **gleasonSecondary** | Gleason Score secundario a nivel WSI |
| **isup** | Grado ISUP derivado a partir de Gleason Primary + Secondary |
| **nc** | Parche no canceroso (Non-Cancerous) |
| **g3** | Gleason pattern 3 (patch-level) |
| **g4** | Gleason pattern 4 (patch-level) |
| **g5** | Gleason pattern 5 (patch-level) |
| **g4c** | Gleason pattern 4 cribriforme (subtipo agresivo) |


El grado ISUP se deriva una Ãºnica vez a nivel WSI durante la carga de los labels clÃ­nicos, y posteriormente se propaga a todos los parches asociados a dicha WSI, garantizando coherencia WSI-level y evitando inconsistencias a nivel patch.

### Nota
Los labels NO son patch-level, solo se registran por trazabilidad.

## 2.1 Cargar labels globales (WSI-level)
"""

import pandas as pd
import os

wsiLabelsPath = os.path.join(DATASET_PATH, "wsi_labels.xlsx")
wsiLabelsDf = pd.read_excel(wsiLabelsPath)
wsiLabelsDf.head()

wsiLabelsDf.columns

wsiLabelsDf = wsiLabelsDf.rename(columns={
    "slide_id": "WSI_ID"
})

wsiLabelsDf.head()

wsiLabelsDf.columns

"""## 2.2 Funciones auxiliares clave"""

def extractWsiId(imageName: str) -> str:
    """
    Extrae el identificador de la WSI a partir del nombre del parche.
    """
    return imageName.split("_Block")[0]

def buildImagePath(imageName):
    return os.path.join(IMAGES_DIR, imageName)

def buildMaskPath(imageName):
    return os.path.join(MASKS_DIR, imageName)

def computeISUP(gleasonPrimary, gleasonSecondary):
    total = gleasonPrimary + gleasonSecondary

    if total <= 6:
        return 1
    elif total == 7 and gleasonPrimary == 3:
        return 2
    elif total == 7 and gleasonPrimary == 4:
        return 3
    elif total == 8:
        return 4
    else:
        return 5

"""## 2.3 Procesar un fold (funciÃ³n central)

### precalcular ISUP
"""

wsiLabelsDf["ISUP"] = wsiLabelsDf.apply(
    lambda r: computeISUP(r["Gleason_primary"], r["Gleason_secondary"]),
    axis=1
)

"""### Construir el diccionario clÃ­nico (YA con ISUP)"""

wsiLabelDict = (
    wsiLabelsDf
    .set_index("WSI_ID")
    .to_dict(orient="index")
)

def processFold(foldName):
    foldRows = []
    foldPath = os.path.join(PARTITION_DIR, "Validation", foldName)

    for split in ["Train", "Test"]:
        splitDf = pd.read_excel(os.path.join(foldPath, f"{split}.xlsx"))

        for _, row in splitDf.iterrows():
            imageName = row["image_name"]
            wsiId = extractWsiId(imageName)

            foldRows.append({
                "imageName": imageName,
                "imagePath": buildImagePath(imageName),
                "maskPath": buildMaskPath(imageName),
                "maskExists": os.path.exists(buildMaskPath(imageName)),
                "wsiId": wsiId,
                "fold": foldName,
                "split": split.lower(),

                # ðŸ”’ Labels WSI-level (constantes)
                "gleasonPrimary": int(wsiLabelDict[wsiId]["Gleason_primary"]),
                "gleasonSecondary": int(wsiLabelDict[wsiId]["Gleason_secondary"]),
                "isup": int(wsiLabelDict[wsiId]["ISUP"]),

                # ðŸŽ¯ Patch-level (desde partition)
                "nc": row["NC"],
                "g3": row["G3"],
                "g4": row["G4"],
                "g5": row["G5"],
                "g4c": row["G4C"],
            })

    return pd.DataFrame(foldRows)

"""## 2.4 ConstrucciÃ³n completa del manifest"""

allFolds = []

for fold in ["Val1", "Val2", "Val3", "Val4"]:
    print(f"Procesando {fold}...")
    allFolds.append(processFold(fold))

manifestDf = pd.concat(allFolds, ignore_index=True)
manifestDf.head()

"""## 2.5 Validaciones crÃ­ticas"""

manifestDf["isup"].apply(type).value_counts()

manifestDf.groupby("wsiId")["isup"].nunique().max()

manifestDf.shape

"""### Integridad de mÃ¡scaras"""

manifestDf["maskExists"].value_counts()

"""### Sin valores nulos"""

manifestDf.isna().sum()

"""### Coherencia WSIâ€“fold"""

manifestDf.groupby(["fold", "split"])["wsiId"].nunique()

"""## 2.6 Guardar el manifest"""

manifestPath = os.path.join(META_PATH, "dataset_manifest.csv")
manifestDf.to_csv(manifestPath, index=False)

print("Manifest guardado en:", manifestPath)

"""Todas las validaciones crÃ­ticas del manifest fueron superadas, incluyendo coherencia WSI-level de los labels clÃ­nicos, integridad de mÃ¡scaras, ausencia de valores nulos y correcta separaciÃ³n por fold y split. El archivo dataset_manifest.csv se considera estable y se utiliza como Ãºnica fuente de verdad para los sprints posteriores.

# **Sprint 3 PreparaciÃ³n MIL (Bag Construction)**

## **Objetivo del Sprint 3**

Transformar el dataset_manifest.csv en una estructura de Bags MIL, donde:

- Bag = WSI
- Instancias = patches
- Label del bag = ISUP
- SeparaciÃ³n estricta por fold y split

Este sprint no entrena modelos. Solo prepara datos correctamente.
"""

import pandas as pd

MANIFEST_PATH = "/content/sicapv2_data/SICAPv2/metadata/dataset_manifest.csv"

manifestDf = pd.read_csv(MANIFEST_PATH)

manifestDf.shape
manifestDf.head()

"""## 3.2 DefiniciÃ³n formal de un Bag MIL

A nivel conceptual:

- Un bag corresponde a una WSI y contiene mÃºltiples instancias (***patches***).
- El bag hereda el label clÃ­nico de la WSI (ISUP), mientras que las instancias pueden tener etiquetas locales (***NC, G3, G4, G5, G4C***).

## 3.3 ConstrucciÃ³n de Bags MIL
"""

from collections import defaultdict

def buildMilBags(manifestDf):
    bags = []

    grouped = manifestDf.groupby(["fold", "split", "wsiId"])

    for (fold, split, wsiId), group in grouped:

        bag = {
            "wsiId": wsiId,
            "fold": fold,
            "split": split,
            "isup": int(group["isup"].iloc[0]),
            "gleasonPrimary": int(group["gleasonPrimary"].iloc[0]),
            "gleasonSecondary": int(group["gleasonSecondary"].iloc[0]),
            "instances": []
        }

        for _, row in group.iterrows():
            instance = {
                "imagePath": row["imagePath"],
                "maskPath": row["maskPath"],
                "patchLabel": {
                    "nc": row["nc"],
                    "g3": row["g3"],
                    "g4": row["g4"],
                    "g5": row["g5"],
                    "g4c": row["g4c"]
                }
            }
            bag["instances"].append(instance)

        bags.append(bag)

    return bags

milBags = buildMilBags(manifestDf)

len(milBags)

"""El nÃºmero de bags debe coincidir con:"""

manifestDf[["fold", "split", "wsiId"]].drop_duplicates().shape[0]

"""## 3.5 Validaciones crÃ­ticas"""

assert all(len(bag["instances"]) > 0 for bag in milBags)

"""## 3.5.2 Un bag no mezcla splits"""

for bag in milBags:
    assert bag["split"] in ["train", "test"]

"""## 3.5.3 Un bag tiene un solo label ISUP"""

for bag in milBags:
    assert isinstance(bag["isup"], int)

"""## 3.6 EstadÃ­sticas MIL"""

import numpy as np

bagSizes = [len(bag["instances"]) for bag in milBags]

print("NÃºmero de bags:", len(milBags))
print("Instancias por bag (min):", np.min(bagSizes))
print("Instancias por bag (max):", np.max(bagSizes))
print("Instancias por bag (mean):", np.mean(bagSizes))

"""## 3.7 SeparaciÃ³n final por fold y split"""

def splitBagsByFold(milBags, foldName):
    trainBags = [b for b in milBags if b["fold"] == foldName and b["split"] == "train"]
    testBags  = [b for b in milBags if b["fold"] == foldName and b["split"] == "test"]
    return trainBags, testBags

trainVal1, testVal1 = splitBagsByFold(milBags, "Val1")

len(trainVal1), len(testVal1)

bagSizes = [len(bag["instances"]) for bag in milBags]

plt.figure()
plt.hist(bagSizes, bins=30)
plt.xlabel("NÃºmero de instancias por bag")
plt.ylabel("Frecuencia")
plt.title("DistribuciÃ³n de instancias por WSI (Bags MIL)")
plt.show()

bagsDf = pd.DataFrame([
    {"fold": b["fold"], "split": b["split"]}
    for b in milBags
])

bagsDf.groupby(["fold", "split"]).size().unstack().plot(kind="bar")
plt.ylabel("NÃºmero de bags")
plt.title("DistribuciÃ³n de bags por fold y split")
plt.show()

"""## **ConstrucciÃ³n y validaciÃ³n de Bags MIL**

A partir del dataset_manifest.csv, se construyÃ³ una representaciÃ³n basada en Aprendizaje de Instancias MÃºltiples (MIL), donde cada Whole Slide Image (WSI) corresponde a un bag y cada parche histopatolÃ³gico a una instancia. El label clÃ­nico del bag se definiÃ³ a partir del grado ISUP asociado a la WSI, mientras que las instancias conservan informaciÃ³n local relacionada con la distribuciÃ³n de patrones histolÃ³gicos (NC, G3, G4, G5 y G4C).

Se validÃ³ que:

- Cada bag contiene exclusivamente instancias pertenecientes a una Ãºnica WSI.
- No existe solapamiento entre conjuntos de entrenamiento y prueba.
- Todos los bags presentan un Ãºnico label ISUP.
- No existen instancias huÃ©rfanas ni bags vacÃ­os.

El conjunto final contiene $496$ bags, con un nÃºmero variable de instancias por bag, lo cual refleja la heterogeneidad espacial propia de las imÃ¡genes histopatolÃ³gicas y justifica el uso de modelos MIL con mecanismos de agregaciÃ³n basados en atenciÃ³n.

# **Sprint 4: Modelado MIL Binario, Benchmarking y EvaluaciÃ³n ClÃ­nica**

## **Objetivo**

Entrenar y comparar modelos de Multiple Instance Learning (MIL) binarios
(Mean Pooling, Max Pooling, ABMIL y SmABMIL) a nivel de Whole Slide Image (WSI),
utilizando embeddings de parches extraÃ­dos mediante una CNN preentrenada.

El sprint incluye benchmarking cuantitativo, evaluaciÃ³n clÃ­nica
y anÃ¡lisis de interpretabilidad basado en mecanismos de atenciÃ³n
a nivel de instancia.


## **1. Decisiones metodolÃ³gicas (justificaciÃ³n tÃ©cnica)**

### 1.1 Backbone CNN

- Arquitectura: ResNet50 preentrenada en ImageNet
- Pesos: preentrenada en ImageNet

JustificaciÃ³n (tesis-ready):
  - Arquitectura profunda con bloques residuales que facilitan el entrenamiento estable
  - Amplio uso en tareas de histopatologÃ­a digital
  - Buen equilibrio entre capacidad representacional y costo computacional
  - Frecuentemente utilizada como baseline en enfoques MIL

## **2. Estrategia de entrenamiento**

**Backbone congelado (feature extractor)**

La red convolucional se utiliza Ãºnicamente como extractor de caracterÃ­sticas:
  - No se realiza fine-tuning en esta etapa
  - Todos los parÃ¡metros del backbone permanecen congelados

Razones:
  - Dataset relativamente pequeÃ±o en nÃºmero de WSIs
  - Riesgo alto de sobreajuste
  - PrÃ¡ctica estÃ¡ndar en pipelines MIL para histopatologÃ­a

## **A. Dataset y trazabilidad**

### 4.A.1 Cargar bags MIL desde Sprint 3
"""

MANIFEST_PATH = "/content/sicapv2_data/SICAPv2/metadata/dataset_manifest.csv"
manifestDf = pd.read_csv(MANIFEST_PATH)

print("Total patches:", len(manifestDf))
print("Total WSIs:", manifestDf["wsiId"].nunique())

"""### 4.A.2 EstadÃ­sticas clave del dataset"""

# ISUP original
isupDist = manifestDf.drop_duplicates("wsiId")["isup"].value_counts().sort_index()

# BinarizaciÃ³n (decisiÃ³n explÃ­cita)
# ISUP >= 2 â†’ cÃ¡ncer clÃ­nicamente significativo
wsiDf = manifestDf.drop_duplicates("wsiId").copy()
wsiDf["labelBinary"] = (wsiDf["isup"] >= 2).astype(int)

binaryDist = wsiDf["labelBinary"].value_counts()

isupDist, binaryDist

"""### 4.A.3 TamaÃ±o de bags (parches por WSI)"""

bagSizes = manifestDf.groupby("wsiId").size()

print("Bags (WSIs):", bagSizes.shape[0])
print("Patches por bag:")
print("  min :", bagSizes.min())
print("  max :", bagSizes.max())
print("  mean:", round(bagSizes.mean(), 2))

plt.hist(bagSizes, bins=30)
plt.xlabel("Patches por WSI")
plt.ylabel("Frecuencia")
plt.title("DistribuciÃ³n de tamaÃ±o de bags (SICAPv2)")
plt.show()

"""## **B. Feature Extraction (ResNet50 congelada)**

### 4.B.1 Dataset de parches (PyTorch)
"""

import torch
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms, models
from PIL import Image
import os

class PatchDataset(Dataset):
    def __init__(self, df, transform=None):
        self.df = df.reset_index(drop=True)
        self.transform = transform

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        img = Image.open(row["imagePath"]).convert("RGB")

        if self.transform:
            img = self.transform(img)

        return img, row["wsiId"]

"""### 4.B.2 Transforms y backbone"""

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(
        mean=[0.485, 0.456, 0.406],
        std =[0.229, 0.224, 0.225]
    )
])

backbone = models.resnet50(pretrained=True)
backbone.fc = torch.nn.Identity()
backbone = backbone.to(device)
backbone.eval()

"""### 4.B.3 Extraer embeddings por fold"""

def extractEmbeddings(df, batchSize=64):
    dataset = PatchDataset(df, transform)
    loader = DataLoader(dataset, batch_size=batchSize, shuffle=False)

    features = {}
    with torch.no_grad():
        for imgs, wsiIds in loader:
            imgs = imgs.to(device)
            emb = backbone(imgs).cpu().numpy()

            for e, wsi in zip(emb, wsiIds):
                features.setdefault(wsi, []).append(e)

    return {k: np.stack(v) for k, v in features.items()}

"""## **C. Modelos MIL y Benchmarking**

### 4.C.1 DefiniciÃ³n de modelos MIL
"""

import torch.nn as nn
import torch.nn.functional as F

"""#### Mean Pooling MIL"""

class MeanPoolingMIL(nn.Module):
    def __init__(self, dim):
        super().__init__()
        self.fc = nn.Linear(dim, 1)

    def forward(self, x):
        return self.fc(x.mean(dim=0))

"""#### Max Pooling MIL"""

class MaxPoolingMIL(nn.Module):
    def __init__(self, inputDim):
        super().__init__()
        self.classifier = nn.Linear(inputDim, 1)

    def forward(self, x):
        x, _ = x.max(dim=0)
        return self.classifier(x)

"""#### ABMIL (Attention-Based MIL)"""

class ABMIL(nn.Module):
    def __init__(self, dim, hidden=256):
        super().__init__()
        self.att = nn.Sequential(
            nn.Linear(dim, hidden),
            nn.Tanh(),
            nn.Linear(hidden, 1)
        )
        self.fc = nn.Linear(dim, 1)

    def forward(self, x):
        A = torch.softmax(self.att(x), dim=0)
        M = torch.sum(A * x, dim=0)
        return self.fc(M), A

"""#### SmABMIL (Gated Attention)"""

class SmABMIL(nn.Module):
    def __init__(self, dim, hidden=256):
        super().__init__()
        self.V = nn.Linear(dim, hidden)
        self.U = nn.Linear(dim, hidden)
        self.W = nn.Linear(hidden, 1)
        self.fc = nn.Linear(dim, 1)

    def forward(self, x):
        A = torch.tanh(self.V(x)) * torch.sigmoid(self.U(x))
        A = torch.softmax(self.W(A), dim=0)
        M = torch.sum(A * x, dim=0)
        return self.fc(M), A

"""## **D. Entrenamiento + EvaluaciÃ³n ClÃ­nica**

### 4.D.1 MÃ©tricas clÃ­nicas
"""

from sklearn.metrics import (
    accuracy_score, recall_score, precision_score,
    confusion_matrix, roc_auc_score, f1_score
)

def evaluate(model, bags, labels):
    yTrue, yPred, yProb = [], [], []

    model.eval()
    with torch.no_grad():
        for bag, y in zip(bags, labels):
            bag = bag.to(device)  # âœ… FIX CRÃTICO

            out = model(bag)
            if isinstance(out, tuple):
                out = out[0]

            p = torch.sigmoid(out).item()

            yTrue.append(y)
            yPred.append(int(p >= 0.5))
            yProb.append(p)

    return {
        "accuracy": accuracy_score(yTrue, yPred),
        "sensitivity": recall_score(yTrue, yPred),
        "specificity": recall_score(yTrue, yPred, pos_label=0),
        "precision": precision_score(yTrue, yPred),
        "f1": f1_score(yTrue, yPred),
        "auc": roc_auc_score(yTrue, yProb),
        "confusion": confusion_matrix(yTrue, yPred)
    }

"""### 4.D.2 Loop completo por fold (benchmarking real)"""

from tqdm.auto import tqdm

EPOCHS = 10
LR = 1e-4
LOSS = nn.BCEWithLogitsLoss()

results = []
trainingLogs = []
trainedModels = {}

for fold in ["Val1", "Val2", "Val3", "Val4"]:
    print(f"\n==============================")
    print(f"        FOLD {fold}")
    print(f"==============================")

    # Split por fold
    dfTrain = manifestDf[(manifestDf.fold == fold) & (manifestDf.split == "train")]
    dfTest  = manifestDf[(manifestDf.fold == fold) & (manifestDf.split == "test")]

    # Embeddings
    embTrain = extractEmbeddings(dfTrain)
    embTest  = extractEmbeddings(dfTest)

    # ConstrucciÃ³n de bags
    trainBags, trainLabels = [], []
    for wsi, emb in embTrain.items():
        y = int(wsiDf.loc[wsiDf.wsiId == wsi, "labelBinary"].iloc[0])
        trainBags.append(torch.tensor(emb, dtype=torch.float32))
        trainLabels.append(y)

    testBags, testLabels = [], []
    for wsi, emb in embTest.items():
        y = int(wsiDf.loc[wsiDf.wsiId == wsi, "labelBinary"].iloc[0])
        testBags.append(torch.tensor(emb, dtype=torch.float32))
        testLabels.append(y)

    # Modelos a comparar
    for Model, name in [
        (MeanPoolingMIL, "MeanMIL"),
        (MaxPoolingMIL, "MaxMIL"),
        (ABMIL, "ABMIL"),
        (SmABMIL, "SmABMIL")
    ]:
        print(f"\nâ–¶ Entrenando modelo: {name}")

        model = Model(trainBags[0].shape[1]).to(device)
        optimizer = torch.optim.Adam(model.parameters(), lr=LR)

        # ===== Entrenamiento =====
        for epoch in range(EPOCHS):
            model.train()
            epochLoss = 0.0

            loop = tqdm(
                zip(trainBags, trainLabels),
                total=len(trainBags),
                leave=False,
                desc=f"{name} | Epoch {epoch+1}/{EPOCHS}"
            )

            for bag, y in loop:
                bag = bag.to(device)
                target = torch.tensor([float(y)], device=device)

                optimizer.zero_grad()
                out = model(bag)
                if isinstance(out, tuple):
                    out = out[0]

                loss = LOSS(out, target)
                loss.backward()
                optimizer.step()

                epochLoss += loss.item()
                loop.set_postfix(loss=loss.item())

            avgLoss = epochLoss / len(trainBags)

            trainingLogs.append({
                "fold": fold,
                "model": name,
                "epoch": epoch + 1,
                "loss": avgLoss
            })

            print(f"    Epoch [{epoch+1}/{EPOCHS}] - Avg Loss: {avgLoss:.4f}")

        # GUARDAMOS EL MODELO
        trainedModels[(name, fold)] = model

        # ===== EvaluaciÃ³n clÃ­nica =====
        metrics = evaluate(model, testBags, testLabels)
        metrics["model"] = name
        metrics["fold"] = fold
        results.append(metrics)

        print(
            f"âœ” EvaluaciÃ³n {name} | Fold {fold}\n"
            f"  Acc: {metrics['accuracy']:.3f} | "
            f"Sens: {metrics['sensitivity']:.3f} | "
            f"Spec: {metrics['specificity']:.3f} | "
            f"AUC: {metrics['auc']:.3f}"
        )

"""## **E â€” Tabla clÃ­nica final**"""

trainLogDf = pd.DataFrame(trainingLogs)
resultsDf  = pd.DataFrame(results)

trainLogDf.head()

resultsDf.head()

"""### **Curva promedio de loss (promedio sobre folds)**"""

plt.figure(figsize=(8, 5))

for modelName in trainLogDf["model"].unique():
    dfModel = (
        trainLogDf[trainLogDf.model == modelName]
        .groupby("epoch")["loss"]
        .mean()
    )
    plt.plot(dfModel.index, dfModel.values, marker="o", label=modelName)

plt.xlabel("Epoch")
plt.ylabel("Loss (BCE)")
plt.title("Curvas de entrenamiento MIL (promedio sobre folds)")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

"""### **Curvas por fold**"""

for modelName in trainLogDf["model"].unique():
    plt.figure(figsize=(7,4))
    for fold in trainLogDf["fold"].unique():
        df = trainLogDf[
            (trainLogDf.model == modelName) &
            (trainLogDf.fold == fold)
        ]
        plt.plot(df.epoch, df.loss, label=fold)

    plt.title(f"Training Loss â€” {modelName}")
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.show()

"""### Tabla clinica"""

metricCols = [
    "accuracy",
    "sensitivity",
    "specificity",
    "precision",
    "f1",
    "auc"
]

summary = (
    resultsDf[["model"] + metricCols]
    .groupby("model")
    .agg(["mean", "std"])
    .round(3)
)

summary

"""### Matriz de Confusion"""

confusionRows = []

for _, row in resultsDf.iterrows():
    tn, fp, fn, tp = row["confusion"].ravel()

    confusionRows.append({
        "model": row["model"],
        "fold": row["fold"],
        "TN": tn,
        "FP": fp,
        "FN": fn,
        "TP": tp
    })

confusionDf = pd.DataFrame(confusionRows)
confusionDf

confusionSummary = (
    confusionDf
    .groupby("model")[["TN", "FP", "FN", "TP"]]
    .agg(["mean", "std"])
    .round(2)
)

confusionSummary

import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

def plotMeanConfusionMatrix(confusionSummary, modelName):
    meanVals = confusionSummary.loc[modelName].xs("mean", level=1)

    cm = np.array([
        [meanVals["TN"], meanVals["FP"]],
        [meanVals["FN"], meanVals["TP"]]
    ])

    plt.figure(figsize=(4.5, 4))
    sns.heatmap(
        cm,
        annot=True,
        fmt=".1f",
        cmap="Blues",
        xticklabels=["Benigno", "Maligno"],
        yticklabels=["Benigno", "Maligno"]
    )
    plt.xlabel("PredicciÃ³n")
    plt.ylabel("Ground Truth")
    plt.title(f"Matriz de ConfusiÃ³n Promedio â€” {modelName}")
    plt.tight_layout()
    plt.show()

for model in confusionSummary.index:
    plotMeanConfusionMatrix(confusionSummary, model)

"""### Boxplot de mÃ©tricas clÃ­nicas por modelo"""

metricsToPlot = ["accuracy", "sensitivity", "specificity", "auc"]
colors = ['#ADD8E6', '#90EE90', '#FFB6C1', '#DDA0DD'] # Lighter shades: LightBlue, LightGreen, LightPink, Plum

for i, metric in enumerate(metricsToPlot):
    plt.figure(figsize=(6,4))
    sns.boxplot(data=resultsDf, x="model", y=metric, color=colors[i]) # Use a different color for each metric
    plt.title(f"DistribuciÃ³n de {metric.upper()} por modelo")
    plt.grid(True)
    plt.show()

"""### Radar plot clÃ­nico"""

from math import pi

def radarPlot(summary, model):
    metrics = ["accuracy", "sensitivity", "specificity", "precision", "f1", "auc"]
    # Correctly access 'mean' values from the MultiIndex columns
    values = summary.loc[model].xs('mean', level=1)[metrics].values.tolist()
    values += values[:1]

    angles = [n / float(len(metrics)) * 2 * pi for n in range(len(metrics))]
    angles += angles[:1]

    plt.figure(figsize=(5,5))
    ax = plt.subplot(111, polar=True)
    ax.plot(angles, values, linewidth=2)
    ax.fill(angles, values, alpha=0.25)
    ax.set_thetagrids(np.degrees(angles[:-1]), metrics)
    ax.set_title(f"Perfil clÃ­nico â€” {model}")
    plt.show()

radarPlot(summary, "SmABMIL")

summary.to_csv("mil_metrics_summary.csv")
confusionSummary.to_csv("mil_confusion_summary.csv")
resultsDf.to_csv("mil_results_per_fold.csv", index=False)
trainLogDf.to_csv("mil_training_logs.csv", index=False)

"""## **F Interpretabilidad por atenciÃ³n (ABMIL / SmABMIL)**

### Visualizar pesos de atenciÃ³n
"""

def visualizeAttention(model, bag, topK=20):
    model.eval()

    bag = bag.to(device)

    with torch.no_grad():
        _, A = model(bag)   # atenciÃ³n
        A = A.squeeze().cpu().numpy()

    # Top parches con mayor atenciÃ³n
    idx = np.argsort(A)[::-1][:topK]

    plt.figure(figsize=(8, 4))
    plt.bar(range(topK), A[idx])
    plt.xlabel("Top patches")
    plt.ylabel("Attention weight")
    plt.title("Top-k Attention Weights per Bag (Instance-level MIL)")
    plt.grid(True)
    plt.show()

    return idx, A

abmilModel   = trainedModels[("ABMIL", "Val1")]
smabmilModel = trainedModels[("SmABMIL", "Val1")]

positiveIdx = [i for i, y in enumerate(testLabels) if y == 1]

assert len(positiveIdx) > 0, "No positive bags in test set"

bagIdx = positiveIdx[0]
bag = testBags[bagIdx]

"""### Visualizar atenciÃ³n â€” ABMIL"""

print("ABMIL â€“ AtenciÃ³n")
visualizeAttention(abmilModel, bag)

"""### Visualizar atenciÃ³n â€” SmABMIL"""

print("SmABMIL â€“ AtenciÃ³n suavizada")
visualizeAttention(smabmilModel, bag)

"""## Visualizar TOP-K parches"""

def getAttentionScores(model, bag):
    model.eval()
    bag = bag.to(device)

    with torch.no_grad():
        logits, A = model(bag)
        prob = torch.sigmoid(logits).item()
        A = A.squeeze().cpu().numpy()

    return A, prob

def plotTopKAttentionBars(model, bag, topK=20, title="Top-K Attention Weights"):
    A, prob = getAttentionScores(model, bag)

    idx = np.argsort(A)[::-1][:topK]

    plt.figure(figsize=(8, 4))
    plt.bar(range(topK), A[idx])
    plt.xlabel("Top instances (patch embeddings)")
    plt.ylabel("Attention weight")
    plt.title(f"{title} | P(bag=1)={prob:.3f}")
    plt.grid(True)
    plt.tight_layout()
    plt.show()

    return idx

plotTopKAttentionBars(abmilModel, bag, title="ABMIL â€“ Attention Distribution")
plotTopKAttentionBars(smabmilModel, bag, title="SmABMIL â€“ Smoothed Attention")

def plotAttentionHistogram(model, bag, title):
    A, _ = getAttentionScores(model, bag)

    plt.figure(figsize=(6,4))
    plt.hist(A, bins=30)
    plt.xlabel("Attention weight")
    plt.ylabel("Frequency")
    plt.title(title)
    plt.grid(True)
    plt.tight_layout()
    plt.show()

plotAttentionHistogram(abmilModel, bag, "ABMIL â€“ Attention Distribution")
plotAttentionHistogram(smabmilModel, bag, "SmABMIL â€“ Attention Distribution")

"""# **Sprint 5: Interpretabilidad Visual y Correlato HistopatolÃ³gico**

## Objetivo:
- Proyectar los pesos de atenciÃ³n aprendidos por modelos MIL basados en atenciÃ³n (ABMIL y SmABMIL) sobre el espacio histopatolÃ³gico original, permitiendo la identificaciÃ³n de regiones tisulares relevantes y su correlato con patrones morfolÃ³gicos clÃ­nicamente significativos.

## 5.A ReconstrucciÃ³n del bag enriquecido

### 5.A.1 FunciÃ³n para construir un bag enriquecido
"""

def buildEnrichedBag(
    wsiId,
    manifestDf,
    backbone,
    transform,
    device,
    batchSize=64
):
    dfWsi = manifestDf[manifestDf.wsiId == wsiId].reset_index(drop=True)

    images = []
    coords  = []

    for _, row in dfWsi.iterrows():
        img = Image.open(row.imagePath).convert("RGB")
        images.append(img)
        coords.append([row.coordX, row.coordY])

    dataset = [
        transform(img) for img in images
    ]

    loader = DataLoader(
        dataset,
        batch_size=batchSize,
        shuffle=False
    )

    features = []

    backbone.eval()
    with torch.no_grad():
        for batch in loader:
            batch = batch.to(device)
            emb = backbone(batch).cpu()
            features.append(emb)

    features = torch.cat(features, dim=0)

    return {
        "features": features,                 # [N, D]
        "coords": torch.tensor(coords),        # [N, 2]
        "images": images,                      # list[PIL.Image]
        "wsiId": wsiId
    }

"""### 5.A.2 ConstrucciÃ³n del bag"""

wsiId = manifestDf[
    (manifestDf.fold == "Val1") &
    (manifestDf.split == "test")
]["wsiId"].iloc[0]

enrichedBag = buildEnrichedBag(
    wsiId=wsiId,
    manifestDf=manifestDf,
    backbone=backbone,
    transform=transform,
    device=device
)

print("Bag construido:")
print("Features:", enrichedBag["features"].shape)
print("Coords  :", enrichedBag["coords"].shape)
print("Images  :", len(enrichedBag["images"]))

"""## 5.B AtenciÃ³n + Heatmap espacial sobre la WSI

### 5.B.1 Obtener atenciÃ³n desde ABMIL
"""

# â€”â€”â€”â€”â€” 5.B.1 Obtener atenciÃ³n desde ABMIL â€”â€”â€”â€”â€”
abmilModel = trainedModels[("ABMIL", "Val1")]
abmilModel.eval()

with torch.no_grad():
    logits_ab, A_ab = abmilModel(enrichedBag["features"].to(device))
    attn_ab = A_ab.squeeze().cpu().numpy()
    prob_ab = torch.sigmoid(logits_ab).item()

print(f"P(WSI=Maligna) â€” ABMIL: {prob_ab:.3f}")

"""### 5.B.2 Heatmap de atenciÃ³n espacial (figura tipo journal)"""

coords = enrichedBag["coords"].numpy()

plt.figure(figsize=(6,6))
plt.scatter(
    coords[:,0], coords[:,1],
    c=attn_ab,
    cmap="hot",
    s=20
)
plt.colorbar(label="Attention weight")
plt.gca().invert_yaxis()
plt.title(
    f"Heatmap de AtenciÃ³n MIL â€” ABMIL\n"
    f"P(cÃ¡ncer clÃ­nicamente significativo) = {prob_ab:.3f}"
)
plt.xlabel("Coordenada X (px)")
plt.ylabel("Coordenada Y (px)")
plt.tight_layout()
plt.show()

# â€”â€”â€”â€”â€” 5.B.1 Obtener atenciÃ³n desde SmABMIL â€”â€”â€”â€”â€”
smabmilModel = trainedModels[("SmABMIL", "Val1")]
smabmilModel.eval()
with torch.no_grad():
    logits, A = smabmilModel(enrichedBag["features"].to(device))
    attn = A.squeeze().cpu().numpy()
    prob = torch.sigmoid(logits).item()

print(f"P(WSI=Maligna) â€” SmABMIL: {prob:.3f}")

plt.figure(figsize=(6,6))
plt.scatter(
    coords[:,0], coords[:,1],
    c=attn,
    cmap="hot",
    s=20
)
plt.colorbar(label="Attention weight")
plt.gca().invert_yaxis()
plt.title(
    f"Heatmap de AtenciÃ³n MIL sobre la WSI â€” SmABMIL\n"
    f"P(cÃ¡ncer clÃ­nicamente significativo) = {prob:.3f}"
)
plt.xlabel("Coordenada X (px)")
plt.ylabel("Coordenada Y (px)")
plt.tight_layout()
plt.show()

"""## 5.C â€” Top-K parches reales (evidencia visual)

### 5.C.1 SelecciÃ³n de parches mÃ¡s relevantes
"""

K = 9
idx = np.argsort(attn)[-K:][::-1]

"""### 5.C.2 Figura tipo journal (3Ã—3)"""

fig, axes = plt.subplots(3, 3, figsize=(7,7))

for ax, i in zip(axes.flatten(), idx):
    ax.imshow(enrichedBag["images"][i])
    ax.set_title(f"Î±={attn[i]:.3f}", fontsize=9)
    ax.axis("off")

plt.suptitle(
    f"Top-{K} parches mÃ¡s relevantes (SmABMIL)\nP(WSI=1)={prob:.3f}",
    fontsize=12
)
plt.tight_layout()
plt.show()



"""##"""